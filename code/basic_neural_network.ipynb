{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is to build a very elementary neural network, trying to predict the expression of certain genes based on DNA sequences in non-coding regions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda/envs/ipykernel_py3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from Bio import SeqIO\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Processing\n",
    "We first perform the **one hot encoding** to translate the DNA based \"AGCT\" into corresponding 0/1 values. One thing to note is that there does exist 'n's in lots of DNA sequences, and we treat them as all false."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "base_pairs = {'A': [1, 0, 0, 0], \n",
    "'C': [0, 1, 0, 0],\n",
    "'G': [0, 0, 1, 0],\n",
    "'T': [0, 0, 0, 1],\n",
    "'a': [1, 0, 0, 0],\n",
    "'c': [0, 1, 0, 0],\n",
    "'g': [0, 0, 1, 0],\n",
    "'t': [0, 0, 0, 1],\n",
    "'n': [0, 0, 0, 0],\n",
    "'N': [0, 0, 0, 0]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following are some functions to get the one hot encoded DNA data into some input matrix that can be fed into neural network algorithms. The major things to note are the following:\n",
    "1. DNA sequences are of difference lengths, some very short (100~ bases), some very long (3000~ bases). Since most sequences are in the length range 1000 - 2000, we decide to only take the first 1000 bases of each sequence to train the neural network and make the predictions. If too long, simply truncate it to length 1000. If too short, simply fill with zeros to extend it. \n",
    "2. DNA sequences are in different strands, some in negative strand, some in positive. We take the complement of the sequence if it is taken form the negative strand so thsat all our data is from the same (positive) strand.\n",
    "3. The entire sequence is *flattend*. For example, AGCT would be transformed into [1,0,0,0,0,0,1,0,0,1,0,0,0,0,0,1] where the first four represent A and the next four represent G and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def align_sequence(length, sequence):\n",
    "    if len(sequence) > length:\n",
    "        aligned_seq = sequence[:length]\n",
    "    else:\n",
    "        aligned_seq = sequence + [0]*(length-len(sequence))\n",
    "    return aligned_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# NOT used\n",
    "def to_positive_strand(strand, sequence):\n",
    "    if strand == '-':\n",
    "        unflattened_seq = [base_pairs[n] for n in sequence.complement()]\n",
    "    else:\n",
    "        unflattened_seq = [base_pairs[n] for n in sequence]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_seq_record(seq_record, X, y):\n",
    "    header = seq_record.description.split('|')\n",
    "    expressed = int(header[1])\n",
    "    y.append(expressed)\n",
    "    # unflattened_seq = to_positive_strand(header[3], seq_record.seq)\n",
    "    # NO NEED to reverse complement\n",
    "    unflattened_seq = [base_pairs[n] for n in seq_record.seq]\n",
    "    flattened_seq = [i for x in unflattened_seq for i in x]\n",
    "    aligned_seq = align_sequence(4000, flattened_seq)\n",
    "    X.append(np.array(aligned_seq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_file(file, X, y):\n",
    "    seq_record_list = list(SeqIO.parse(\"../data/input/3.24_species_only/\" + file,\"fasta\"))\n",
    "    for i in range(len(seq_record_list)):\n",
    "        process_seq_record(seq_record_list[i], X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training size is the number of files to read for training. Read 100 files would give us 2400 sequences. <br/> For this simple model, we use 2400 sequence to train the neural network and 240 sequences to test its performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "training_size = 500\n",
    "test_size = 50\n",
    "file_count = 0\n",
    "\n",
    "X_train = []\n",
    "y_train = []\n",
    "X_test = []\n",
    "y_test = []\n",
    "\n",
    "for file in os.listdir(\"../data/input/3.24_species_only\"):\n",
    "    if file.endswith(\".fa\"):\n",
    "        if (file_count < training_size):\n",
    "            read_file(file, X_train, y_train)\n",
    "        elif (file_count < training_size + test_size):\n",
    "            read_file(file, X_test, y_test)\n",
    "        file_count += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We examine the shape of all the training and test data matrix to check that the above code works as we expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(12000, 4000), (12000, 1), (1200, 4000), (1200, 1)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = np.array(X_train)\n",
    "y_train = np.array(y_train)\n",
    "if len(y_train.shape) == 1:\n",
    "    y_train = np.transpose(np.array([y_train]))\n",
    "X_test = np.array(X_test)\n",
    "y_test = np.transpose(np.array(y_test))\n",
    "if len(y_test.shape) == 1:\n",
    "    y_test = np.transpose(np.array([y_test]))\n",
    "[X_train.shape, y_train.shape, X_test.shape, y_test.shape]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression\n",
    "Before actually getting into the neural network, we first try to implement a very simple logistic regression model to get a taste of the prediction procedure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import linear_model as lm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = lm.LogisticRegression()\n",
    "model.fit(X_train, y_train.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.593"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_predicted = np.array(model.predict(X_test))\n",
    "round(sum(y_test.ravel() == y_predicted)/y_test.shape[0], 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This result of the rate of  correct prediction is slightly better, if any, than random guessing. This suggests that a lot of work needs to be done before we get a satisfying neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network with Keras\n",
    "Now that we have the data ready in the desired numpy array format with correct shapes, we can proceed to train the neural network with our training data in keras and use our test data to see how accurate it performs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell implements a sequential neural network with 1 input layer (4000 neurons), 4 hidden layers (1000, 400, 40, 10 neurons repectively) and 1 output layer with keras. <br/>\n",
    "All layers except the final use **relu** as activation functions while the final layer uses **sigmoid** as activation function. The cost fucntion is just defined as the binary crossentrophy. <br/> We train our neural network with 12000 DNA sequences in our training data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "12000/12000 [==============================] - 8s 644us/step - loss: 0.6742 - acc: 0.5971\n",
      "Epoch 2/10\n",
      "12000/12000 [==============================] - 8s 683us/step - loss: 0.6556 - acc: 0.6106\n",
      "Epoch 3/10\n",
      "12000/12000 [==============================] - 8s 650us/step - loss: 0.6359 - acc: 0.6281\n",
      "Epoch 4/10\n",
      "12000/12000 [==============================] - 9s 723us/step - loss: 0.6100 - acc: 0.6623\n",
      "Epoch 5/10\n",
      "12000/12000 [==============================] - 8s 706us/step - loss: 0.5754 - acc: 0.7017\n",
      "Epoch 6/10\n",
      "12000/12000 [==============================] - 8s 678us/step - loss: 0.5328 - acc: 0.7380\n",
      "Epoch 7/10\n",
      "12000/12000 [==============================] - 7s 611us/step - loss: 0.4815 - acc: 0.7786\n",
      "Epoch 8/10\n",
      "12000/12000 [==============================] - 7s 609us/step - loss: 0.4365 - acc: 0.8052\n",
      "Epoch 9/10\n",
      "12000/12000 [==============================] - 7s 619us/step - loss: 0.4067 - acc: 0.8159\n",
      "Epoch 10/10\n",
      "12000/12000 [==============================] - 7s 623us/step - loss: 0.3816 - acc: 0.8276\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x181ad098d0>"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model, Sequential\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(units=1000, activation='relu', input_dim=4000))\n",
    "model.add(Dense(units=400, activation='relu'))\n",
    "model.add(Dense(units=40, activation='relu'))\n",
    "model.add(Dense(units=10, activation='relu'))\n",
    "model.add(Dense(units=1, activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer='SGD',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.fit(X_train, y_train, batch_size=100, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the accuracy on the training set reaches 83% after 10 epoches. <br/>\n",
    "Now we test the performance of this model with our test data. <br/> The test data contains 1200 DNA sequences. This size, I think, is already big enough for us to believe that the performance on this test data is quite representative of how the model would perform on new data in general (in other words, with little bias). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "result = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "correct = list(np.apply_along_axis(lambda x: 0 if x<0.5 else 1, 1, result))==y_test.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.618"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "round(sum(correct)/y_test.shape[0], 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result is roughly 62%, clearly better than random guessing, which at least suggests that the neural network is actually running. <br/>\n",
    "However, it is still far from what would be considered a satisfying prediction algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some major points to consider that may be helpful to improving the performance of the neural network:\n",
    "1. Use more complex neural network structure rather than the simple sequential model used above.\n",
    "2. Incorporate other information like what region of genome is the sequence located, the mapping of transcription factors, etc.\n",
    "3. Improve the way sequences of different lengths are aligned (Our current approach is truncating the long, filling zero with the short, which probably is too naive and causes significant loss of information)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Whole data set OR NOT?\n",
    "Time changes as data size increases\n",
    "(data size increases & more than 1000 bases)\n",
    "2. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
