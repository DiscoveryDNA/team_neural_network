{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import ast\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from Bio import SeqIO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell imports all the TFBS scores and transform them into a dictionary called `all_scores`.\n",
    "\n",
    "`all_scores` has the data structure: `{species: {motif: {raw_position: score}}}`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A helper function to extract the motif name from the csv name.\n",
    "def get_motif(name):\n",
    "    if 'cad_FlyReg.fm' in name:\n",
    "        return 'cad_FlyReg.fm'\n",
    "    if 'hb_nar2008.fm' in name:\n",
    "        return 'hb_nar2008.fm'\n",
    "    if 'bcd_FlyReg.fm' in name:\n",
    "        return 'bcd_FlyReg.fm'\n",
    "\n",
    "path = '../data/input/5_TFBS_score_subset_30May2018'\n",
    "all_csvs = glob.glob(path + '/*.csv')\n",
    "all_scores = {}\n",
    "for csv_ in all_csvs:\n",
    "    with open(csv_, encoding='utf-8') as csv_file:\n",
    "        motif = get_motif(csv_file.name)\n",
    "        for a_line in csv_file:\n",
    "            curr_line = a_line.split('\\t')\n",
    "            strand = curr_line[6]\n",
    "            if strand == 'positive\\n':\n",
    "                score = float(curr_line[2])\n",
    "                species = curr_line[4]\n",
    "                raw_position = int(curr_line[5])\n",
    "                if species not in all_scores:\n",
    "                    all_scores[species] = {}\n",
    "                if motif not in all_scores[species]:\n",
    "                    all_scores[species][motif] = {}\n",
    "                all_scores[species][motif][raw_position] = score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The aim of the following cell is to produce a one-hot encoding scheme with TFBS scores embedded for each DNA sequence segment.\n",
    "\n",
    "It consists of three parts:\n",
    "\n",
    "1. Read in all DNA sequence segments.\n",
    "2. Transform each position of the DNA sequence into a 4-letter one-hot encoding based on the `base_pairs` dictionary.\n",
    "3. For each position, attach the TFBS scores to the end of the one-hot encoding.\n",
    "4. Output the final encoding into `txt` files for bookkeeping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Use the following dictionary to perform the transformation\n",
    "base_pairs = {'A': [1, 0, 0, 0], \n",
    "              'C': [0, 1, 0, 0],\n",
    "              'G': [0, 0, 1, 0],\n",
    "              'T': [0, 0, 0, 1],\n",
    "              'a': [1, 0, 0, 0],\n",
    "              'c': [0, 1, 0, 0],\n",
    "              'g': [0, 0, 1, 0],\n",
    "              't': [0, 0, 0, 1],\n",
    "              'n': [0, 0, 0, 0],\n",
    "              'N': [0, 0, 0, 0]}\n",
    "\n",
    "file_num_limit = 110    # The maximum number of files to be decoded\n",
    "file_count = 0\n",
    "\n",
    "# Iterate through every file\n",
    "for file in os.listdir(\"../data/input/3.24_species_only\"):\n",
    "    one_hot = []\n",
    "    to_write = False\n",
    "    # When the number of file decoded has reached the limit, stop\n",
    "    if file_count < file_num_limit:\n",
    "        data = list(SeqIO.parse(\"../data/input/3.24_species_only/\" + file,\"fasta\"))\n",
    "        for n in range(0, len(data)):\n",
    "            # Extract the header information\n",
    "            header = data[n].description.split('|')\n",
    "            descr = data[n].description\n",
    "            regionID = header[0]\n",
    "            expressed = header[1]\n",
    "            speciesID = header[2]\n",
    "            strand = header[3]\n",
    "            # Complement all sequences in the negative DNA strand\n",
    "            if strand == '-':\n",
    "                # Using the syntax [e for e in base_pairs[n]] to create a new pointer for each position\n",
    "                one_hot.append([descr, expressed, speciesID, [[e for e in base_pairs[n]] for n in data[n].seq.complement()]])\n",
    "            else:\n",
    "                one_hot.append([descr, expressed, speciesID, [[e for e in base_pairs[n]] for n in data[n].seq]])\n",
    "        # Attach the TFBS scores to the end of each position\n",
    "        for item in one_hot:\n",
    "            # Only outputs sequences that currently have TFBS scores\n",
    "            # Ignore all sequences that do not have TFBS scores yet\n",
    "            if descr in all_scores:\n",
    "                to_write = True\n",
    "                i = 0\n",
    "                for encoding in item[3]:\n",
    "                    # Take care of positions that do not have TFBS scores, attaching 0 as placeholder (i.e. NA)\n",
    "                    if i not in all_scores[descr]['cad_FlyReg.fm']:\n",
    "                        encoding.extend([0, 0, 0])\n",
    "                    else:\n",
    "                        encoding.append(all_scores[descr]['cad_FlyReg.fm'][i])\n",
    "                        encoding.append(all_scores[descr]['hb_nar2008.fm'][i])\n",
    "                        encoding.append(all_scores[descr]['bcd_FlyReg.fm'][i])\n",
    "                    i += 1\n",
    "                # Write the final encoding into txt files\n",
    "        if to_write:\n",
    "            with open(\"../data/output/\" + regionID + \".txt\", mode=\"w\", encoding='utf-8') as output:\n",
    "                output.write(str(one_hot))\n",
    "            file_count += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The rest of the notebook uses the one-hot encoding files produced above to build a neural network prototype to make sure everything works as intended.\n",
    "\n",
    "The following cell reads in one-hot encoding files as a list `seq_record_list`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2640"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = '../data/output'\n",
    "all_txts = glob.glob(path + '/*.txt')\n",
    "seq_record_list = []\n",
    "# Iterate through all one-hot encoding files\n",
    "for txt_ in all_txts:\n",
    "    with open(txt_, encoding='utf-8') as f:\n",
    "        # attach the one-hot encoding information of this file to the end of seq_record_list\n",
    "        seq_record_list += ast.literal_eval(f.read())\n",
    "len(seq_record_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell transforms the data into a format that is recognizable by the neural network model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A helper function to flatten a 2d list to 1d.\n",
    "# Input: [[1, 2], [2, 3], [3, 4, 5]]\n",
    "# Output: [1, 2, 2, 3, 3, 4, 5]\n",
    "def flatten(lst):\n",
    "    new_lst = []\n",
    "    for sub_lst in lst:\n",
    "        for item in sub_lst:\n",
    "            new_lst.append(item)\n",
    "    return new_lst\n",
    "\n",
    "# A helper function to transform a lst so that its length becomes read_len by:\n",
    "# 1. If len(lst) > read_len, curtail the end of the lst.\n",
    "# 2. If len(lst) < read_len, keep extending the end of the lst with 0 (NA).\n",
    "def curtail(lst, read_len):\n",
    "    if len(lst) > read_len:\n",
    "        lst = lst[:read_len]\n",
    "    else:\n",
    "        for i in range(read_len - len(lst)):\n",
    "            lst.append([0, 0, 0, 0, 0, 0, 0])\n",
    "    return lst\n",
    "\n",
    "# Produce the train-test split\n",
    "# length_read: the length that you want all DNA sequences to conform to\n",
    "def prepare_input(training_size, test_size, length_read):\n",
    "    X_train = []\n",
    "    y_train = []\n",
    "    X_test = []\n",
    "    y_test = []\n",
    "    seq_count = 0\n",
    "    while seq_count < training_size:\n",
    "        X_train.append(flatten(curtail(seq_record_list[seq_count][3], length_read)))\n",
    "        y_train.append(int(seq_record_list[seq_count][1]))\n",
    "        seq_count += 1\n",
    "    while seq_count < (training_size + test_size):\n",
    "        X_test.append(flatten(curtail(seq_record_list[seq_count][3], length_read)))\n",
    "        y_test.append(int(seq_record_list[seq_count][1]))\n",
    "        seq_count += 1\n",
    "    return X_train, y_train, X_test, y_test\n",
    "\n",
    "# Turn list into numpy tensors that can directly feed into a neural network model\n",
    "def to_np_array(X_train, y_train, X_test, y_test):\n",
    "    X_train = np.array(X_train)\n",
    "    y_train = np.array(y_train)\n",
    "    if len(y_train.shape) == 1:\n",
    "        y_train = np.transpose(np.array([y_train]))\n",
    "    X_test = np.array(X_test)\n",
    "    y_test = np.transpose(np.array(y_test))\n",
    "    if len(y_test.shape) == 1:\n",
    "        y_test = np.transpose(np.array([y_test]))\n",
    "    return X_train, y_train, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(2112, 14000), (2112, 1), (528, 14000), (528, 1)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, y_train, X_test, y_test = prepare_input(2112, 528, 2000)\n",
    "X_train, y_train, X_test, y_test = to_np_array(X_train, y_train, X_test, y_test)\n",
    "[X_train.shape, y_train.shape, X_test.shape, y_test.shape]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/andyfang/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from sklearn import linear_model as lm\n",
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model, Sequential"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following is a logistic regression model. Just to check if everything works as intended."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = lm.LogisticRegression()\n",
    "model.fit(X_train, y_train.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.642"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_predicted = np.array(model.predict(X_test))\n",
    "round(sum(y_test.ravel() == y_predicted)/y_test.shape[0], 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The rest of the cells build a basic neural network, just for the sake of testing whether the data is usable.\n",
    "\n",
    "Using this neural network prototype, we can achieve a test accuracy of 0.652, incating lots of improvement potentials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_nn(X_train, y_train, pr):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(units=1000, activation='relu', input_dim=14000))\n",
    "    model.add(Dense(units=400, activation='relu'))\n",
    "    model.add(Dense(units=40, activation='relu'))\n",
    "    model.add(Dense(units=10, activation='elu'))\n",
    "    model.add(Dense(units=1, activation='sigmoid'))\n",
    "    model.compile(optimizer = 'SGD',\n",
    "                  loss = 'binary_crossentropy',\n",
    "                  metrics = ['accuracy'])\n",
    "    model.fit(X_train, y_train, batch_size=100, epochs=5, verbose = pr)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test_accuracy(model, X_test, y_test):\n",
    "    result = model.predict(X_test)\n",
    "    correct = list(np.apply_along_axis(lambda x: 0 if x<0.5 else 1, 1, result))==y_test.ravel()\n",
    "    return round(sum(correct)/y_test.shape[0], 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.6719 - acc: 0.7694\n",
      "Epoch 2/5\n",
      "2112/2112 [==============================] - 3s 2ms/step - loss: 0.0701 - acc: 0.9948\n",
      "Epoch 3/5\n",
      "2112/2112 [==============================] - 3s 1ms/step - loss: 0.0150 - acc: 1.0000\n",
      "Epoch 4/5\n",
      "2112/2112 [==============================] - 3s 1ms/step - loss: 0.0074 - acc: 1.0000\n",
      "Epoch 5/5\n",
      "2112/2112 [==============================] - 3s 1ms/step - loss: 0.0047 - acc: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.652"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_accuracy(train_nn(X_train, y_train, 1), X_test, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
