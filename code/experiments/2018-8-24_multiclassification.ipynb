{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/matplotlib/__init__.py:1067: UserWarning: Duplicate key in file \"/home/ubuntu/.config/matplotlib/matplotlibrc\", line #2\n",
      "  (fname, cnt))\n",
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/matplotlib/__init__.py:1067: UserWarning: Duplicate key in file \"/home/ubuntu/.config/matplotlib/matplotlibrc\", line #3\n",
      "  (fname, cnt))\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_buffer_path = \"/home/ubuntu/newOutput/10_percent/random_0.1_instance_7.txt\"\n",
    "random_buffer_path = \"/home/ubuntu/newOutput/random_sequences/random_sequence_buffer.txt\"\n",
    "classes_path = \"/home/ubuntu/data/team_neural_network/data/input/classification_table3_21Aug2018.csv\"\n",
    "curtail_len = 3000\n",
    "motif_num = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "with open(real_buffer_path, \"rb\") as buff:\n",
    "    seq_record_list = pickle.load(buff)\n",
    "expression = []\n",
    "with open(classes_path, encoding='utf-8') as csv_file:\n",
    "    for a_line in csv_file:\n",
    "        curr_line = a_line.split(',')\n",
    "        stage = curr_line[3]\n",
    "        vtid = curr_line[1]\n",
    "        if vtid != '\"VTID\"':\n",
    "            expression.append([vtid, stage])\n",
    "for a in range(0,len(seq_record_list)):\n",
    "    seq_record_list[a][0] = seq_record_list[a][0].split('|')[0][2:]    \n",
    "for a in range(0, len(expression)):\n",
    "    expression[a][0] = expression[a][0][3:-1]\n",
    "seq_record_list = pd.DataFrame(seq_record_list)\n",
    "seq_record_list.columns = ['vtid', 1, 2, 3]\n",
    "expression = pd.DataFrame(expression)\n",
    "expression.columns = ['vtid', 1]\n",
    "seq_record_list = seq_record_list.merge(expression, on='vtid')\n",
    "seq_record_list = seq_record_list.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sequences in training/validation set are: 7051\n",
      "Number of sequences in testing set are: 1037\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from random import shuffle\n",
    "\n",
    "first_list = [] # to add to training set\n",
    "second_list = [] # to add to test set\n",
    "current = [] # contains all 24 sequences from the same DNA section\n",
    "\n",
    "for i in range(len(seq_record_list)):\n",
    "    current.append(seq_record_list.pop())\n",
    "    if len(current) == 24:\n",
    "        shuffle(current) # Shuffle the 24 sequences from the same DNA section\n",
    "        random_select = random.randint(18, 24) # Allocate the number of sequences to the training set\n",
    "        first_list.extend(current[:random_select])\n",
    "        second_list.extend(current[random_select:])\n",
    "        current = []\n",
    "\n",
    "shuffle(first_list) # Shuffle again to eliminate dependencies\n",
    "shuffle(second_list) # Shuffle again to eliminate dependencies\n",
    "\n",
    "seq_record_list = first_list + second_list\n",
    "\n",
    "print(\"Number of sequences in training/validation set are: \" + str(len(first_list)))\n",
    "print(\"Number of sequences in testing set are: \" + str(len(second_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_val_num = len(first_list)\n",
    "test_num = len(second_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A helper function to flatten a 2d list to 1d.\n",
    "# Input: [[1, 2], [2, 3], [3, 4, 5]]\n",
    "# Output: [1, 2, 2, 3, 3, 4, 5]\n",
    "def flatten(lst):\n",
    "    new_lst = []\n",
    "    for sub_lst in lst:\n",
    "        for item in sub_lst:\n",
    "            new_lst.append(item)\n",
    "    return new_lst\n",
    "\n",
    "# A helper function to transform a lst so that its length becomes read_len by:\n",
    "# 1. If len(lst) > read_len, curtail the end of the lst.\n",
    "# 2. If len(lst) < read_len, keep extending the end of the lst with 0 (NA).\n",
    "def curtail(lst, read_len, motif_number):\n",
    "    if len(lst) > read_len:\n",
    "        lst = lst[:read_len]\n",
    "    else:\n",
    "        for i in range(read_len - len(lst)):\n",
    "            lst.append([0 for _ in range(motif_number + 4)])\n",
    "    return lst\n",
    "\n",
    "# Produce the train-test split\n",
    "# length_read: the length that you want all DNA sequences to conform to\n",
    "def prepare_input(training_size, test_size, length_read, original_list, motif_number):\n",
    "    X_train = []\n",
    "    y_train = []\n",
    "    X_test = []\n",
    "    y_test = []\n",
    "    seq_count = 0\n",
    "    while seq_count < training_size:\n",
    "        X_train.append(flatten(curtail(original_list[seq_count][3], length_read, motif_number)))\n",
    "        y_train.append(original_list[seq_count][4])\n",
    "        seq_count += 1\n",
    "    while seq_count < (training_size + test_size):\n",
    "        X_test.append(flatten(curtail(original_list[seq_count][3], length_read, motif_number)))\n",
    "        y_test.append(original_list[seq_count][4])\n",
    "        seq_count += 1\n",
    "    return X_train, y_train, X_test, y_test\n",
    "\n",
    "# Turn list into numpy tensors that can directly feed into a neural network model\n",
    "def to_np_array(X_train, y_train, X_test, y_test):\n",
    "    X_train = np.array(X_train)\n",
    "    y_train = np.array(y_train)\n",
    "    if len(y_train.shape) == 1:\n",
    "        y_train = np.transpose(np.array([y_train]))\n",
    "    X_test = np.array(X_test)\n",
    "    y_test = np.transpose(np.array(y_test))\n",
    "    if len(y_test.shape) == 1:\n",
    "        y_test = np.transpose(np.array([y_test]))\n",
    "    return X_train, y_train, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(7051, 21000), (7051, 1), (1037, 21000), (1037, 1)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, y_train, X_test, y_test = prepare_input(train_val_num, test_num, curtail_len, seq_record_list, motif_num)\n",
    "X_train, y_train, X_test, y_test = to_np_array(X_train, y_train, X_test, y_test)\n",
    "[X_train.shape, y_train.shape, X_test.shape, y_test.shape]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "y_train_new = [];\n",
    "for row in y_train:\n",
    "    a = [0]*64\n",
    "    count = 0\n",
    "    if re.search('4_6', row[0]):\n",
    "        count+=1\n",
    "    if re.search('7_8', row[0]):\n",
    "        count+=2\n",
    "    if re.search('9_10', row[0]):\n",
    "        count+=4\n",
    "    if re.search('11_12', row[0]):\n",
    "        count+=8\n",
    "    if re.search('13_14', row[0]):\n",
    "        count+=16\n",
    "    if re.search('15_16', row[0]):\n",
    "        count+=32\n",
    "    a[count] = 1\n",
    "    y_train_new.append(a)\n",
    "y_train_new = np.asarray(y_train_new)\n",
    "y_test_new = [];\n",
    "for row in y_train:\n",
    "    a = [0]*64\n",
    "    count = 0\n",
    "    if re.search('4_6', row[0]):\n",
    "        count+=1\n",
    "    if re.search('7_8', row[0]):\n",
    "        count+=2\n",
    "    if re.search('9_10', row[0]):\n",
    "        count+=4\n",
    "    if re.search('11_12', row[0]):\n",
    "        count+=8\n",
    "    if re.search('13_14', row[0]):\n",
    "        count+=16\n",
    "    if re.search('15_16', row[0]):\n",
    "        count+=32\n",
    "    a[count] = 1\n",
    "    y_test_new.append(a)\n",
    "y_test_new = np.asarray(y_test_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Dense, CuDNNLSTM, CuDNNGRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_rnn = X_train.reshape(train_val_num, curtail_len, motif_num + 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6345 samples, validate on 706 samples\n",
      "Epoch 1/5\n",
      "6345/6345 [==============================] - 109s 17ms/step - loss: 2.5453 - acc: 0.4485 - val_loss: 2.1293 - val_acc: 0.4419\n",
      "Epoch 2/5\n",
      "6345/6345 [==============================] - 52s 8ms/step - loss: 2.0714 - acc: 0.4496 - val_loss: 2.0886 - val_acc: 0.4419\n",
      "Epoch 3/5\n",
      "6345/6345 [==============================] - 52s 8ms/step - loss: 2.0574 - acc: 0.4496 - val_loss: 2.0834 - val_acc: 0.4419\n",
      "Epoch 4/5\n",
      "6345/6345 [==============================] - 52s 8ms/step - loss: 2.0514 - acc: 0.4496 - val_loss: 2.0830 - val_acc: 0.4419\n",
      "Epoch 5/5\n",
      "6345/6345 [==============================] - 52s 8ms/step - loss: 2.0512 - acc: 0.4496 - val_loss: 2.0887 - val_acc: 0.4419\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(CuDNNLSTM(128, input_shape=(curtail_len, motif_num + 4), return_sequences=True))\n",
    "model.add(CuDNNLSTM(128))\n",
    "model.add(Dense(64, activation='sigmoid'))\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "history = model.fit(X_train_rnn, y_train_new, epochs=5, batch_size=128, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs = range(1, len(acc) + 1)\n",
    "\n",
    "plt.plot(epochs, acc, 'bo', label='Training Accuracy')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation Accuracy')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.xlabel('epoches')\n",
    "plt.legend()\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(epochs, loss, 'bo', label='Training Loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('epoches')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_tensorflow_p36)",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
