{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Date**: 2019-01-29\n",
    "\n",
    "**Authors**: Adam Stafford\n",
    "\n",
    "**Purpose**: To test whether shuffling the input improves the accuracy when using a bidirectional LSTM.\n",
    "\n",
    "This notebook tests a naive alignment scheme that selects the first 1000 positions of a DNA sequence (add null positions to the end if a sequence is shorter than 1000). The sequences are shuffled before feeding into the neural network.\n",
    "\n",
    "**Background**:\n",
    "- The idea of this experiment is based on the understanding that any sequence bears more resemblance with its 23 'siblings' (which are next to each other in the list of sequences before shuffling). Without shuffling, either all siblings are in the training set, or all of them are in the validation set. By shuffling the sequences, all siblings would be randomly distributed into the training set and the validation set. We might be able to extract more information for a sequence based on those of its siblings that are distributed into the training set.\n",
    "\n",
    "**Experiment**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/matplotlib/__init__.py:1067: UserWarning: Duplicate key in file \"/home/ubuntu/.config/matplotlib/matplotlibrc\", line #2\n",
      "  (fname, cnt))\n",
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/matplotlib/__init__.py:1067: UserWarning: Duplicate key in file \"/home/ubuntu/.config/matplotlib/matplotlibrc\", line #3\n",
      "  (fname, cnt))\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the `pickle` buffered list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/new_list_buffer.txt\", \"rb\") as buff:\n",
    "    seq_record_list = pickle.load(buff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell retains only 4800 sequences, for the purpose of testing speed. It also randomly shuffles the 4800 sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_record_list = seq_record_list[:4800]\n",
    "from random import shuffle\n",
    "shuffle(seq_record_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell transforms the data into a format that is recognizable by the neural network model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A helper function to flatten a 2d list to 1d.\n",
    "# Input: [[1, 2], [2, 3], [3, 4, 5]]\n",
    "# Output: [1, 2, 2, 3, 3, 4, 5]\n",
    "def flatten(lst):\n",
    "    new_lst = []\n",
    "    for sub_lst in lst:\n",
    "        for item in sub_lst:\n",
    "            new_lst.append(item)\n",
    "    return new_lst\n",
    "\n",
    "# A helper function to transform a lst so that its length becomes read_len by:\n",
    "# 1. If len(lst) > read_len, curtail the end of the lst.\n",
    "# 2. If len(lst) < read_len, keep extending the end of the lst with 0 (NA).\n",
    "def curtail(lst, read_len):\n",
    "    if len(lst) > read_len:\n",
    "        lst = lst[:read_len]\n",
    "    else:\n",
    "        for i in range(read_len - len(lst)):\n",
    "            lst.append([0, 0, 0, 0, 0, 0, 0, 0])\n",
    "    return lst\n",
    "\n",
    "# Produce the train-test split\n",
    "# length_read: the length that you want all DNA sequences to conform to\n",
    "def prepare_input(training_size, test_size, length_read):\n",
    "    X_train = []\n",
    "    y_train = []\n",
    "    X_test = []\n",
    "    y_test = []\n",
    "    seq_count = 0\n",
    "    while seq_count < training_size:\n",
    "        X_train.append(flatten(curtail(seq_record_list[seq_count][3], length_read)))\n",
    "        y_train.append(int(seq_record_list[seq_count][1]))\n",
    "        seq_count += 1\n",
    "    while seq_count < (training_size + test_size):\n",
    "        X_test.append(flatten(curtail(seq_record_list[seq_count][3], length_read)))\n",
    "        y_test.append(int(seq_record_list[seq_count][1]))\n",
    "        seq_count += 1\n",
    "    return X_train, y_train, X_test, y_test\n",
    "\n",
    "# Turn list into numpy tensors that can directly feed into a neural network model\n",
    "def to_np_array(X_train, y_train, X_test, y_test):\n",
    "    X_train = np.array(X_train)\n",
    "    y_train = np.array(y_train)\n",
    "    if len(y_train.shape) == 1:\n",
    "        y_train = np.transpose(np.array([y_train]))\n",
    "    X_test = np.array(X_test)\n",
    "    y_test = np.transpose(np.array(y_test))\n",
    "    if len(y_test.shape) == 1:\n",
    "        y_test = np.transpose(np.array([y_test]))\n",
    "    return X_train, y_train, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(4320, 8000), (4320, 1), (480, 8000), (480, 1)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, y_train, X_test, y_test = prepare_input(4320, 480, 1000)\n",
    "X_train, y_train, X_test, y_test = to_np_array(X_train, y_train, X_test, y_test)\n",
    "[X_train.shape, y_train.shape, X_test.shape, y_test.shape]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cells build a recurrent network with one bidirectional LSTM layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Dense, CuDNNLSTM, Bidirectional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_rnn = X_train.reshape(4320, 1000, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3888 samples, validate on 432 samples\n",
      "Epoch 1/150\n",
      "3888/3888 [==============================] - 15s 4ms/step - loss: 0.6897 - acc: 0.5201 - val_loss: 0.6852 - val_acc: 0.5486\n",
      "Epoch 2/150\n",
      "3888/3888 [==============================] - 12s 3ms/step - loss: 0.6834 - acc: 0.5278 - val_loss: 0.6842 - val_acc: 0.5486\n",
      "Epoch 3/150\n",
      "3888/3888 [==============================] - 12s 3ms/step - loss: 0.6798 - acc: 0.5123 - val_loss: 0.6827 - val_acc: 0.5486\n",
      "Epoch 4/150\n",
      "3888/3888 [==============================] - 11s 3ms/step - loss: 0.6781 - acc: 0.5198 - val_loss: 0.6834 - val_acc: 0.5486\n",
      "Epoch 5/150\n",
      "3888/3888 [==============================] - 11s 3ms/step - loss: 0.6773 - acc: 0.5216 - val_loss: 0.6836 - val_acc: 0.5486\n",
      "Epoch 6/150\n",
      "3888/3888 [==============================] - 11s 3ms/step - loss: 0.6761 - acc: 0.5273 - val_loss: 0.6842 - val_acc: 0.5463\n",
      "Epoch 7/150\n",
      "3888/3888 [==============================] - 11s 3ms/step - loss: 0.6750 - acc: 0.5383 - val_loss: 0.6850 - val_acc: 0.5463\n",
      "Epoch 8/150\n",
      "3888/3888 [==============================] - 11s 3ms/step - loss: 0.6741 - acc: 0.5430 - val_loss: 0.6846 - val_acc: 0.5417\n",
      "Epoch 9/150\n",
      "3888/3888 [==============================] - 11s 3ms/step - loss: 0.6735 - acc: 0.5437 - val_loss: 0.6835 - val_acc: 0.5486\n",
      "Epoch 10/150\n",
      "3888/3888 [==============================] - 11s 3ms/step - loss: 0.6723 - acc: 0.5509 - val_loss: 0.6824 - val_acc: 0.5741\n",
      "Epoch 11/150\n",
      "3888/3888 [==============================] - 11s 3ms/step - loss: 0.6717 - acc: 0.5574 - val_loss: 0.6823 - val_acc: 0.5625\n",
      "Epoch 12/150\n",
      "3888/3888 [==============================] - 11s 3ms/step - loss: 0.6700 - acc: 0.5684 - val_loss: 0.6936 - val_acc: 0.5162\n",
      "Epoch 13/150\n",
      "3888/3888 [==============================] - 11s 3ms/step - loss: 0.6702 - acc: 0.5617 - val_loss: 0.6825 - val_acc: 0.5486\n",
      "Epoch 14/150\n",
      "3888/3888 [==============================] - 11s 3ms/step - loss: 0.6688 - acc: 0.5674 - val_loss: 0.6811 - val_acc: 0.5671\n",
      "Epoch 15/150\n",
      "3888/3888 [==============================] - 11s 3ms/step - loss: 0.6701 - acc: 0.5599 - val_loss: 0.6792 - val_acc: 0.5856\n",
      "Epoch 16/150\n",
      "3888/3888 [==============================] - 11s 3ms/step - loss: 0.6691 - acc: 0.5723 - val_loss: 0.6898 - val_acc: 0.5347\n",
      "Epoch 17/150\n",
      "3888/3888 [==============================] - 11s 3ms/step - loss: 0.6688 - acc: 0.5702 - val_loss: 0.6788 - val_acc: 0.5856\n",
      "Epoch 18/150\n",
      "3888/3888 [==============================] - 11s 3ms/step - loss: 0.6674 - acc: 0.5700 - val_loss: 0.6790 - val_acc: 0.5949\n",
      "Epoch 19/150\n",
      "3888/3888 [==============================] - 11s 3ms/step - loss: 0.6666 - acc: 0.5772 - val_loss: 0.6854 - val_acc: 0.5370\n",
      "Epoch 20/150\n",
      "3888/3888 [==============================] - 11s 3ms/step - loss: 0.6661 - acc: 0.5761 - val_loss: 0.6790 - val_acc: 0.5718\n",
      "Epoch 21/150\n",
      "3888/3888 [==============================] - 11s 3ms/step - loss: 0.6657 - acc: 0.5761 - val_loss: 0.6821 - val_acc: 0.5440\n",
      "Epoch 22/150\n",
      "3888/3888 [==============================] - 11s 3ms/step - loss: 0.6652 - acc: 0.5862 - val_loss: 0.6808 - val_acc: 0.5509\n",
      "Epoch 23/150\n",
      "3888/3888 [==============================] - 11s 3ms/step - loss: 0.6648 - acc: 0.5800 - val_loss: 0.7091 - val_acc: 0.5162\n",
      "Epoch 24/150\n",
      "3888/3888 [==============================] - 11s 3ms/step - loss: 0.6641 - acc: 0.5892 - val_loss: 0.6822 - val_acc: 0.5625\n",
      "Epoch 25/150\n",
      "3888/3888 [==============================] - 11s 3ms/step - loss: 0.6639 - acc: 0.5851 - val_loss: 0.6876 - val_acc: 0.5463\n",
      "Epoch 26/150\n",
      "3888/3888 [==============================] - 11s 3ms/step - loss: 0.6646 - acc: 0.5797 - val_loss: 0.6790 - val_acc: 0.5880\n",
      "Epoch 27/150\n",
      "3888/3888 [==============================] - 11s 3ms/step - loss: 0.6621 - acc: 0.5908 - val_loss: 0.6798 - val_acc: 0.5718\n",
      "Epoch 28/150\n",
      "3888/3888 [==============================] - 11s 3ms/step - loss: 0.6617 - acc: 0.5910 - val_loss: 0.6822 - val_acc: 0.5625\n",
      "Epoch 29/150\n",
      "3888/3888 [==============================] - 11s 3ms/step - loss: 0.6629 - acc: 0.5874 - val_loss: 0.6790 - val_acc: 0.5741\n",
      "Epoch 30/150\n",
      "3888/3888 [==============================] - 11s 3ms/step - loss: 0.6582 - acc: 0.6065 - val_loss: 0.6814 - val_acc: 0.5602\n",
      "Epoch 31/150\n",
      "3888/3888 [==============================] - 11s 3ms/step - loss: 0.6598 - acc: 0.5980 - val_loss: 0.6773 - val_acc: 0.5810\n",
      "Epoch 32/150\n",
      "3888/3888 [==============================] - 11s 3ms/step - loss: 0.6595 - acc: 0.5905 - val_loss: 0.6767 - val_acc: 0.5787\n",
      "Epoch 33/150\n",
      "3888/3888 [==============================] - 11s 3ms/step - loss: 0.6614 - acc: 0.5934 - val_loss: 0.6798 - val_acc: 0.5694\n",
      "Epoch 34/150\n",
      "3888/3888 [==============================] - 11s 3ms/step - loss: 0.6568 - acc: 0.5965 - val_loss: 0.6736 - val_acc: 0.5880\n",
      "Epoch 35/150\n",
      "2560/3888 [==================>...........] - ETA: 3s - loss: 0.6580 - acc: 0.6055"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(CuDNNLSTM(8, input_shape=(1000, 8), return_sequences=True))\n",
    "#model.add(Bidirectional(CuDNNLSTM(8), input_shape=(1000, 8)))\n",
    "model.add(CuDNNLSTM(8, return_sequences=True))\n",
    "model.add(CuDNNLSTM(4, return_sequences=True))\n",
    "model.add(CuDNNLSTM(4))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n",
    "history = model.fit(X_train_rnn, y_train, epochs=150, batch_size=128, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Result**:\n",
    "\n",
    "The following cell **visualize** the training/validation accuracies and losses over each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs = range(1, len(acc) + 1)\n",
    "\n",
    "plt.plot(epochs, acc, 'bo', label='Training Accuracy')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation Accuracy')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.xlabel('epoches')\n",
    "plt.legend()\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(epochs, loss, 'bo', label='Training Loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('epoches')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utility\n",
    "utility.save_model(model, 'models/adam_compare_2019_01_29.json', 'models/adam_compare_2019_01_29_weights.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The validation trend now follows closely to the training trend.\n",
    "\n",
    "**Conclusion**:\n",
    "\n",
    "- The result of the experiment demonstrates a marked improvement over previous experiments and suggests that the combination of shuffling the data and using a bidirectional neural network improves accuracy.\n",
    "- To summarize, for any sequence, its most resemblance sequences are its 23 ‘siblings’. If we don’t shuffle the data, either all 24 ‘siblings’ are in the training data, or all of them are in the validation data. By shuffling them, the 24 ‘siblings’ are randomly distributed into both training and validation data, allowing us to extract more information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_tensorflow_p36)",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
