{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tesla K80\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils import data\n",
    "import torch\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from lookahead import Lookahead\n",
    "from models_n_training import *\n",
    "from utilities import sampling, one_hot_encoding, curtail, get_training_data, load_data, data_split, dianostic_plots, pad_for_detector\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
    "if use_cuda:\n",
    "    print(torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading model from checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape after conv1d torch.Size([256, 150, 227])\n",
      "shape before lstm torch.Size([256, 227, 150])\n",
      "shape after lstm torch.Size([256, 227, 200])\n",
      "shape after flattening torch.Size([256, 45400])\n"
     ]
    }
   ],
   "source": [
    "batch_size = 256\n",
    "_, C, L = train_x.shape\n",
    "pesudo_input = torch.rand(batch_size, C, L, dtype=train_x.dtype)\n",
    "model = HybridNet(pesudo_input,\n",
    "                   num_filters=150, \n",
    "                   filter_size=30, \n",
    "                   rnn_size=100, \n",
    "                   fc_out1=500, fc_out2= 250,\n",
    "                   dp1=0.8, dp2=0.8, dp3=0.8).to(device)\n",
    "\n",
    "model.load_state_dict(torch.load('/home/ubuntu/data/models/hybrid_net3.state'))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation accuracy on the original validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15780 (12624, 3388, 4) (12624,) (3156, 3388, 4) (3156,)\n"
     ]
    }
   ],
   "source": [
    "data_x = pickle.load(open('/home/ubuntu/data/dmel_seq/data_x_nlc.np', 'rb'))\n",
    "data_y = pickle.load(open('/home/ubuntu/data/dmel_seq/data_y.np', 'rb'))\n",
    "\n",
    "train_x, train_y, val_x, val_y = data_split(data_x, data_y, seed = 157)\n",
    "\n",
    "train_x, val_x = pad_for_detector(train_x, 30), pad_for_detector(val_x, 30)\n",
    "train_x.shape, train_y.shape, val_x.shape, val_y.shape\n",
    "\n",
    "train_x, val_x = torch.from_numpy(train_x.transpose([0, 2, 1])).float(), torch.from_numpy(val_x.transpose([0, 2, 1])).float()\n",
    "train_y, val_y = torch.from_numpy(train_y).float(), torch.from_numpy(val_y).float()\n",
    "\n",
    "# Generate dataset for data loader\n",
    "train_dataset = data.TensorDataset(train_x, train_y)\n",
    "val_dataset = data.TensorDataset(val_x, val_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Validation loss 0.6765187887045053, validation acc 0.8980511675824177] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Generate data loaders\n",
    "def get_acc(y_hat, y):\n",
    "    y_pred = np.where(y_hat >=0.5, 1, 0)\n",
    "    return np.mean(y_pred == y)\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "val_loss_list, val_acc_list = [], []\n",
    "val_loader = data.DataLoader(val_dataset, batch_size=batch_size)\n",
    "total_val_steps = len(val_loader)\n",
    "with torch.no_grad():\n",
    "    val_loss_sum, val_acc_sum = 0, 0\n",
    "    for j, (batch, labels) in enumerate(val_loader):\n",
    "        batch, labels = batch.to(device), labels.to(device)\n",
    "        y_hat = model(batch)\n",
    "        loss = criterion(y_hat, labels)\n",
    "        val_loss_sum += loss.item()\n",
    "        val_acc_sum += get_acc(y_hat.cpu().detach().numpy(), labels.cpu().detach().numpy())\n",
    "    avg_val_loss = val_loss_sum/total_val_steps\n",
    "    avg_val_acc = val_acc_sum/total_val_steps\n",
    "    val_loss_list.append(avg_val_loss)\n",
    "    val_acc_list.append(avg_val_acc)\n",
    "    print('[Validation loss {}, validation acc {}] \\n'.format(avg_val_loss, avg_val_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation accuracy on a different validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15780 (12624, 3388, 4) (12624,) (3156, 3388, 4) (3156,)\n"
     ]
    }
   ],
   "source": [
    "train_x, train_y, val_x, val_y = data_split(data_x, data_y, seed = 42)\n",
    "\n",
    "train_x, val_x = pad_for_detector(train_x, 30), pad_for_detector(val_x, 30)\n",
    "train_x.shape, train_y.shape, val_x.shape, val_y.shape\n",
    "\n",
    "train_x, val_x = torch.from_numpy(train_x.transpose([0, 2, 1])).float(), torch.from_numpy(val_x.transpose([0, 2, 1])).float()\n",
    "train_y, val_y = torch.from_numpy(train_y).float(), torch.from_numpy(val_y).float()\n",
    "\n",
    "# Generate dataset for data loader\n",
    "train_dataset = data.TensorDataset(train_x, train_y)\n",
    "val_dataset = data.TensorDataset(val_x, val_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Validation loss 0.1715787210716651, validation acc 0.969622825091575] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Generate data loaders\n",
    "def get_acc(y_hat, y):\n",
    "    y_pred = np.where(y_hat >=0.5, 1, 0)\n",
    "    return np.mean(y_pred == y)\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "val_loss_list, val_acc_list = [], []\n",
    "val_loader = data.DataLoader(val_dataset, batch_size=batch_size)\n",
    "total_val_steps = len(val_loader)\n",
    "with torch.no_grad():\n",
    "    val_loss_sum, val_acc_sum = 0, 0\n",
    "    for j, (batch, labels) in enumerate(val_loader):\n",
    "        batch, labels = batch.to(device), labels.to(device)\n",
    "        y_hat = model(batch)\n",
    "        loss = criterion(y_hat, labels)\n",
    "        val_loss_sum += loss.item()\n",
    "        val_acc_sum += get_acc(y_hat.cpu().detach().numpy(), labels.cpu().detach().numpy())\n",
    "    avg_val_loss = val_loss_sum/total_val_steps\n",
    "    avg_val_acc = val_acc_sum/total_val_steps\n",
    "    val_loss_list.append(avg_val_loss)\n",
    "    val_acc_list.append(avg_val_acc)\n",
    "    print('[Validation loss {}, validation acc {}] \\n'.format(avg_val_loss, avg_val_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROC curve (on the original validation set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hybrid_net_infer(model, seq, thresh=0.5):\n",
    "    \"\"\" final score = avg(score(seq), score(seq comp))\n",
    "    - Input:\n",
    "    model (pytorch model): hybrid_net\n",
    "    seq (torch tensor): seq to be evaluated\n",
    "    thresh (float): threshold for judgement\n",
    "    -Output: 1 for positive or 0 for negative\n",
    "    -Note: base_pairs = {'A': [1, 0, 0, 0], \n",
    "                          'C': [0, 1, 0, 0],\n",
    "                          'G': [0, 0, 1, 0],\n",
    "                          'T': [0, 0, 0, 1],\n",
    "                          'a': [1, 0, 0, 0],\n",
    "                          'c': [0, 1, 0, 0],\n",
    "                          'g': [0, 0, 1, 0],\n",
    "                          't': [0, 0, 0, 1],\n",
    "                          'n': [0, 0, 0, 0],\n",
    "                          'N': [0, 0, 0, 0]}\n",
    "    \"\"\"\n",
    "    char_seq = get_char_list(list(seq.cpu().numpy()))\n",
    "    complement_dict = {'A': [0, 0, 0, 1], \n",
    "                       'T': [1, 0, 0, 0],\n",
    "                       'C': [0, 0, 1, 0], \n",
    "                       'G': [0, 0, 1, 0]}\n",
    "    seq_comp = [complement_dict[base] for base in seq]\n",
    "    seq_comp = torch.Tensor(seq_comp).cuda()\n",
    "    y_hat1, y_hat2 = model(seq), model(seq_comp)\n",
    "    y_hat = (y_hat1. + y_hat2)/2\n",
    "    if y_hat >= thresh:\n",
    "        return 1\n",
    "    else: \n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15780 (12624, 3388, 4) (12624,) (3156, 3388, 4) (3156,)\n"
     ]
    }
   ],
   "source": [
    "data_x = pickle.load(open('/home/ubuntu/data/dmel_seq/data_x_nlc.np', 'rb'))\n",
    "data_y = pickle.load(open('/home/ubuntu/data/dmel_seq/data_y.np', 'rb'))\n",
    "\n",
    "train_x, train_y, val_x, val_y = data_split(data_x, data_y, seed = 157)\n",
    "\n",
    "train_x, val_x = pad_for_detector(train_x, 30), pad_for_detector(val_x, 30)\n",
    "train_x.shape, train_y.shape, val_x.shape, val_y.shape\n",
    "\n",
    "train_x, val_x = torch.from_numpy(train_x.transpose([0, 2, 1])).float(), torch.from_numpy(val_x.transpose([0, 2, 1])).float()\n",
    "train_y, val_y = torch.from_numpy(train_y).float(), torch.from_numpy(val_y).float()\n",
    "\n",
    "# Generate dataset for data loader\n",
    "train_dataset = data.TensorDataset(train_x, train_y)\n",
    "val_dataset = data.TensorDataset(val_x, val_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
