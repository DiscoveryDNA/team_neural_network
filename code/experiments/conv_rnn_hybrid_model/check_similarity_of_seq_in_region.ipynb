{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os, shutil, pickle, shelve\n",
    "from Bio import SeqIO\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.models import load_model, Model\n",
    "from keras.utils import plot_model\n",
    "from keras.models import Model, Sequential\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, MaxPooling1D, Flatten, Conv1D, LSTM, CuDNNLSTM, Bidirectional\n",
    "from keras.optimizers import RMSprop, Adam\n",
    "from keras.initializers import glorot_normal\n",
    "import keras\n",
    "#from utilities import sampling, one_hot_encoding, curtail, get_training_data, load_data, data_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample data from:\n",
    "data_dir = \"/home/ubuntu/group_volume/team_neural_network/data/input/3.24_species_only\"\n",
    "output_folder_path = \"../../../../temp/buffers/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "\n",
    "def sampling(src, dest, num_samples, seed=42):\n",
    "    \"\"\"  \n",
    "    Sample NUM_SAMPLES of data from SRC and copy them to DEST.\n",
    "    Set random seed to be SEED.\n",
    "    \"\"\"\n",
    "    np.random.seed(seed)\n",
    "    all_data_lst = np.array(os.listdir(src))\n",
    "    n = len(all_data_lst)\n",
    "    sample_indices = np.random.choice(np.arange(n), num_samples, replace=False)\n",
    "    sample_files = all_data_lst[sample_indices]\n",
    "\n",
    "    for file in sample_files:\n",
    "        shutil.copy(os.path.join(src, file),\n",
    "                    dest)\n",
    "    print('copied samples to {}'.format(dest))\n",
    "\n",
    "def pad_for_detector(input_x, kernel_size):\n",
    "    \"\"\" Pad the input matrix such that the (i, k) entry of the output \n",
    "        matrix is the score of motif detector k aligned to position i.\n",
    "    input_x has shape = (N, n, 4)\n",
    "    kernel_size has shape m\n",
    "    output has shape = (N, n + 2m - 2, 4)\n",
    "    \"\"\"\n",
    "    N, n, C = input_x.shape\n",
    "    pad_value, num_pad = 0.25, kernel_size - 1\n",
    "    pad_matrix = np.full((N, num_pad, C), pad_value)\n",
    "    return np.concatenate((pad_matrix, input_x, pad_matrix), axis=1)\n",
    "\n",
    "def one_hot_encoding(input_folder_path, output_file_path, \n",
    "                     max_file_num=10000):\n",
    "    \"\"\"\n",
    "    Given the data in INPUT_FOLDER_PATH, encode them and save\n",
    "    as a buffer called OUTPUT_FILE_PATH.\n",
    "    \n",
    "    Note: INPUT_FOLDER_PATH is a directory while OUTPUT_FILE_PATH\n",
    "          is a file.\n",
    "    \"\"\"\n",
    "    import os\n",
    "    # Use the following dictionary to perform the transformation\n",
    "    base_pairs = {'A': [1, 0, 0, 0], \n",
    "                  'C': [0, 1, 0, 0],\n",
    "                  'G': [0, 0, 1, 0],\n",
    "                  'T': [0, 0, 0, 1],\n",
    "                  'a': [1, 0, 0, 0],\n",
    "                  'c': [0, 1, 0, 0],\n",
    "                  'g': [0, 0, 1, 0],\n",
    "                  't': [0, 0, 0, 1],\n",
    "                  'n': [0, 0, 0, 0],\n",
    "                  'N': [0, 0, 0, 0]}\n",
    "\n",
    "    file_num_limit = max_file_num    # The maximum number of files to be decoded\n",
    "    file_count = 0\n",
    "\n",
    "    # Iterate through every file\n",
    "    all_regions = []\n",
    "    for file in os.listdir(input_folder_path):\n",
    "        # When the number of file decoded has reached the limit, stop\n",
    "        if file_count < file_num_limit:\n",
    "            #print(input_folder_path + file)\n",
    "            data = list(SeqIO.parse(input_folder_path + file,\"fasta\"))\n",
    "            for n in range(0, len(data)):\n",
    "                # Extract the header information\n",
    "                header = data[n].description.split('|')\n",
    "                descr = data[n].description\n",
    "                regionID = header[0]\n",
    "                expressed = header[1]\n",
    "                speciesID = header[2]\n",
    "                strand = header[3]\n",
    "                # Complement all sequences in the negative DNA strand\n",
    "    #             if strand == '-':\n",
    "    #                 # Using the syntax [e for e in base_pairs[n]] to create a new pointer for each position\n",
    "    #                 one_hot.append([descr, expressed, speciesID, [[e for e in base_pairs[n]] for n in data[n].seq.complement()]])\n",
    "    #             else:\n",
    "                all_regions.append([descr, expressed, speciesID, [[e for e in base_pairs[n]] for n in data[n].seq]])\n",
    "            file_count += 1\n",
    "\n",
    "    with open(output_file_path, mode=\"wb\") as output:\n",
    "        print(\"save to {}\".format(output_file_path))\n",
    "        pickle.dump(all_regions, output)\n",
    "    return all_regions\n",
    "\n",
    "def curtail(lst, read_len):\n",
    "    \"\"\" A helper function of get_training_data\n",
    "    \"\"\"\n",
    "    if len(lst) > read_len:\n",
    "        lst = lst[:read_len]\n",
    "    else:\n",
    "        for i in range(read_len - len(lst)):\n",
    "            lst.append([0, 0, 0, 0])\n",
    "    return lst\n",
    "\n",
    "def get_training_data(input_data, output_folder_path,\n",
    "                      max_len, train_x_name, train_y_name):\n",
    "    \"\"\" \n",
    "    Convert INPUT_DATA to ready-to-be-fed training data and \n",
    "        corresponding labels.\n",
    "    Save them to OUTPUT_FOLDER_PATH with name TRAIN_X_NAME and\n",
    "        TRAIN_Y_NAME.\n",
    "    INPUT_DATA is directly generated by the function one_hot_encoding. \n",
    "    \"\"\"\n",
    "    train_x, train_y = [], []\n",
    "    for region in input_data:\n",
    "        y, x = int(region[1]), region[3]\n",
    "        x = curtail(x, max_len)  # Curtail\n",
    "        x = np.array(x).flatten() # Flatten\n",
    "        x = x.reshape((1000, 4)) # Reshape\n",
    "        train_x.append(x)\n",
    "        train_y.append(y)\n",
    "\n",
    "    train_x, train_y = np.array(train_x), np.array(train_y)\n",
    "\n",
    "    print(train_x.shape, train_y.shape)\n",
    "\n",
    "    with open(os.path.join(output_folder_path, train_x_name), mode=\"wb\") as output:\n",
    "        print(\"save to {}\".format(os.path.join(output_folder_path, train_x_name)))\n",
    "        pickle.dump(train_x, output)\n",
    "\n",
    "    with open(os.path.join(output_folder_path, train_y_name), mode=\"wb\") as output:\n",
    "        print(\"save to {}\".format(os.path.join(output_folder_path, train_y_name)))\n",
    "        pickle.dump(train_y, output)\n",
    "    return train_x, train_y\n",
    "\n",
    "def data_split(data_x, data_y, val_split=0.2, seed=42):\n",
    "    \"\"\"\n",
    "    Given totally N data, randomly sample N*VAL_SPLIT of them \n",
    "        to form validation data.\n",
    "    Set random seed to SEED.\n",
    "    \"\"\"\n",
    "    # Split it into training and validation data sets\n",
    "    N = data_x.shape[0]\n",
    "    num_val = int(N * val_split)\n",
    "\n",
    "    np.random.seed(seed)\n",
    "    val_indices = np.random.choice(np.arange(N), num_val, replace=False)\n",
    "    train_indices = np.arange(N)[~np.isin(np.arange(N), val_indices)]\n",
    "\n",
    "    train_x, train_y = data_x[train_indices], data_y[train_indices]\n",
    "    val_x, val_y = data_x[val_indices], data_y[val_indices]\n",
    "\n",
    "    print(N, train_x.shape, train_y.shape, val_x.shape, val_y.shape)\n",
    "    return train_x, train_y, val_x, val_y\n",
    "\n",
    "\n",
    "def dianostic_plots(train_acc, train_loss, val_train_acc, val_loss):\n",
    "    \"\"\"  Plot dianostic plots of a model:\n",
    "    Plot 1: Traning loss & validation loss against epochs\n",
    "    Plot 2: Training acc & validation acc against epochs\n",
    "    \"\"\"\n",
    "    epochs = range(1, len(train_acc) + 1)\n",
    "\n",
    "    plt.plot(epochs, train_acc, '-', label='Training train_accuracy')\n",
    "    plt.plot(epochs, val_train_acc, '-', label='Validation Accuracy')\n",
    "    plt.title('Training and Validation Accuracy')\n",
    "    plt.xlabel('epoches')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(epochs, loss, '-', label='Training Loss')\n",
    "    plt.plot(epochs, val_loss, '-', label='Validation Loss')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.xlabel('epoches')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -r /home/ubuntu/data/temp/train\n",
    "!mkdir /home/ubuntu/data/temp/train\n",
    "!rm -r /home/ubuntu/data/temp/val\n",
    "!mkdir /home/ubuntu/data/temp/val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "copied training samples to /home/ubuntu/data/temp/train/\n",
      "copied validation samples to /home/ubuntu/data/temp/val/\n",
      "save to ../../../../temp/buffers/train.data\n",
      "save to ../../../../temp/buffers/val.data\n",
      "(12000, 1000, 4) (12000,)\n",
      "save to ../../../../temp/buffers/train_x.data\n",
      "save to ../../../../temp/buffers/train_y.data\n",
      "(2400, 1000, 4) (2400,)\n",
      "save to ../../../../temp/buffers/val_x.data\n",
      "save to ../../../../temp/buffers/val_y.data\n"
     ]
    }
   ],
   "source": [
    "# Sample training and validation data\n",
    "# Make sure that they don't have intersection.\n",
    "all_data_lst = np.array(os.listdir(data_dir))\n",
    "n = len(all_data_lst)\n",
    "train_files = all_data_lst[:500]\n",
    "num_val = 100\n",
    "val_indices = np.random.choice(np.arange(500, n), num_val, replace = False)\n",
    "val_files = all_data_lst[val_indices]\n",
    "\n",
    "train_dest = '/home/ubuntu/data/temp/train/'\n",
    "for file in train_files:\n",
    "    shutil.copy(os.path.join(data_dir, file),\n",
    "                          train_dest)\n",
    "print('copied training samples to {}'.format(train_dest))\n",
    "\n",
    "val_dest = '/home/ubuntu/data/temp/val/'\n",
    "for file in val_files:\n",
    "    shutil.copy(os.path.join(data_dir, file),\n",
    "                          val_dest)\n",
    "print('copied validation samples to {}'.format(val_dest))\n",
    "\n",
    "# Preprocess train and val data so that they are ready to be fed to models\n",
    "train_output_path = os.path.join(output_folder_path, 'train.data')\n",
    "val_output_path = os.path.join(output_folder_path, 'val.data')\n",
    "\n",
    "train_regions = one_hot_encoding(train_dest, train_output_path)\n",
    "val_regions = one_hot_encoding(val_dest, val_output_path)\n",
    "train_x, train_y = get_training_data(train_regions, output_folder_path,\n",
    "                                   max_len = 1000, \n",
    "                                   train_x_name = 'train_x.data', \n",
    "                                   train_y_name = 'train_y.data')\n",
    "val_x, val_y = get_training_data(val_regions, output_folder_path,\n",
    "                                   max_len = 1000, \n",
    "                                   train_x_name = 'val_x.data', \n",
    "                                   train_y_name = 'val_y.data')\n",
    "# Pad for motif detectors\n",
    "train_x, val_x = pad_for_detector(train_x, 15), pad_for_detector(val_x, 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "和# Define model:\n",
    "class HybridModel:\n",
    "    def __init__(self, K, M, input_length, rnn_size, config):\n",
    "        # Extract configaration of the model:\n",
    "        pool_size, strides = config['pool_size'], config['strides']\n",
    "        dr1, dr2 = config['dr1'], config['dr2'] # dropout rates\n",
    "        d = config['d'] # size of dense layers\n",
    "        optimizer = config['opt']\n",
    "        learning_rate = config['learning_rate']\n",
    "        is_training = config['is_training'] # to control the dropout layers\n",
    "        \n",
    "        # Create the placeholders for the inputs:\n",
    "        self.input = tf.placeholder(tf.float32, shape=[None, input_length, 4])\n",
    "        self.targets = tf.placeholder(tf.int32, shape=[None, ])\n",
    "        \n",
    "        # Define layers for the model:\n",
    "        self.K = K # number of filters\n",
    "        \n",
    "        self.conv = tf.layers.Conv1D(K, M,\n",
    "                                     strides=1, padding='valid',\n",
    "                                     use_bias=True, name='conv')\n",
    "        self.lm_cell_fw = tf.nn.rnn_cell.LSTMCell(num_units = rnn_size, dtype = tf.float32, name='lm_cell_fw')\n",
    "        self.lm_cell_bw = tf.nn.rnn_cell.LSTMCell(num_units = rnn_size, dtype = tf.float32, name='lm_cell_bw')\n",
    "        \n",
    "        # Feed in input\n",
    "        #print(self.input.shape)\n",
    "        self.activations = tf.nn.relu(self.conv(self.input))\n",
    "        #print(self.activations.shape)\n",
    "        outputs = tf.layers.max_pooling1d(self.activations, \n",
    "                                          pool_size=pool_size,\n",
    "                                          strides=strides)\n",
    "        outputs = tf.layers.dropout(outputs,\n",
    "                                    rate=dr1,\n",
    "                                    training=is_training)\n",
    "        #print(outputs.shape)\n",
    "        \n",
    "        outputs, states = tf.nn.bidirectional_dynamic_rnn(self.lm_cell_fw,\n",
    "                                                          self.lm_cell_bw,\n",
    "                                                          outputs, dtype = tf.float32)\n",
    "        outputs = tf.concat(outputs, axis=2)\n",
    "        #print(outputs.shape)\n",
    "        outputs = tf.layers.dropout(outputs,\n",
    "                                    rate=dr2,\n",
    "                                    training=is_training)\n",
    "        \n",
    "        outputs = tf.nn.relu(tf.layers.dense(outputs, d, name='dense1'))\n",
    "        #print(outputs.shape)\n",
    "        output_logits = tf.layers.dense(outputs, 1, name='dense2')\n",
    "        #print(output_logits.shape)\n",
    "        \n",
    "        self.loss = tf.losses.sparse_softmax_cross_entropy(self.targets, output_logits)\n",
    "        #print(self.loss.shape)\n",
    "        self.global_step = tf.train.get_or_create_global_step()\n",
    "        self.train_op = optimizer.minimize(self.loss)\n",
    "        self.saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-6-f19474ae39f1>:22: LSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\n",
      "WARNING:tensorflow:From /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From <ipython-input-6-f19474ae39f1>:31: max_pooling1d (from tensorflow.python.layers.pooling) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.max_pooling1d instead.\n",
      "WARNING:tensorflow:From <ipython-input-6-f19474ae39f1>:34: dropout (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.dropout instead.\n",
      "WARNING:tensorflow:From /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/keras/layers/core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From <ipython-input-6-f19474ae39f1>:39: bidirectional_dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.Bidirectional(keras.layers.RNN(cell))`, which is equivalent to this API\n",
      "WARNING:tensorflow:From /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/ops/rnn.py:443: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n",
      "WARNING:tensorflow:From <ipython-input-6-f19474ae39f1>:46: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.dense instead.\n",
      "WARNING:tensorflow:From /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/ops/losses/losses_impl.py:209: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph() # Reset the computational graph before defining a new model.\n",
    "\n",
    "optimizers = {'adam': tf.train.AdamOptimizer(learning_rate=1e-3),\n",
    "              'rmsprop': tf.train.RMSPropOptimizer(learning_rate=1e-3)}\n",
    "opt = optimizers['rmsprop']\n",
    "model_config = {'pool_size': 5, 'strides': 5, 'dr1': 0.6, 'dr2': 0.7, 'd': 20, \n",
    "          'opt': opt, 'learning_rate': 1e-2, 'is_training': True}\n",
    "model = HybridModel(K=30, M=15, input_length=1028, rnn_size=15, config=model_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define training function:\n",
    "def train(model, train_x, train_y, val_x, val_y, config, verbose=True, print_every=10):\n",
    "    epochs, iteration, output_path = config['epochs'], config['iteration'], config['output_path']\n",
    "    val_loss_record = []\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        for epoch in range(epochs):\n",
    "            print(\"epoch {}\".format(epoch))\n",
    "            for iters in range(iteration):\n",
    "                # Here is how you obtain a batch:\n",
    "                batch_size = train_x.shape[0] // iteration\n",
    "                #print(batch_size)\n",
    "                train_indices = np.random.choice(np.arange(train_x.shape[0]), batch_size, replace=False)\n",
    "                sub_train_x, sub_train_y = train_x[train_indices, :, :], train_y[train_indices]\n",
    "                feed = {model.input: sub_train_x, model.targets: sub_train_y}\n",
    "                model.is_training = True\n",
    "                step, train_loss, _ = sess.run([model.global_step, model.loss, model.train_op], feed_dict=feed)\n",
    "                if verbose:\n",
    "                    if iters % print_every == 0:\n",
    "                        print(\"    iteration {}, train_loss: {}\".format(iters, train_loss))\n",
    "            val_indices = np.random.choice(np.arange(val_x.shape[0]), 2400, replace = False)\n",
    "            sub_val_x, sub_val_y = val_x[val_indices, :, :], val_y[val_indices]\n",
    "            feed = {model.input: sub_val_x, model.targets: sub_val_y}\n",
    "            model.is_training = False\n",
    "            val_loss = sess.run([model.loss], feed_dict=feed)\n",
    "            val_loss_record.append(val_loss)\n",
    "            print(\"validation loss: {}\".format(val_loss))\n",
    "        # Here is how you save the model weights\n",
    "        model.saver.save(sess, output_path)\n",
    "    return val_loss_record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0\n",
      "    iteration 0, train_loss: 5.316475868225098\n",
      "    iteration 10, train_loss: 5.3064680099487305\n",
      "    iteration 20, train_loss: 5.302048683166504\n",
      "    iteration 30, train_loss: 5.244299411773682\n",
      "    iteration 40, train_loss: 5.192083835601807\n",
      "    iteration 50, train_loss: 5.112571716308594\n",
      "    iteration 60, train_loss: 4.954061985015869\n",
      "    iteration 70, train_loss: 4.788250923156738\n",
      "    iteration 80, train_loss: 4.484421730041504\n",
      "    iteration 90, train_loss: 4.041566848754883\n",
      "validation loss: [3.4941983]\n",
      "epoch 1\n",
      "    iteration 0, train_loss: 3.5287530422210693\n",
      "    iteration 10, train_loss: 3.017548084259033\n",
      "    iteration 20, train_loss: 2.5676815509796143\n",
      "    iteration 30, train_loss: 2.2607271671295166\n",
      "    iteration 40, train_loss: 1.9518146514892578\n",
      "    iteration 50, train_loss: 1.7613474130630493\n",
      "    iteration 60, train_loss: 1.5250385999679565\n",
      "    iteration 70, train_loss: 1.3509308099746704\n",
      "    iteration 80, train_loss: 1.257437825202942\n",
      "    iteration 90, train_loss: 1.1407610177993774\n",
      "validation loss: [1.091132]\n",
      "epoch 2\n",
      "    iteration 0, train_loss: 1.0951988697052002\n",
      "    iteration 10, train_loss: 1.0773593187332153\n",
      "    iteration 20, train_loss: 0.9773648381233215\n",
      "    iteration 30, train_loss: 0.9333990812301636\n",
      "    iteration 40, train_loss: 0.8961750268936157\n",
      "    iteration 50, train_loss: 0.8686762452125549\n",
      "    iteration 60, train_loss: 0.8278119564056396\n",
      "    iteration 70, train_loss: 0.8419800400733948\n",
      "    iteration 80, train_loss: 0.7669342160224915\n",
      "    iteration 90, train_loss: 0.7935888767242432\n",
      "validation loss: [0.80553764]\n",
      "epoch 3\n",
      "    iteration 0, train_loss: 0.8025377988815308\n",
      "    iteration 10, train_loss: 0.7532624006271362\n",
      "    iteration 20, train_loss: 0.7969720959663391\n",
      "    iteration 30, train_loss: 0.7409849762916565\n",
      "    iteration 40, train_loss: 0.7633826732635498\n",
      "    iteration 50, train_loss: 0.7615658640861511\n",
      "    iteration 60, train_loss: 0.7686342597007751\n",
      "    iteration 70, train_loss: 0.7630475163459778\n",
      "    iteration 80, train_loss: 0.7270181179046631\n",
      "    iteration 90, train_loss: 0.7562175989151001\n",
      "validation loss: [0.76793826]\n",
      "epoch 4\n",
      "    iteration 0, train_loss: 0.7233548760414124\n",
      "    iteration 10, train_loss: 0.7319936752319336\n",
      "    iteration 20, train_loss: 0.6800941824913025\n",
      "    iteration 30, train_loss: 0.7089623808860779\n",
      "    iteration 40, train_loss: 0.7001160979270935\n",
      "    iteration 50, train_loss: 0.7234756946563721\n",
      "    iteration 60, train_loss: 0.773571252822876\n",
      "    iteration 70, train_loss: 0.7287524938583374\n",
      "    iteration 80, train_loss: 0.6590582728385925\n",
      "    iteration 90, train_loss: 0.702932596206665\n",
      "validation loss: [0.74667287]\n",
      "epoch 5\n",
      "    iteration 0, train_loss: 0.6972037553787231\n",
      "    iteration 10, train_loss: 0.7121552228927612\n",
      "    iteration 20, train_loss: 0.72622150182724\n",
      "    iteration 30, train_loss: 0.6806094646453857\n",
      "    iteration 40, train_loss: 0.6979238986968994\n",
      "    iteration 50, train_loss: 0.6619425415992737\n",
      "    iteration 60, train_loss: 0.7234996557235718\n",
      "    iteration 70, train_loss: 0.7007811665534973\n",
      "    iteration 80, train_loss: 0.6831071376800537\n",
      "    iteration 90, train_loss: 0.6613232493400574\n",
      "validation loss: [0.7437829]\n",
      "epoch 6\n",
      "    iteration 0, train_loss: 0.6664555668830872\n",
      "    iteration 10, train_loss: 0.6973344087600708\n",
      "    iteration 20, train_loss: 0.7059877514839172\n",
      "    iteration 30, train_loss: 0.669189453125\n",
      "    iteration 40, train_loss: 0.6883293390274048\n",
      "    iteration 50, train_loss: 0.6670529842376709\n",
      "    iteration 60, train_loss: 0.670251727104187\n",
      "    iteration 70, train_loss: 0.679151177406311\n",
      "    iteration 50, train_loss: 0.6139538884162903\n",
      "    iteration 60, train_loss: 0.6330219507217407\n",
      "    iteration 70, train_loss: 0.6743590235710144\n",
      "    iteration 80, train_loss: 0.675387978553772\n",
      "    iteration 90, train_loss: 0.6364380717277527\n",
      "validation loss: [0.75781924]\n",
      "epoch 10\n",
      "    iteration 0, train_loss: 0.6553108096122742\n",
      "    iteration 10, train_loss: 0.6292659640312195\n",
      "    iteration 20, train_loss: 0.6618996858596802\n",
      "    iteration 30, train_loss: 0.6142454743385315\n",
      "    iteration 40, train_loss: 0.6309539675712585\n",
      "    iteration 50, train_loss: 0.6208643317222595\n",
      "    iteration 60, train_loss: 0.6630452275276184\n",
      "    iteration 70, train_loss: 0.6693059206008911\n",
      "    iteration 80, train_loss: 0.6522084474563599\n",
      "    iteration 90, train_loss: 0.6234152913093567\n",
      "validation loss: [0.767828]\n",
      "epoch 11\n",
      "    iteration 0, train_loss: 0.5680552124977112\n",
      "    iteration 10, train_loss: 0.6144631505012512\n",
      "    iteration 20, train_loss: 0.6192344427108765\n",
      "    iteration 30, train_loss: 0.6555488109588623\n",
      "    iteration 40, train_loss: 0.6685646176338196\n",
      "    iteration 50, train_loss: 0.6542105078697205\n",
      "    iteration 60, train_loss: 0.5601991415023804\n",
      "    iteration 70, train_loss: 0.6231706142425537\n",
      "    iteration 80, train_loss: 0.6578443050384521\n",
      "    iteration 90, train_loss: 0.5945529341697693\n",
      "validation loss: [0.7882567]\n",
      "epoch 12\n",
      "    iteration 0, train_loss: 0.6334193348884583\n",
      "    iteration 10, train_loss: 0.6939929723739624\n",
      "    iteration 20, train_loss: 0.6183626055717468\n",
      "    iteration 30, train_loss: 0.5583469867706299\n",
      "    iteration 40, train_loss: 0.649371862411499\n",
      "    iteration 50, train_loss: 0.600835919380188\n",
      "    iteration 60, train_loss: 0.6339182257652283\n",
      "    iteration 70, train_loss: 0.6515223979949951\n",
      "    iteration 80, train_loss: 0.6310877203941345\n",
      "    iteration 90, train_loss: 0.6402854919433594\n",
      "validation loss: [0.78597176]\n",
      "epoch 13\n",
      "    iteration 0, train_loss: 0.5953759551048279\n",
      "    iteration 10, train_loss: 0.5848149061203003\n",
      "    iteration 20, train_loss: 0.6121404767036438\n",
      "    iteration 30, train_loss: 0.6108196377754211\n",
      "    iteration 40, train_loss: 0.5898379683494568\n",
      "    iteration 50, train_loss: 0.6355561017990112\n",
      "    iteration 60, train_loss: 0.6491444110870361\n",
      "    iteration 70, train_loss: 0.6324475407600403\n",
      "    iteration 80, train_loss: 0.6158462762832642\n",
      "    iteration 90, train_loss: 0.5930662751197815\n",
      "validation loss: [0.7475242]\n",
      "epoch 14\n",
      "    iteration 0, train_loss: 0.6708657145500183\n",
      "    iteration 10, train_loss: 0.5827577114105225\n",
      "    iteration 20, train_loss: 0.644805371761322\n",
      "    iteration 30, train_loss: 0.6535254716873169\n",
      "    iteration 40, train_loss: 0.5943357348442078\n",
      "    iteration 50, train_loss: 0.594918966293335\n",
      "    iteration 60, train_loss: 0.567791759967804\n",
      "    iteration 70, train_loss: 0.6320418119430542\n",
      "    iteration 80, train_loss: 0.61342853307724\n",
      "    iteration 90, train_loss: 0.6263601183891296\n",
      "validation loss: [0.7908417]\n",
      "epoch 15\n",
      "    iteration 0, train_loss: 0.6380596160888672\n",
      "    iteration 10, train_loss: 0.6282740235328674\n",
      "    iteration 20, train_loss: 0.7108511328697205\n",
      "    iteration 30, train_loss: 0.6407496929168701\n",
      "    iteration 40, train_loss: 0.5869537591934204\n",
      "    iteration 50, train_loss: 0.6717491149902344\n",
      "    iteration 60, train_loss: 0.7495696544647217\n",
      "    iteration 70, train_loss: 0.6085382103919983\n",
      "    iteration 80, train_loss: 0.637143075466156\n",
      "    iteration 90, train_loss: 0.6239439845085144\n",
      "validation loss: [0.8166701]\n",
      "epoch 16\n",
      "    iteration 0, train_loss: 0.624923825263977\n",
      "    iteration 10, train_loss: 0.5866079926490784\n",
      "    iteration 20, train_loss: 0.6677649021148682\n",
      "    iteration 30, train_loss: 0.5925537347793579\n",
      "    iteration 40, train_loss: 0.6335605382919312\n",
      "    iteration 50, train_loss: 0.6299447417259216\n",
      "    iteration 60, train_loss: 0.6774860620498657\n",
      "    iteration 70, train_loss: 0.6776304841041565\n",
      "    iteration 80, train_loss: 0.6194342374801636\n",
      "    iteration 90, train_loss: 0.6162540316581726\n",
      "validation loss: [0.8113423]\n",
      "epoch 17\n",
      "    iteration 0, train_loss: 0.578640878200531\n",
      "    iteration 10, train_loss: 0.6153110265731812\n",
      "    iteration 20, train_loss: 0.6051714420318604\n",
      "    iteration 30, train_loss: 0.6269853115081787\n",
      "    iteration 40, train_loss: 0.5743637084960938\n",
      "    iteration 50, train_loss: 0.5975841879844666\n",
      "    iteration 60, train_loss: 0.5828112363815308\n",
      "    iteration 70, train_loss: 0.6305658221244812\n",
      "    iteration 80, train_loss: 0.6132868528366089\n",
      "    iteration 90, train_loss: 0.6914779543876648\n",
      "validation loss: [0.849587]\n",
      "epoch 18\n",
      "    iteration 0, train_loss: 0.5737289190292358\n",
      "    iteration 10, train_loss: 0.5940548181533813\n",
      "    iteration 20, train_loss: 0.6133483648300171\n",
      "    iteration 30, train_loss: 0.5950480103492737\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    iteration 40, train_loss: 0.5328354239463806\n",
      "    iteration 50, train_loss: 0.7072104215621948\n",
      "    iteration 60, train_loss: 0.582050621509552\n",
      "    iteration 70, train_loss: 0.6450433135032654\n",
      "    iteration 80, train_loss: 0.6060560941696167\n",
      "    iteration 90, train_loss: 0.5870246291160583\n",
      "validation loss: [0.8410568]\n",
      "epoch 19\n",
      "    iteration 0, train_loss: 0.6380292773246765\n",
      "    iteration 10, train_loss: 0.5647208094596863\n",
      "    iteration 20, train_loss: 0.5667112469673157\n",
      "    iteration 30, train_loss: 0.5864130258560181\n",
      "    iteration 40, train_loss: 0.5818300247192383\n",
      "    iteration 50, train_loss: 0.6338233947753906\n",
      "    iteration 60, train_loss: 0.5850740075111389\n",
      "    iteration 70, train_loss: 0.5984933376312256\n",
      "    iteration 80, train_loss: 0.6231352686882019\n",
      "    iteration 90, train_loss: 0.6800747513771057\n",
      "validation loss: [0.8884973]\n",
      "epoch 20\n",
      "    iteration 0, train_loss: 0.6271294951438904\n",
      "    iteration 10, train_loss: 0.5974477529525757\n",
      "    iteration 20, train_loss: 0.5340130925178528\n",
      "    iteration 30, train_loss: 0.6887209415435791\n",
      "    iteration 40, train_loss: 0.6027240753173828\n",
      "    iteration 50, train_loss: 0.6421302556991577\n",
      "    iteration 60, train_loss: 0.6467610597610474\n",
      "    iteration 70, train_loss: 0.6425591707229614\n",
      "    iteration 80, train_loss: 0.6081082820892334\n",
      "    iteration 90, train_loss: 0.5844619870185852\n",
      "validation loss: [0.84021544]\n",
      "epoch 21\n",
      "    iteration 0, train_loss: 0.6257889866828918\n",
      "    iteration 10, train_loss: 0.6329196691513062\n",
      "    iteration 20, train_loss: 0.6348478198051453\n",
      "    iteration 30, train_loss: 0.6313899755477905\n",
      "    iteration 40, train_loss: 0.5288383960723877\n",
      "    iteration 50, train_loss: 0.5906020998954773\n",
      "    iteration 60, train_loss: 0.560951292514801\n",
      "    iteration 70, train_loss: 0.6541569232940674\n",
      "    iteration 80, train_loss: 0.5815222263336182\n",
      "    iteration 90, train_loss: 0.5420226454734802\n",
      "validation loss: [0.8382986]\n",
      "epoch 22\n",
      "    iteration 0, train_loss: 0.6227113008499146\n",
      "    iteration 10, train_loss: 0.5952398777008057\n",
      "    iteration 20, train_loss: 0.5611360669136047\n",
      "    iteration 30, train_loss: 0.6106231808662415\n",
      "    iteration 40, train_loss: 0.5875489711761475\n",
      "    iteration 50, train_loss: 0.5987406373023987\n",
      "    iteration 60, train_loss: 0.62376469373703\n",
      "    iteration 70, train_loss: 0.6283194422721863\n",
      "    iteration 80, train_loss: 0.6373729705810547\n",
      "    iteration 90, train_loss: 0.587967038154602\n",
      "validation loss: [0.82141984]\n",
      "epoch 23\n",
      "    iteration 0, train_loss: 0.5738271474838257\n",
      "    iteration 10, train_loss: 0.6014230251312256\n",
      "    iteration 20, train_loss: 0.5693585276603699\n",
      "    iteration 30, train_loss: 0.5484582185745239\n",
      "    iteration 40, train_loss: 0.5582523345947266\n",
      "    iteration 50, train_loss: 0.5951968431472778\n",
      "    iteration 60, train_loss: 0.5907213091850281\n",
      "    iteration 70, train_loss: 0.6678531169891357\n",
      "    iteration 80, train_loss: 0.5605900883674622\n",
      "    iteration 90, train_loss: 0.5308507084846497\n",
      "validation loss: [0.828043]\n",
      "epoch 24\n",
      "    iteration 0, train_loss: 0.590214192867279\n",
      "    iteration 10, train_loss: 0.6215816736221313\n",
      "    iteration 20, train_loss: 0.524422824382782\n",
      "    iteration 30, train_loss: 0.5308802127838135\n",
      "    iteration 40, train_loss: 0.5774440765380859\n",
      "    iteration 50, train_loss: 0.5481007099151611\n",
      "    iteration 60, train_loss: 0.5558719635009766\n",
      "    iteration 70, train_loss: 0.5939181447029114\n",
      "    iteration 80, train_loss: 0.751376211643219\n",
      "    iteration 90, train_loss: 0.6592050194740295\n",
      "validation loss: [0.8425576]\n",
      "epoch 25\n",
      "    iteration 0, train_loss: 0.5346538424491882\n",
      "    iteration 10, train_loss: 0.519959568977356\n",
      "    iteration 20, train_loss: 0.5588201880455017\n",
      "    iteration 30, train_loss: 0.6084454655647278\n",
      "    iteration 40, train_loss: 0.626020073890686\n",
      "    iteration 50, train_loss: 0.638135552406311\n",
      "    iteration 60, train_loss: 0.6237730383872986\n",
      "    iteration 70, train_loss: 0.5817497372627258\n",
      "    iteration 80, train_loss: 0.6564027070999146\n",
      "    iteration 90, train_loss: 0.6003597378730774\n",
      "validation loss: [0.83920157]\n",
      "epoch 26\n",
      "    iteration 0, train_loss: 0.5777974724769592\n",
      "    iteration 10, train_loss: 0.6136558055877686\n",
      "    iteration 20, train_loss: 0.660081684589386\n",
      "    iteration 30, train_loss: 0.53962641954422\n",
      "    iteration 40, train_loss: 0.6072338819503784\n",
      "    iteration 50, train_loss: 0.5340225100517273\n",
      "    iteration 60, train_loss: 0.4893668293952942\n",
      "    iteration 70, train_loss: 0.602851152420044\n",
      "    iteration 80, train_loss: 0.693555474281311\n",
      "    iteration 90, train_loss: 0.5811331868171692\n",
      "validation loss: [0.8672233]\n",
      "epoch 27\n",
      "    iteration 0, train_loss: 0.6016048192977905\n",
      "    iteration 10, train_loss: 0.5932546257972717\n",
      "    iteration 20, train_loss: 0.5832043886184692\n",
      "    iteration 30, train_loss: 0.5246027112007141\n",
      "    iteration 40, train_loss: 0.6476370692253113\n",
      "    iteration 50, train_loss: 0.6140590906143188\n",
      "    iteration 60, train_loss: 0.6188011169433594\n",
      "    iteration 70, train_loss: 0.5525799989700317\n",
      "    iteration 80, train_loss: 0.570600688457489\n",
      "    iteration 90, train_loss: 0.5661057829856873\n",
      "validation loss: [0.8407785]\n",
      "epoch 28\n",
      "    iteration 0, train_loss: 0.5643827319145203\n",
      "    iteration 10, train_loss: 0.5537406802177429\n",
      "    iteration 20, train_loss: 0.5524342060089111\n",
      "    iteration 30, train_loss: 0.5350266695022583\n",
      "    iteration 40, train_loss: 0.6074647903442383\n",
      "    iteration 50, train_loss: 0.5749892592430115\n",
      "    iteration 60, train_loss: 0.5277013778686523\n",
      "    iteration 70, train_loss: 0.6222678422927856\n",
      "    iteration 80, train_loss: 0.5334458947181702\n",
      "    iteration 90, train_loss: 0.6327968835830688\n",
      "validation loss: [0.8353654]\n",
      "epoch 29\n",
      "    iteration 0, train_loss: 0.5994991064071655\n",
      "    iteration 10, train_loss: 0.6073080897331238\n",
      "    iteration 20, train_loss: 0.5567752718925476\n",
      "    iteration 30, train_loss: 0.5978353023529053\n",
      "    iteration 40, train_loss: 0.7030004858970642\n",
      "    iteration 50, train_loss: 0.5639874935150146\n",
      "    iteration 60, train_loss: 0.5459834933280945\n",
      "    iteration 70, train_loss: 0.6751578450202942\n",
      "    iteration 80, train_loss: 0.5912061333656311\n",
      "    iteration 90, train_loss: 0.5517722964286804\n",
      "validation loss: [0.83862245]\n",
      "epoch 30\n",
      "    iteration 0, train_loss: 0.6214351654052734\n",
      "    iteration 10, train_loss: 0.6153269410133362\n",
      "    iteration 20, train_loss: 0.6098426580429077\n",
      "    iteration 30, train_loss: 0.5100232362747192\n",
      "    iteration 40, train_loss: 0.5957230925559998\n",
      "    iteration 50, train_loss: 0.755101203918457\n",
      "    iteration 60, train_loss: 0.5574937462806702\n",
      "    iteration 70, train_loss: 0.6275985836982727\n",
      "    iteration 80, train_loss: 0.5715628862380981\n",
      "    iteration 90, train_loss: 0.4828752875328064\n",
      "validation loss: [0.90620995]\n",
      "epoch 31\n",
      "    iteration 0, train_loss: 0.5253482460975647\n",
      "    iteration 10, train_loss: 0.5538983941078186\n",
      "    iteration 20, train_loss: 0.6177392601966858\n",
      "    iteration 30, train_loss: 0.5067062973976135\n",
      "    iteration 40, train_loss: 0.5069900751113892\n",
      "    iteration 50, train_loss: 0.5406678318977356\n",
      "    iteration 60, train_loss: 0.6589539647102356\n",
      "    iteration 70, train_loss: 0.5848909616470337\n",
      "    iteration 80, train_loss: 0.5502932071685791\n",
      "    iteration 90, train_loss: 0.5661525130271912\n",
      "validation loss: [0.8724129]\n",
      "epoch 32\n",
      "    iteration 0, train_loss: 0.6451166868209839\n",
      "    iteration 10, train_loss: 0.6298277378082275\n",
      "    iteration 20, train_loss: 0.5588143467903137\n",
      "    iteration 30, train_loss: 0.557827889919281\n",
      "    iteration 40, train_loss: 0.5753175020217896\n",
      "    iteration 50, train_loss: 0.641812264919281\n",
      "    iteration 60, train_loss: 0.5984556674957275\n",
      "    iteration 70, train_loss: 0.6005313396453857\n",
      "    iteration 80, train_loss: 0.600589394569397\n",
      "    iteration 90, train_loss: 0.5741313099861145\n",
      "validation loss: [0.8475212]\n",
      "epoch 33\n",
      "    iteration 0, train_loss: 0.6155416965484619\n",
      "    iteration 10, train_loss: 0.5957351326942444\n",
      "    iteration 20, train_loss: 0.5288150906562805\n",
      "    iteration 30, train_loss: 0.6230296492576599\n",
      "    iteration 40, train_loss: 0.6142832636833191\n",
      "    iteration 50, train_loss: 0.4910857379436493\n",
      "    iteration 60, train_loss: 0.5670285820960999\n",
      "    iteration 70, train_loss: 0.5447873473167419\n",
      "    iteration 80, train_loss: 0.5724374055862427\n",
      "    iteration 90, train_loss: 0.6427282691001892\n",
      "validation loss: [0.8277387]\n",
      "epoch 34\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    iteration 0, train_loss: 0.5614534616470337\n",
      "    iteration 10, train_loss: 0.5596186518669128\n",
      "    iteration 20, train_loss: 0.5345996618270874\n",
      "    iteration 30, train_loss: 0.5178002715110779\n",
      "    iteration 40, train_loss: 0.5544251799583435\n",
      "    iteration 50, train_loss: 0.5298944711685181\n",
      "    iteration 60, train_loss: 0.4668157994747162\n",
      "    iteration 70, train_loss: 0.5077944993972778\n",
      "    iteration 80, train_loss: 0.6658449769020081\n",
      "    iteration 90, train_loss: 0.5721036195755005\n",
      "validation loss: [0.86555916]\n"
     ]
    }
   ],
   "source": [
    "# Save the model for later analysis\n",
    "save_model_path = '/home/ubuntu/data/team_neural_network/code/models'\n",
    "model_name = 'hybrid_net-tensorflow.h5'\n",
    "output_path = os.path.join(save_model_path, model_name)\n",
    "training_config = {'epochs': 35, 'iteration': 100, 'output_path': output_path}\n",
    "\n",
    "val_loss_record = train(model, train_x, train_y, val_x, val_y, training_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7ff97859e5c0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAEWCAYAAACdaNcBAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAAIABJREFUeJzt3XmUXGWd//H3t6p6SVd19k7IHpYgCdkT44JRIuAgCBEHEUZU3BgZf8dxzujP5ee4HR2PHgYYdQ6OjgsogsgmCogBMwIuYBKSEJJAAgTT6dDpJHSn962+vz/u7U6l00ul091VdfvzOqdO37p1695vbpJPP/XcW89j7o6IiERLLNcFiIjI0FO4i4hEkMJdRCSCFO4iIhGkcBcRiSCFu4hIBCncJa+Z2VwzczNLhM8fMrMPZLPtII71eTP7n5OpVyRfKNxlWJnZb83sq72sX2tmr5xoELv72939liGo61wzq+yx739394+c7L57OdY1ZvbEUO9XpD8KdxlutwBXm5n1WP8+4DZ378hBTSKRp3CX4XYfMAlY3bXCzCYA7wBuDZ9fbGZPm9kRM9trZl/ua2dm9r9m9pFwOW5m15vZQTN7Ebi4x7YfNLMdZlZvZi+a2T+G65PAQ8B0M2sIH9PN7Mtm9rOM919qZs+aWW143PkZr+0xs0+Z2VYzqzOzX5hZ6YmenPC495vZYTPbbWYfzXhtlZltCM9LtZndEK4vNbOfmdmhsLa/mtnUEz22RJvCXYaVuzcDdwLvz1h9BbDT3beEzxvD18cTBPR1ZvbOLHb/UYJfEsuAlcDlPV4/EL4+FvggcKOZLXf3RuDtQJW7p8JHVeYbzexM4Hbgk0AF8CDwazMr7vHnuBA4FVgMXJNFzT3dAVQC08P6/93M3hq+9p/Af7r7WOB0gvMI8AFgHDCL4Bfnx4DmQRxbIkzhLiPhFuDyjJbt+8N1ALj7/7r7M+6edvetBKH6liz2ewVwk7vvdffDwDcyX3T3B9z9BQ/8AfgdGZ8gBvAe4AF3X+fu7cD1wBjgjRnbfNvdq8Jj/xpYmuW+ATCzWcA5wGfcvcXdNwP/w9FfhO3AGWY22d0b3P0vGesnAWe4e6e7b3T3IydybIk+hbsMO3d/AjgIvNPMTgdWAT/vet3MXmdm682sxszqCFqik7PY9XRgb8bzlzNfNLO3m9lfwi6PWuCiLPfbte/u/bl7OjzWjIxtXslYbgJSWe478xiH3b0+Y93LGcf4MHAmsDPsenlHuP6nwMPAHWZWZWbfMrOiEzy2RJzCXUbKrQQt0quBh929OuO1nwP3A7PcfRzwPaDnBdje7Cfomugyu2vBzEqAuwla3FPdfTxB10rXfgcaDrUKmJOxPwuPtS+LurJVBUw0s/KMdbO7juHuu9z9KmAK8E3gLjNLunu7u3/F3RcQfJJ4B8d2e4ko3GXE3AqcT9BP3vNWxnKCFmyLma0C/iHLfd4JfMLMZoYXaT+b8VoxUALUAB1m9nbgbRmvVwOTzGxcP/u+2MzOC1vF/wq0An/KsraeLLwQ2v1w973h/r4RrltM0Fr/WfiGq82sIvzUUBvuJ21ma8xskZnFgSME3TTpQdYlEaVwlxHh7nsIgixJ0ErP9E/AV82sHvgiRy8cDuQHBN0TW4BNwD0Zx6sHPhHu61WCXxj3Z7y+k6Bv/8XwjpPpPep9juBTxncIupQuAS5x97Ysa+vpjQQXPbsf4T3+VwFzCVrx9wJfcvdHwvdcCDxrZg0EF1evDC9QnwLcRRDsO4A/EHTViHQzTdYhIhI9armLiESQwl1EJIIU7iIiEaRwFxGJoEENjToUJk+e7HPnzs3V4UVECtLGjRsPunvFQNvlLNznzp3Lhg0bcnV4EZGCZGYvD7yVumVERCJJ4S4iEkEKdxGRCMpZn7uIjIz29nYqKytpaWnJdSlyAkpLS5k5cyZFRYMb8FPhLhJxlZWVlJeXM3fuXI6f7VDykbtz6NAhKisrOfXUUwe1jwG7ZcLR6p4ysy3hlGNf6WWba8KxuDeHjyGfZFhEBqelpYVJkyYp2AuImTFp0qST+rSVTcu9FXiruzeEQ58+YWYPZcwK0+UX7v5/Bl2JiAwbBXvhOdm/swFb7uEUZQ3h06LwkbOhJJ97pZ7/+N1zHG4c7MirIiLRl9XdMuEs85sJJhxe5+5P9rLZ34czwd8Vzg3Z236uDWdz31BTUzOogl+saeA7v9/NK3W6OCSS79asWcPDDz98zLqbbrqJ6667rt/3pVLBjIVVVVVcfnnPec8D55577oBfhLzppptoamrqfn7RRRdRW1vbzzuy8+Uvf5nrr7/+pPcznLIK93AS3qXATGCVmS3sscmvgbnuvhhYx/Ez7XTt5/vuvtLdV1ZUDPjt2V4lS4KepMa2jkG9X0RGzlVXXcUdd9xxzLo77riDq666Kqv3T58+nbvuumvQx+8Z7g8++CDjx48f9P4KyQnd5+7utcB6ghliMtcfcvfW8On/ACuGprzjpUqDcG9oVbiL5LvLL7+cBx54gLa2oBt1z549VFVVsXr1ahoaGjjvvPNYvnw5ixYt4le/+tVx79+zZw8LFwZtyebmZq688krmz5/PZZddRnNzc/d21113HStXruTss8/mS1/6EgDf/va3qaqqYs2aNaxZswYIhj05ePAgADfccAMLFy5k4cKF3HTTTd3Hmz9/Ph/96Ec5++yzedvb3nbMcQbS2z4bGxu5+OKLWbJkCQsXLuQXv/gFAJ/97GdZsGABixcv5lOf+tQJnddsDHhB1cwqgHZ3rzWzMcAFBJP1Zm4zzd33h08vJZj6a1ikwpZ7Q4vCXeREfeXXz7K96siQ7nPB9LF86ZKze31t4sSJrFq1ioceeoi1a9dyxx13cMUVV2BmlJaWcu+99zJ27FgOHjzI61//ei699NI+LyTefPPNlJWVsWPHDrZu3cry5cu7X/v617/OxIkT6ezs5LzzzmPr1q184hOf4IYbbmD9+vVMnjz5mH1t3LiRH//4xzz55JO4O6973et4y1vewoQJE9i1axe33347P/jBD7jiiiu4++67ufrqqwc8D33t88UXX2T69Ok88MADANTV1XHo0CHuvfdedu7ciZkNSVdRT9m03KcB681sK/BXgj7335jZV83s0nCbT4S3SW4hmLfymiGvNNTdLaOWu0hByOyayeyScXc+//nPs3jxYs4//3z27dtHdXV1n/t57LHHukN28eLFLF68uPu1O++8k+XLl7Ns2TKeffZZtm/f3m9NTzzxBJdddhnJZJJUKsW73vUuHn/8cQBOPfVUli5dCsCKFSvYs2dPVn/Ovva5aNEi1q1bx2c+8xkef/xxxo0bx7hx4ygtLeXDH/4w99xzD2VlZVkd40QM2HJ3963Asl7WfzFj+XPA54a2tN51t9wV7iInrK8W9nBau3Yt//Iv/8KmTZtoampixYqg1/a2226jpqaGjRs3UlRUxNy5cwd1X/dLL73E9ddfz1//+lcmTJjANddcc1L3h5eUlHQvx+PxE+qW6c2ZZ57Jpk2bePDBB/nCF77Aeeedxxe/+EWeeuopHn30Ue666y6++93v8vvf//6kjtNTwY0tkyyOAwp3kUKRSqVYs2YNH/rQh465kFpXV8eUKVMoKipi/fr1vPxy/yPZvvnNb+bnP/85ANu2bWPr1q0AHDlyhGQyybhx46iuruahhx7qfk95eTn19fXH7Wv16tXcd999NDU10djYyL333svq1atP6s/Z1z6rqqooKyvj6quv5tOf/jSbNm2ioaGBuro6LrroIm688Ua2bNlyUsfuTcENP5CIxygtiqlbRqSAXHXVVVx22WXH3Dnz3ve+l0suuYRFixaxcuVKzjrrrH73cd111/HBD36Q+fPnM3/+/O5PAEuWLGHZsmWcddZZzJo1i3POOaf7Pddeey0XXngh06dPZ/369d3rly9fzjXXXMOqVasA+MhHPsKyZcuy7oIB+NrXvtZ90RSCYR562+fDDz/Mpz/9aWKxGEVFRdx8883U19ezdu1aWlpacHduuOGGrI+bLXPPzfeRVq5c6YOdrGPl1x7hggVT+ca7Fg1xVSLRs2PHDubPn5/rMmQQevu7M7ON7r5yoPcWXLcMQKokrm4ZEZF+FGS4J0sS6pYREelHwYa7Wu4i2ctV96sM3sn+nRVkuJer5S6StdLSUg4dOqSALyBd47mXlpYOeh8Fd7cMqOUuciJmzpxJZWUlgx2sT3KjayamwSrYcFfLXSQ7RUVFg57NRwpXYXbLlKrlLiLSn4IM92Rxgpb2NB2d6VyXIiKSlwoz3EuCIQgaWztzXImISH4qyHAv7xrTXRN2iIj0qiDDPakx3UVE+lXY4a6LqiIivSrIcC/XhB0iIv0qyHBXy11EpH8FGe6ajUlEpH8FHe7qlhER6V1BhrvulhER6V9BhntxIkZxPKb73EVE+lCQ4Q7Bt1TVLSMi0ruCDfdUaULDD4iI9KFgwz1ZnKBefe4iIr0q2HBPaUx3EZE+FW64lyZo1AVVEZFeDRjuZlZqZk+Z2RYze9bMvtLLNiVm9gsz221mT5rZ3OEoNlOyJKFbIUVE+pBNy70VeKu7LwGWAhea2et7bPNh4FV3PwO4Efjm0JZ5vFSxZmMSEenLgOHugYbwaVH46DmN+lrglnD5LuA8M7Mhq7IXwd0yCncRkd5k1eduZnEz2wwcANa5+5M9NpkB7AVw9w6gDpg0lIX2lCxJ0NjWSTrd8/eMiIhkFe7u3unuS4GZwCozWziYg5nZtWa2wcw21NTUDGYX3VJdU+3poqqIyHFO6G4Zd68F1gMX9nhpHzALwMwSwDjgUC/v/767r3T3lRUVFYOrOJQqKQI0j6qISG+yuVumwszGh8tjgAuAnT02ux/4QLh8OfB7dx/W/pKuSbIbWtuH8zAiIgUpkcU204BbzCxO8MvgTnf/jZl9Fdjg7vcDPwR+ama7gcPAlcNWcejomO5quYuI9DRguLv7VmBZL+u/mLHcArx7aEvrn8Z0FxHpW8F+Q7VrTHeNLyMicryCDXe13EVE+law4d7VctetkCIixyvYcC8v1STZIiJ9KdhwL0nEiMdMg4eJiPSiYMPdzEgWa6o9EZHeFGy4A5SXFuk+dxGRXhR0uCdL4vqGqohILwo83DVJtohIbwo63FMlmrBDRKQ3CncRkQgq6HAPumUU7iIiPRV0uKvlLiLSu4IP98bWDoZ56HgRkYJT0OGeLEmQdmhu1x0zIiKZCjrcUxpfRkSkV4Ud7l2TZOtedxGRYxR0uCeLw5a7Bg8TETlGQYf70XlUFe4iIpkKO9xLNRuTiEhvCjrck2q5i4j0qqDDXd0yIiK9i0S4q1tGRORYBR3uZcVxzNRyFxHpqaDDPZhqT+PLiIj0VNDhDkfHlxERkaMKPtyDqfYU7iIimQYMdzObZWbrzWy7mT1rZv/cyzbnmlmdmW0OH18cnnKPFwz7q+EHREQyJbLYpgP4V3ffZGblwEYzW+fu23ts97i7v2PoS+xfqlTdMiIiPQ3Ycnf3/e6+KVyuB3YAM4a7sGwlixMaW0ZEpIcT6nM3s7nAMuDJXl5+g5ltMbOHzOzsPt5/rZltMLMNNTU1J1xsbzQbk4jI8bIOdzNLAXcDn3T3Iz1e3gTMcfclwHeA+3rbh7t/391XuvvKioqKwdZ8jFRpgsY2hbuISKaswt3MigiC/TZ3v6fn6+5+xN0bwuUHgSIzmzyklfYhqan2RESOk83dMgb8ENjh7jf0sc0p4XaY2apwv4eGstC+pEoStHc6rR3pkTiciEhByOZumXOA9wHPmNnmcN3ngdkA7v494HLgOjPrAJqBK32EmtLJ4q7ZmDooLYqPxCFFRPLegOHu7k8ANsA23wW+O1RFnYhUaREQTLU3KZWLCkRE8k/Bf0O1ax7V+tb2HFciIpI/Cj7ck93D/upbqiIiXQo+3DWmu4jI8SIT7vUKdxGRbgUf7km13EVEjlPw4Z4qVbiLiPRU8OGeLA67ZTR4mIhIt4IP93jMGFMUV8tdRCRDwYc7aPAwEZGeohHuJQl1y4iIZIhEuCdL1C0jIpIpEuGeKknoG6oiIhkiE+6ajUlE5KhIhHtS4S4icozIhLv63EVEjopEuJer5S4icoxIhHuyJEFrR5r2Tk21JyICEQp30PgyIiJdIhHu5WG4q2tGRCQQiXBPKtxFRI4RkXAP5lFVt4yISCAS4V5e2tVy17dURUQgIuHe3S2jwcNERICohHux7pYREckUiXA/2i2jcBcRgYiEu+6WERE5ViTCvSgeozgRU7eMiEhowHA3s1lmtt7MtpvZs2b2z71sY2b2bTPbbWZbzWz58JTbN40vIyJyVCKLbTqAf3X3TWZWDmw0s3Xuvj1jm7cD88LH64Cbw58jRiNDiogcNWDL3d33u/umcLke2AHM6LHZWuBWD/wFGG9m04a82n5oTHcRkaNOqM/dzOYCy4Ane7w0A9ib8byS438BYGbXmtkGM9tQU1NzYpUOIFUSV7iLiISyDnczSwF3A5909yODOZi7f9/dV7r7yoqKisHsok+aR1VE5Kiswt3MigiC/TZ3v6eXTfYBszKezwzXjRh1y4iIHJXN3TIG/BDY4e439LHZ/cD7w7tmXg/Uufv+IaxzQJokW0TkqGzuljkHeB/wjJltDtd9HpgN4O7fAx4ELgJ2A03AB4e+1P6ldLeMiEi3AcPd3Z8AbIBtHPj4UBU1GMmSBE1tnXSmnXis33JFRCIvEt9QhaDlDtDYpta7iEh0wr1UI0OKiHSJTLhrTHcRkaMiE+6pcKo93TEjIhKpcC8C0BeZRESIULgnu1vu7TmuREQk9yIT7qkSTZItItIlcuGuu2VERCIU7ppqT0TkqMiEe0kiRiJmCncRESIU7mam2ZhEREKRCXfQyJAiIl2iF+76hqqISLTCPVkS18BhIiJELNxTpUW6z11EhKiFe0mchhZ9Q1VEJFLhnizWJNkiIhCxcE+V6lZIERGIWriXJGho6yCY9U9EZPSKVLgnSxK4Q1ObumZEZHSLVLhr8DARkUAkw13fUhWR0S5S4a6RIUVEApEKd7XcRUQCkQx33esuIqNdpMJd86iKiAQiFe6aR1VEJDBguJvZj8zsgJlt6+P1c82szsw2h48vDn2Z2UmV6lZIERGARBbb/AT4LnBrP9s87u7vGJKKTsKYojgxQ2O6i8ioN2DL3d0fAw6PQC0nzcxIFms2JhGRoepzf4OZbTGzh8zs7L42MrNrzWyDmW2oqakZokMfS4OHiYgMTbhvAua4+xLgO8B9fW3o7t9395XuvrKiomIIDn28pOZRFRE5+XB39yPu3hAuPwgUmdnkk65skBTuIiJDEO5mdoqZWbi8KtznoZPd72CVl6hbRkRkwLtlzOx24FxgsplVAl8CigDc/XvA5cB1ZtYBNANXeg4HVE+WxDlQ35Krw4uI5IUBw93drxrg9e8S3CqZF5IlmmpPRCRS31CFoFtGfe4iMtpFLtyTYZ+7ptoTkdEskuHekXZaO9K5LkVEJGciF+7lpRrTXUQkcuGeLNbgYSIi0Qv3cNjfeg0eJiKjWOTC/ehsTAp3ERm9ohfuXWO6tyncRWT0il64h1PtqVtGREazyIV7UpNki4hEL9zV5y4iEsFw77oVsl7hLiKjWOTCPRYzyorjarmLyKgWuXCHoGtG4S4io1lkw13dMiIymkUy3JNquYvIKBfJcFe3jIiMdpEM92CSbN3nLiKjVyTDPVUSp6G1PddliIjkTDTDvVTzqIrI6BbJcE9qHlURGeUiGe6p4gRtHWnaNNWeiIxSkQz3pMaXEZFRLpLhPrm8BIBdBxpyXImISG5EMtzPO2sKY0sT3PKnPbkuRUQkJyIZ7smSBFetms1D2/ZT+WpTrssRERlxkQx3gPe/cS5mxk///HKuSxERGXEDhruZ/cjMDpjZtj5eNzP7tpntNrOtZrZ86Ms8cTPGj+HCs0/h9qf+pgurIjLqZNNy/wlwYT+vvx2YFz6uBW4++bKGxofedCpHWjq4Z1NlrksRERlRA4a7uz8GHO5nk7XArR74CzDezKYNVYEnY/ns8SyZNZ4f/3EP6bTnuhwRkREzFH3uM4C9Gc8rw3XHMbNrzWyDmW2oqakZgkP3z8z40DlzefFgI394fviPJyKSL0b0gqq7f9/dV7r7yoqKihE55kWLpjF1bAk/+uNLI3I8EZF8MBThvg+YlfF8ZrguLxTFY7z/DXN5fNdBnq+uz3U5IiIjYijC/X7g/eFdM68H6tx9/xDsd8j8w6rZlCRi/FitdxEZJbK5FfJ24M/Aa8ys0sw+bGYfM7OPhZs8CLwI7AZ+APzTsFU7SBOSxbxr+Uzu2bSPw41tuS5HRGTYJQbawN2vGuB1Bz4+ZBUNkw+dM5fbn/obtz/1Nz6+5oxclyMiIySddjrdKYpH9jubvRow3KNi3tRyVs+bzK1/3sNHV59GcWJ0/UWLjDYdnWl+ubGSG9c9T0NrB687dSKr51Wwet5kzpiSwsxyXeKwGjXhDsGXmj7447/y0Lb9rF3a692aIiet8tUm7t64jz+9cJA3nj6Zd6+cyfTxY3Jd1qjh7vxuezXf+u1OXqhpZMWcCcyfVs4fdx9i/XPbAThlbClvmjeZ1fMmc84Zk5mcKslx1UPPgl6Vkbdy5UrfsGHDiB4znXbOv/EPpEoS/Orj50T+N7eMnJb2Tn677RV+uXEvf3rhEABnVKTYdaABM1g9r4L3rJzF+QumUJKI57ja4ZNOO7trGjjS3M78aWO751YYKRv2HOYbD+1k48uvcnpFks9ceBYXLJja/X997+Emnth9kCd2HeSJ3Qepaw7mWl4wbSyrTp3IwhnjWDRjHKdXJEkMUzfO3sNNJOLGtHGD+4VvZhvdfeWA242mcAf46V9e5t/u28bd172BFXMmjvjxJf91pp2YMeAvf3fn6b21/HJDJb/ZUkV9awezJo7h8uWz+PsVM5g5oYy9h5v45Ya9/HJjJfvrWphQVsRly2byntfO4jWnlI/Qn2j41Le0s3lvLRtffpVNf6vl6b+9Sn1LMJZTzODMqeUsnRV8U3zJzPGcOTU1LKG5q7qeb/72OR7ZUc3UsSX8y/lncvmKmf0eqzPtbNtXxxO7D/L4rhq27K2juT2Ye7m0KMaCaWNZNGNcEPgzx3FGxeBqT6edzZW1PLK9mkd3HOC56nquffNpfP6i+YP6syrc+9DU1sHr//1RVs+r4L/emxdjnEkeqHy1iUe2V/PIjgP85cVDxMwYO6aI8WVFjBtTxPgxwc+ude7wwDP72X2ggdKiGBctnMa7V87idadOJBY7/pdCZ9p5fFcNd27Yy7rt1bR3OktmjeeypdOZmCohETPiMSNuRjxu3c8TsRgliRgzJ4xhYrJ4SD9tptPOwYZW9tU2U1Xbwv66Zl5taiMRi1GcCI5bnIhRHA9/hst1ze3dQf5cdT3uYAavmVrOstkTWDFnAhPKithSWceWvbVsqayltiloIY8pirNoxjiWzBrH6RUpykoSlBXFKSuOM6Y4TllxgrLiePhIEItBe6d3T5vZ3pmmNfzZ1pGmpb2Tezbt45cb95IsTvCxc0/nQ+ecypjiE/901Jl2Xqxp4Jl9dTyzr45t++p4tuoITW1HA3/elHLOmJLi9Ipk+DPFnEnJ467hNbV18Piugzy6o5rf7zzAwYY24jHjtXMncP78qbxtwSnMnlQ2qL83hXs/vvHQDn7w2Is89n/XMHPC4E6wFDZ355l9dTyyvZp1Ow6wY/8RAE6vSLLmNVNIhCFW19xGXXM7tU3twfOmdurDUUZXzJnAu1fM5OLF0ygvLcr62IcaWrn36X3cuWEvz1dnP1tYsjjO7ElJZk8cw5xJSWZNLGPOxDJmTyxj6thSmts7aWztoL6lg8a2DhpaOqhv7aCxNViubW5jf21LEOZ1zbxS10J757H//+MxozOLcZjKSxNBkM+ewPI5Qct8bB/nwN352+EmNu+tZfPeWrbsrWVb1ZEhm+O4OB7jfW+Yw8fXnMHEZPGQ7LNLZ9p56WAY+JVH2HWgnhcONFBV19K9TTxmzJlYxulTUpw6Ocmu6nr++MIh2jrSlJcmOPc1Uzh//hTOPXMK48qy/3fSF4V7P/bVNvPmb63nI286lc8N8qOR5Iemtg627TvC1spatlcdodM9aAUWJTJag10twwQxgz+/cIhHdlRTfaSVmMHKORO5YMFUzps/hdMqUgMes6MzTUtHmtRJ9ie7O/tqm2lpT9OZdjrSadJp6Eh3PXc6005TWyd7Dzfxtx6PEw3HeMw4ZWwp08eXMn38GKaNG8OMcLnrMbY0gTu0daaDR0fGI3xeWhTjtMmpXj+hZKutI82B+haa2zppCh/N7R1Hl8OfaXeK4zGK4kZxIk5xIlg++qkizulTkoPuvx6sxtYOXqxpZHdNPbsPNPDCgUZ21zSw52Aj08aXcv78qVwwfyqvPXXikN+CmW24j6q7ZbrMGD+GCxeewk//8jIdaee8+VN47dyh/0uQodXWkWbnK0fYUlnH1r21bK2sY9eBeroamlPHllCSiIfh0EFTeye9tV3KiuO8eV4FFyyYypqzppxway8Rj5Eagn8rZjboT47ptHOgvpWXDzXy8uEmaupbKSuOkypJBI/SxDHLyZIEyeIE8SwC2QxKY3FKi4bvwm9xIlbQn5qTJQkWzQz64jOl045lcb1mJIzKljsEV6z/7Vfb+NPuQ7R1Hvvx6S1nVjC+bGg/3vWnqraZ+7dU8WJNA2NLg37dsaWJ8GcR5V3LY4qoSJXk7B59d6emoZXd1Q3sOtDA7gMNNLZ1UBSLEY8bRTEjEY+RiBmJeNBfXBQ3zpxazup5FYPqB60+0sJvtu7nwWf280xlHW2dQWt1YrKYxTPHsXjmeJaE/8mmlJceV29rRzpoBbYHgd/SnuaMKalhDS6R4aRumSw1th698LH+uaMXPlbMmRAG/ZRhuS2qrqmdB7ft576n9/HkS8Fw+RXlJTS1dtAYXsDpTXlJgrcvOoXLls3s8+LdyXB3jrR0cLixjb8dbmJXdfCxc/eBINC7bh2DoN913JgiOjqDLoWOtNPR6bR3Hu1W6FJaFOPpCOwTAAAJFklEQVRNZ1RwwYIpnDd/ar/3Fb/a2MZD217h/i3BuXEPblVbPW8yi2eOZ/HMccycMCYvWkciI03hPghdtyw9uiO4ZWnnK8EoksXxGKdVJHnNKeWcObXrkWLWhLITCteW9k5+v/MA9z29j/99roa2zjSnVSR559IZrF06nTmTkkDQp1vf0sGRlnaONAc/g4t77WzY8yq/3bafxrZOpo8rZe2yGbxr2QzmTR34trqW9k6er65n5/56KmubebWxjcOZj6Y2Xm1sOyaUASaUFTFvajnzpqSCR7hcUV7Sb8B2tZw3vvwq67ZXs257NftqmzGD5bMncMGCqVywYCqnV6RoaO1g3fZXuH9zFY/vOkhH2jltcpJLlkznkiXTOWPKwH3hIqOBwn0I7D3cxJMvHWZXdT3PV9fzfHUD+2qbu18fUxRn3tQUsyaWBRd44jEScaMoHgsvAoWPhPFSTSO/3fYK9a0dVJSXcOmS6bxz6QwWzhh7wi3QprYO1m2v5t6n9/H4roN0pp2FM8byzqUzuHTpdCpSJVTVtbBz/xF27D/Cjlfq2bn/CC8dbCQzt8eXFTExWczEsuLgZ8ZjQlkx08eP4cypKSYN0bf33J3t+490B/2zVcEdKrMnllF9pIXWjjTTx5V2B/rZ00/83IhEncJ9mNS3tLPrQAO7qut57pUGnq+up6q2mbbO4N7b9k6nvevOgs509wW9VEmCvzv7FN65bDpvPH1yVhe2slFT38qvt1Rx79P7eGZfHTELLvZ0fZEEYOaEMcyfNpb5p5Qzf9pYzpo2llkTxgzbN/Cyta+2mUe2V/PY8zVMHz+GS5dOZ8XsCUPe1SQSJQr3PNGZDvqg4zEb9rtxdh+o576nqzjc1NYd5GeeUt7n/cciUnh0K2SeiMeMeGxk7sw4Y0o5n/q714zIsUQkv+nGbhGRCFK4i4hEkMJdRCSCFO4iIhGkcBcRiSCFu4hIBCncRUQiSOEuIhJBOfuGqpnVAC/38tJk4OAIl3OyVPPIUM3Dr9DqhdFX8xx3rxhoo5yFe1/MbEM2X63NJ6p5ZKjm4Vdo9YJq7ou6ZUREIkjhLiISQfkY7t/PdQGDoJpHhmoefoVWL6jmXuVdn7uIiJy8fGy5i4jISVK4i4hEUF6Fu5ldaGbPmdluM/tsruvJhpntMbNnzGyzmeXl1FJm9iMzO2Bm2zLWTTSzdWa2K/w5IZc1Zuqj3i+b2b7wPG82s4tyWWNPZjbLzNab2XYze9bM/jlcn8/nua+a8/Zcm1mpmT1lZlvCmr8Srj/VzJ4Ms+MXZlac61qh33p/YmYvZZzjpUN+cHfPiwcQB14ATgOKgS3AglzXlUXde4DJua5jgBrfDCwHtmWs+xbw2XD5s8A3c13nAPV+GfhUrmvrp+ZpwPJwuRx4HliQ5+e5r5rz9lwDBqTC5SLgSeD1wJ3AleH67wHX5brWAer9CXD5cB47n1ruq4Dd7v6iu7cBdwBrc1xTJLj7Y8DhHqvXAreEy7cA7xzRovrRR715zd33u/umcLke2AHMIL/Pc1815y0PNIRPi8KHA28F7grX58157qfeYZdP4T4D2JvxvJI8/4cWcuB3ZrbRzK7NdTEnYKq77w+XXwGm5rKYLP0fM9sadtvkTfdGT2Y2F1hG0EoriPPco2bI43NtZnEz2wwcANYRfOKvdfeOcJO8yo6e9bp71zn+eniObzSzkqE+bj6Fe6F6k7svB94OfNzM3pzrgk6UB58Z8/2e2JuB04GlwH7gP3JbTu/MLAXcDXzS3Y9kvpav57mXmvP6XLt7p7svBWYSfOI/K8cl9atnvWa2EPgcQd2vBSYCnxnq4+ZTuO8DZmU8nxmuy2vuvi/8eQC4l+AfWyGoNrNpAOHPAzmup1/uXh3+J0kDPyAPz7OZFRGE5G3ufk+4Oq/Pc281F8K5BnD3WmA98AZgvJklwpfyMjsy6r0w7BJzd28FfswwnON8Cve/AvPCq97FwJXA/TmuqV9mljSz8q5l4G3Atv7flTfuBz4QLn8A+FUOaxlQV0CGLiPPzrOZGfBDYIe735DxUt6e575qzudzbWYVZjY+XB4DXEBwrWA9cHm4Wd6c5z7q3ZnxC98Irg8M+TnOq2+ohrdc3URw58yP3P3rOS6pX2Z2GkFrHSAB/Dwfazaz24FzCYYZrQa+BNxHcIfBbIKhl69w97y4iNlHvecSdBM4wR1K/5jRl51zZvYm4HHgGSAdrv48QR92vp7nvmq+ijw912a2mOCCaZygcXqnu381/L94B0EXx9PA1WGrOKf6qff3QAXB3TSbgY9lXHgdmmPnU7iLiMjQyKduGRERGSIKdxGRCFK4i4hEkMJdRCSCFO4iIhGkcBfph5mda2a/yXUdIidK4S4iEkEKd4kEM7s6HDd7s5n9dzhYU0M4KNOzZvaomVWE2y41s7+Egzbd2zUwlpmdYWaPhGNvbzKz08Pdp8zsLjPbaWa3hd8qxMxWmNkfwkHjHs741uEnwjHSt5rZHTk5ITLqKdyl4JnZfOA9wDnhAE2dwHuBJLDB3c8G/kDwTVeAW4HPuPtigm9ndq2/Dfgvd18CvJFg0CwIRkv8JMFY56cB54RjsnyHYEzuFcCPgK5vJ38WWBbu/2PD86cW6V9i4E1E8t55wArgr2GjegzBAF1p4BfhNj8D7jGzccB4d/9DuP4W4JfhGEEz3P1eAHdvAQj395S7V4bPNwNzgVpgIbAu3CbO0V8GW4HbzOw+gmEeREacwl2iwIBb3P1zx6w0+7ce2w12rI3MMUo6Cf7fGPCsu7+hl+0vJphN6hLg/5nZooyxxkVGhLplJAoeBS43synQPW/pHIJ/310jBf4D8IS71wGvmtnqcP37gD+EMxFVmtk7w32UmFlZP8d8DqgwszeE2xeZ2dlmFgNmuft6gjG6xwGpIf3TimRBLXcpeO6+3cy+QDAjVgxoBz4ONBJMjvAFgm6a94Rv+QDwvTC8XwQ+GK5/H/DfZvbVcB/v7ueYbWZ2OfDtsKsnQTCi6fPAz8J1Bnw7HMdbZERpVEiJLDNrcHe1mmVUUreMiEgEqeUuIhJBarmLiESQwl1EJIIU7iIiEaRwFxGJIIW7iEgE/X+m1k9/NivoRgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "epochs = range(1, len(val_loss_record) + 1)\n",
    "\n",
    "plt.plot(epochs, val_loss_record, '-', label='Validation Loss')\n",
    "plt.title('Validation Loss')\n",
    "plt.xlabel('epoches')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
