{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, shutil, pickle, shelve\n",
    "from Bio import SeqIO\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.models import load_model, Model\n",
    "from keras.utils import plot_model\n",
    "from keras.models import Model, Sequential\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, MaxPooling1D, Flatten, Conv1D, LSTM, CuDNNLSTM, Bidirectional, TimeDistributed\n",
    "from keras.optimizers import RMSprop, Adam\n",
    "from keras.initializers import glorot_normal\n",
    "from utilities import sampling, one_hot_encoding, curtail, get_training_data, load_data, data_split, dianostic_plots, pad_for_detector\n",
    "from utilities import get_char_list, get_activated_subseq, get_freqs, get_candidates, get_motif\n",
    "import keras\n",
    "import utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3543"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Subset sampling: save to me_samples; \"me\" for \"mutually exclusive\"\n",
    "output_folder_path = \"../../../../temp/buffers/me_samples\"\n",
    "\n",
    "data_dir = \"/home/ubuntu/group_volume/team_neural_network/data/input/3.24_species_only\"\n",
    "\n",
    "len(os.listdir(data_dir)) # total number of regions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start creating 5 folds of training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "L = len(os.listdir(data_dir))\n",
    "k = 5\n",
    "r = 1/(k + 1)\n",
    "partition = [int(n*r*L) for n in range(0, k)] + [L]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 590, 1181, 1771, 2362, 3543]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "folds = [np.arange(partition[i], partition[i+1]) for i in range(k)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "copied training samples to /home/ubuntu/data/temp/folds/fold1\n",
      "copied training samples to /home/ubuntu/data/temp/folds/fold2\n",
      "copied training samples to /home/ubuntu/data/temp/folds/fold3\n",
      "copied training samples to /home/ubuntu/data/temp/folds/fold4\n",
      "copied training samples to /home/ubuntu/data/temp/folds/fold5\n"
     ]
    }
   ],
   "source": [
    "all_data_lst = np.array(os.listdir(data_dir))\n",
    "fold_root_dir = '/home/ubuntu/data/temp/folds'\n",
    "for i in range(k):\n",
    "    fold_files = all_data_lst[folds[i]]\n",
    "    fold_dir = os.path.join(fold_root_dir, 'fold'+str(i+1))\n",
    "    for file in fold_files:\n",
    "        shutil.copy(os.path.join(data_dir, file),\n",
    "                              fold_dir)\n",
    "    print('copied training samples to {}'.format(fold_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save to /home/ubuntu/data/temp/buffers/folds/fold1/fold1.data\n",
      "(14160, 1000, 4) (14160,)\n",
      "save to /home/ubuntu/data/temp/buffers/folds/fold1/fold1_x.data\n",
      "save to /home/ubuntu/data/temp/buffers/folds/fold1/fold1_y.data\n",
      "save to /home/ubuntu/data/temp/buffers/folds/fold2/fold2.data\n",
      "(14184, 1000, 4) (14184,)\n",
      "save to /home/ubuntu/data/temp/buffers/folds/fold2/fold2_x.data\n",
      "save to /home/ubuntu/data/temp/buffers/folds/fold2/fold2_y.data\n",
      "save to /home/ubuntu/data/temp/buffers/folds/fold3/fold3.data\n",
      "(14160, 1000, 4) (14160,)\n",
      "save to /home/ubuntu/data/temp/buffers/folds/fold3/fold3_x.data\n",
      "save to /home/ubuntu/data/temp/buffers/folds/fold3/fold3_y.data\n",
      "save to /home/ubuntu/data/temp/buffers/folds/fold4/fold4.data\n",
      "(14184, 1000, 4) (14184,)\n",
      "save to /home/ubuntu/data/temp/buffers/folds/fold4/fold4_x.data\n",
      "save to /home/ubuntu/data/temp/buffers/folds/fold4/fold4_y.data\n",
      "save to /home/ubuntu/data/temp/buffers/folds/fold5/fold5.data\n",
      "(28344, 1000, 4) (28344,)\n",
      "save to /home/ubuntu/data/temp/buffers/folds/fold5/fold5_x.data\n",
      "save to /home/ubuntu/data/temp/buffers/folds/fold5/fold5_y.data\n"
     ]
    }
   ],
   "source": [
    "k_fold_data = []\n",
    "i = 0\n",
    "fold_output_path = '/home/ubuntu/data/temp/buffers/folds/'\n",
    "for i in range(k):\n",
    "    one_fold_output_path = os.path.join(fold_output_path, 'fold'+str(i+1))\n",
    "    all_regions = one_hot_encoding(os.path.join(fold_root_dir, 'fold'+str(i+1)+'/'),\n",
    "                                   os.path.join(one_fold_output_path, 'fold'+str(i+1)+'.data'))\n",
    "    temp_x, temp_y = get_training_data(all_regions, one_fold_output_path,\n",
    "                                       1000, 'fold'+str(i+1)+'_x.data', 'fold'+str(i+1)+'_y.data')\n",
    "    k_fold_data.append((temp_x, temp_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finish creating 5 folds of data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start training model and extracting motifs on all 5 folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28344 (22676, 1000, 4) (22676,) (5668, 1000, 4) (5668,)\n"
     ]
    }
   ],
   "source": [
    "data_x = pickle.load(open('/home/ubuntu/data/temp/buffers/folds/fold5/fold5_x.data', 'rb'))\n",
    "data_y = pickle.load(open('/home/ubuntu/data/temp/buffers/folds/fold5/fold5_y.data', 'rb'))\n",
    "train_x, train_y, val_x, val_y = data_split(data_x, data_y, seed = 157)\n",
    "train_x, val_x = pad_for_detector(train_x, 10), pad_for_detector(val_x, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 15 # number of filters\n",
    "m = 10 # filter size\n",
    "def get_hybrid(opt, num_filters, kernel_size):\n",
    "    \"\"\"  Return a hybrid network given a optimizer\n",
    "    \"\"\"\n",
    "    model = Sequential()\n",
    "    model.add(Conv1D(filters = num_filters, \n",
    "                     kernel_size = kernel_size, \n",
    "                     padding = 'valid',\n",
    "                     activation = 'relu'))\n",
    "    model.add(MaxPooling1D(pool_size = 5, strides = 5))\n",
    "    model.add(Dropout(0.1))\n",
    "    #model.add(Bidirectional(LSTM(20)))\n",
    "    #model.add(Bidirectional(CuDNNLSTM(15, return_sequences=True)))\n",
    "    model.add(Bidirectional(CuDNNLSTM(20)))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(50))\n",
    "    model.add(keras.layers.LeakyReLU(alpha=0.3))\n",
    "    model.add(Dense(20))\n",
    "    model.add(keras.layers.LeakyReLU(alpha=0.3))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(optimizer=opt, loss='binary_crossentropy', metrics=['acc'])\n",
    "    return model\n",
    "\n",
    "\n",
    "def train(model, train_x, train_y, val_data, config = {'epochs': 35, 'batch_size': 256}):\n",
    "    \"\"\"  Train model for a given config, training data, and validation data\n",
    "    \"\"\"\n",
    "    epochs, batch_size = config['epochs'], config['batch_size']\n",
    "    return model.fit(train_x, train_y, epochs=epochs, batch_size=batch_size, validation_data=val_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start training\n",
    "# Set up some configurations\n",
    "optimizers = {'adam': Adam(lr=1e-3, beta_1=0.9, beta_2=0.999, epsilon=None, decay=1e-2, amsgrad=False),\n",
    "              'rmsprop': RMSprop(lr=1e-2, rho=0.9, epsilon=None, decay=1e-2)}\n",
    "config = {'epochs': 100, 'batch_size': 562}\n",
    "opt = optimizers['rmsprop']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************************************\n",
      "Training on fold 1\n",
      "14160 (11328, 1000, 4) (11328,) (2832, 1000, 4) (2832,)\n",
      "Train on 11328 samples, validate on 2832 samples\n",
      "Epoch 1/100\n",
      "11328/11328 [==============================] - 9s 805us/step - loss: 0.7347 - acc: 0.5475 - val_loss: 0.6894 - val_acc: 0.5576\n",
      "Epoch 2/100\n",
      "11328/11328 [==============================] - 4s 352us/step - loss: 0.6854 - acc: 0.5640 - val_loss: 0.6841 - val_acc: 0.5576\n",
      "Epoch 3/100\n",
      "11328/11328 [==============================] - 4s 349us/step - loss: 0.6822 - acc: 0.5597 - val_loss: 0.6859 - val_acc: 0.5593\n",
      "Epoch 4/100\n",
      "11328/11328 [==============================] - 4s 337us/step - loss: 0.6759 - acc: 0.5620 - val_loss: 0.7783 - val_acc: 0.5576\n",
      "Epoch 5/100\n",
      "11328/11328 [==============================] - 4s 336us/step - loss: 0.6701 - acc: 0.5794 - val_loss: 0.7651 - val_acc: 0.4527\n",
      "Epoch 6/100\n",
      "11328/11328 [==============================] - 4s 335us/step - loss: 0.6467 - acc: 0.6018 - val_loss: 0.6311 - val_acc: 0.6243\n",
      "Epoch 7/100\n",
      "11328/11328 [==============================] - 4s 334us/step - loss: 0.6160 - acc: 0.6475 - val_loss: 0.6121 - val_acc: 0.6571\n",
      "Epoch 8/100\n",
      "11328/11328 [==============================] - 4s 334us/step - loss: 0.5825 - acc: 0.6800 - val_loss: 0.5781 - val_acc: 0.7041\n",
      "Epoch 9/100\n",
      "11328/11328 [==============================] - 4s 335us/step - loss: 0.5582 - acc: 0.7040 - val_loss: 0.5702 - val_acc: 0.7059\n",
      "Epoch 10/100\n",
      "11328/11328 [==============================] - 4s 335us/step - loss: 0.5392 - acc: 0.7240 - val_loss: 0.6022 - val_acc: 0.6967\n",
      "Epoch 11/100\n",
      "11328/11328 [==============================] - 4s 334us/step - loss: 0.5221 - acc: 0.7352 - val_loss: 0.5707 - val_acc: 0.7044\n",
      "Epoch 12/100\n",
      "11328/11328 [==============================] - 4s 338us/step - loss: 0.5014 - acc: 0.7547 - val_loss: 0.5108 - val_acc: 0.7535\n",
      "Epoch 13/100\n",
      "11328/11328 [==============================] - 4s 332us/step - loss: 0.4965 - acc: 0.7564 - val_loss: 0.4816 - val_acc: 0.7578\n",
      "Epoch 14/100\n",
      "11328/11328 [==============================] - 4s 332us/step - loss: 0.4756 - acc: 0.7718 - val_loss: 0.5161 - val_acc: 0.7493\n",
      "Epoch 15/100\n",
      "11328/11328 [==============================] - 4s 326us/step - loss: 0.4676 - acc: 0.7759 - val_loss: 0.4818 - val_acc: 0.7701\n",
      "Epoch 16/100\n",
      "11328/11328 [==============================] - 4s 324us/step - loss: 0.4595 - acc: 0.7786 - val_loss: 0.4687 - val_acc: 0.7786\n",
      "Epoch 17/100\n",
      "11328/11328 [==============================] - 4s 326us/step - loss: 0.4456 - acc: 0.7877 - val_loss: 0.4273 - val_acc: 0.7984\n",
      "Epoch 18/100\n",
      "11328/11328 [==============================] - 4s 325us/step - loss: 0.4301 - acc: 0.7933 - val_loss: 0.4617 - val_acc: 0.7737\n",
      "Epoch 19/100\n",
      "11328/11328 [==============================] - 4s 317us/step - loss: 0.4294 - acc: 0.7931 - val_loss: 0.4240 - val_acc: 0.8005\n",
      "Epoch 20/100\n",
      "11328/11328 [==============================] - 4s 316us/step - loss: 0.4252 - acc: 0.7963 - val_loss: 0.4173 - val_acc: 0.8118\n",
      "Epoch 21/100\n",
      "11328/11328 [==============================] - 4s 323us/step - loss: 0.4185 - acc: 0.8027 - val_loss: 0.4317 - val_acc: 0.7888\n",
      "Epoch 22/100\n",
      "11328/11328 [==============================] - 4s 319us/step - loss: 0.4141 - acc: 0.8029 - val_loss: 0.4419 - val_acc: 0.7945\n",
      "Epoch 23/100\n",
      "11328/11328 [==============================] - 4s 319us/step - loss: 0.4104 - acc: 0.8101 - val_loss: 0.4166 - val_acc: 0.7970\n",
      "Epoch 24/100\n",
      "11328/11328 [==============================] - 4s 318us/step - loss: 0.3969 - acc: 0.8129 - val_loss: 0.4131 - val_acc: 0.8076\n",
      "Epoch 25/100\n",
      "11328/11328 [==============================] - 4s 312us/step - loss: 0.3957 - acc: 0.8137 - val_loss: 0.4070 - val_acc: 0.8146\n",
      "Epoch 26/100\n",
      "11328/11328 [==============================] - 4s 322us/step - loss: 0.3890 - acc: 0.8198 - val_loss: 0.3983 - val_acc: 0.8174\n",
      "Epoch 27/100\n",
      "11328/11328 [==============================] - 4s 313us/step - loss: 0.3881 - acc: 0.8196 - val_loss: 0.4460 - val_acc: 0.7938\n",
      "Epoch 28/100\n",
      "11328/11328 [==============================] - 3s 308us/step - loss: 0.3840 - acc: 0.8205 - val_loss: 0.4040 - val_acc: 0.8178\n",
      "Epoch 29/100\n",
      "11328/11328 [==============================] - 3s 307us/step - loss: 0.3721 - acc: 0.8295 - val_loss: 0.4149 - val_acc: 0.8100\n",
      "Epoch 30/100\n",
      "11328/11328 [==============================] - 4s 310us/step - loss: 0.3741 - acc: 0.8302 - val_loss: 0.4650 - val_acc: 0.7730\n",
      "Epoch 31/100\n",
      "11328/11328 [==============================] - 3s 302us/step - loss: 0.3698 - acc: 0.8303 - val_loss: 0.3764 - val_acc: 0.8309\n",
      "Epoch 32/100\n",
      "11328/11328 [==============================] - 3s 303us/step - loss: 0.3604 - acc: 0.8334 - val_loss: 0.3746 - val_acc: 0.8259\n",
      "Epoch 33/100\n",
      "11328/11328 [==============================] - 3s 301us/step - loss: 0.3620 - acc: 0.8346 - val_loss: 0.3813 - val_acc: 0.8309\n",
      "Epoch 34/100\n",
      "11328/11328 [==============================] - 3s 304us/step - loss: 0.3563 - acc: 0.8404 - val_loss: 0.3589 - val_acc: 0.8422\n",
      "Epoch 35/100\n",
      "11328/11328 [==============================] - 3s 296us/step - loss: 0.3500 - acc: 0.8422 - val_loss: 0.3630 - val_acc: 0.8415\n",
      "Epoch 36/100\n",
      "11328/11328 [==============================] - 3s 299us/step - loss: 0.3492 - acc: 0.8437 - val_loss: 0.3716 - val_acc: 0.8400\n",
      "Epoch 37/100\n",
      "11328/11328 [==============================] - 3s 297us/step - loss: 0.3455 - acc: 0.8438 - val_loss: 0.3709 - val_acc: 0.8372\n",
      "Epoch 38/100\n",
      "11328/11328 [==============================] - 3s 303us/step - loss: 0.3440 - acc: 0.8430 - val_loss: 0.3703 - val_acc: 0.8425\n",
      "Epoch 39/100\n",
      "11328/11328 [==============================] - 3s 295us/step - loss: 0.3360 - acc: 0.8509 - val_loss: 0.3412 - val_acc: 0.8542\n",
      "Epoch 40/100\n",
      "11328/11328 [==============================] - 3s 297us/step - loss: 0.3306 - acc: 0.8531 - val_loss: 0.3614 - val_acc: 0.8443\n",
      "Epoch 41/100\n",
      "11328/11328 [==============================] - 3s 299us/step - loss: 0.3365 - acc: 0.8501 - val_loss: 0.3369 - val_acc: 0.8535\n",
      "Epoch 42/100\n",
      "11328/11328 [==============================] - 3s 295us/step - loss: 0.3303 - acc: 0.8484 - val_loss: 0.3534 - val_acc: 0.8475\n",
      "Epoch 43/100\n",
      "11328/11328 [==============================] - 3s 296us/step - loss: 0.3280 - acc: 0.8545 - val_loss: 0.3390 - val_acc: 0.8520\n",
      "Epoch 44/100\n",
      "11328/11328 [==============================] - 3s 302us/step - loss: 0.3222 - acc: 0.8597 - val_loss: 0.3853 - val_acc: 0.8231\n",
      "Epoch 45/100\n",
      "11328/11328 [==============================] - 3s 295us/step - loss: 0.3206 - acc: 0.8581 - val_loss: 0.3385 - val_acc: 0.8531\n",
      "Epoch 46/100\n",
      "11328/11328 [==============================] - 3s 297us/step - loss: 0.3188 - acc: 0.8616 - val_loss: 0.3453 - val_acc: 0.8570\n",
      "Epoch 47/100\n",
      "11328/11328 [==============================] - 3s 295us/step - loss: 0.3102 - acc: 0.8645 - val_loss: 0.3292 - val_acc: 0.8679\n",
      "Epoch 48/100\n",
      "11328/11328 [==============================] - 3s 298us/step - loss: 0.3130 - acc: 0.8625 - val_loss: 0.3303 - val_acc: 0.8651\n",
      "Epoch 49/100\n",
      "11328/11328 [==============================] - 3s 292us/step - loss: 0.3128 - acc: 0.8631 - val_loss: 0.3368 - val_acc: 0.8524\n",
      "Epoch 50/100\n",
      "11328/11328 [==============================] - 3s 295us/step - loss: 0.3133 - acc: 0.8624 - val_loss: 0.3238 - val_acc: 0.8708\n",
      "Epoch 51/100\n",
      "11328/11328 [==============================] - 3s 297us/step - loss: 0.3082 - acc: 0.8663 - val_loss: 0.3233 - val_acc: 0.8633\n",
      "Epoch 52/100\n",
      "11328/11328 [==============================] - 3s 293us/step - loss: 0.3030 - acc: 0.8655 - val_loss: 0.3297 - val_acc: 0.8648\n",
      "Epoch 53/100\n",
      "11328/11328 [==============================] - 3s 298us/step - loss: 0.3078 - acc: 0.8656 - val_loss: 0.3119 - val_acc: 0.8708\n",
      "Epoch 54/100\n",
      "11328/11328 [==============================] - 3s 298us/step - loss: 0.3006 - acc: 0.8694 - val_loss: 0.3177 - val_acc: 0.8697\n",
      "Epoch 55/100\n",
      "11328/11328 [==============================] - 3s 295us/step - loss: 0.2979 - acc: 0.8713 - val_loss: 0.3319 - val_acc: 0.8552\n",
      "Epoch 56/100\n",
      "11328/11328 [==============================] - 3s 285us/step - loss: 0.2901 - acc: 0.8743 - val_loss: 0.3170 - val_acc: 0.8697\n",
      "Epoch 57/100\n",
      "11328/11328 [==============================] - 3s 293us/step - loss: 0.2938 - acc: 0.8723 - val_loss: 0.3152 - val_acc: 0.8697\n",
      "Epoch 58/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11328/11328 [==============================] - 3s 285us/step - loss: 0.2913 - acc: 0.8742 - val_loss: 0.3073 - val_acc: 0.8757\n",
      "Epoch 59/100\n",
      "11328/11328 [==============================] - 3s 288us/step - loss: 0.2902 - acc: 0.8750 - val_loss: 0.3142 - val_acc: 0.8676\n",
      "Epoch 60/100\n",
      "11328/11328 [==============================] - 3s 286us/step - loss: 0.2886 - acc: 0.8741 - val_loss: 0.3170 - val_acc: 0.8697\n",
      "Epoch 61/100\n",
      "11328/11328 [==============================] - 3s 290us/step - loss: 0.2872 - acc: 0.8781 - val_loss: 0.3024 - val_acc: 0.8799\n",
      "Epoch 62/100\n",
      "11328/11328 [==============================] - 3s 280us/step - loss: 0.2902 - acc: 0.8742 - val_loss: 0.3164 - val_acc: 0.8683\n",
      "Epoch 63/100\n",
      "11328/11328 [==============================] - 3s 281us/step - loss: 0.2834 - acc: 0.8774 - val_loss: 0.3107 - val_acc: 0.8718\n",
      "Epoch 64/100\n",
      "11328/11328 [==============================] - 3s 280us/step - loss: 0.2831 - acc: 0.8772 - val_loss: 0.2924 - val_acc: 0.8789\n",
      "Epoch 65/100\n",
      "11328/11328 [==============================] - 3s 288us/step - loss: 0.2794 - acc: 0.8777 - val_loss: 0.2983 - val_acc: 0.8782\n",
      "Epoch 66/100\n",
      "11328/11328 [==============================] - 3s 285us/step - loss: 0.2812 - acc: 0.8791 - val_loss: 0.3005 - val_acc: 0.8757\n",
      "Epoch 67/100\n",
      "11328/11328 [==============================] - 3s 280us/step - loss: 0.2794 - acc: 0.8799 - val_loss: 0.3108 - val_acc: 0.8694\n",
      "Epoch 68/100\n",
      "11328/11328 [==============================] - 3s 279us/step - loss: 0.2718 - acc: 0.8841 - val_loss: 0.2912 - val_acc: 0.8870\n",
      "Epoch 69/100\n",
      "11328/11328 [==============================] - 3s 281us/step - loss: 0.2785 - acc: 0.8802 - val_loss: 0.3011 - val_acc: 0.8775\n",
      "Epoch 70/100\n",
      "11328/11328 [==============================] - 3s 284us/step - loss: 0.2755 - acc: 0.8784 - val_loss: 0.2984 - val_acc: 0.8842\n",
      "Epoch 71/100\n",
      "11328/11328 [==============================] - 3s 284us/step - loss: 0.2712 - acc: 0.8850 - val_loss: 0.3108 - val_acc: 0.8750\n",
      "Epoch 72/100\n",
      "11328/11328 [==============================] - 3s 274us/step - loss: 0.2684 - acc: 0.8839 - val_loss: 0.2980 - val_acc: 0.8732\n",
      "Epoch 73/100\n",
      "11328/11328 [==============================] - 3s 277us/step - loss: 0.2646 - acc: 0.8891 - val_loss: 0.3251 - val_acc: 0.8665\n",
      "Epoch 74/100\n",
      "11328/11328 [==============================] - 3s 277us/step - loss: 0.2698 - acc: 0.8817 - val_loss: 0.2986 - val_acc: 0.8768\n",
      "Epoch 75/100\n",
      "11328/11328 [==============================] - 3s 275us/step - loss: 0.2661 - acc: 0.8852 - val_loss: 0.2814 - val_acc: 0.8856\n",
      "Epoch 76/100\n",
      "11328/11328 [==============================] - 3s 276us/step - loss: 0.2625 - acc: 0.8848 - val_loss: 0.3201 - val_acc: 0.8637\n",
      "Epoch 77/100\n",
      "11328/11328 [==============================] - 3s 276us/step - loss: 0.2638 - acc: 0.8859 - val_loss: 0.2921 - val_acc: 0.8782\n",
      "Epoch 78/100\n",
      "11328/11328 [==============================] - 3s 275us/step - loss: 0.2574 - acc: 0.8901 - val_loss: 0.3147 - val_acc: 0.8676\n",
      "Epoch 79/100\n",
      "11328/11328 [==============================] - 3s 281us/step - loss: 0.2573 - acc: 0.8885 - val_loss: 0.2814 - val_acc: 0.8824\n",
      "Epoch 80/100\n",
      "11328/11328 [==============================] - 3s 282us/step - loss: 0.2640 - acc: 0.8866 - val_loss: 0.2846 - val_acc: 0.8831\n",
      "Epoch 81/100\n",
      "11328/11328 [==============================] - 3s 282us/step - loss: 0.2620 - acc: 0.8897 - val_loss: 0.2762 - val_acc: 0.8927\n",
      "Epoch 82/100\n",
      "11328/11328 [==============================] - 3s 280us/step - loss: 0.2601 - acc: 0.8903 - val_loss: 0.2763 - val_acc: 0.8919\n",
      "Epoch 83/100\n",
      "11328/11328 [==============================] - 3s 277us/step - loss: 0.2549 - acc: 0.8933 - val_loss: 0.2838 - val_acc: 0.8881\n",
      "Epoch 84/100\n",
      "11328/11328 [==============================] - 3s 284us/step - loss: 0.2603 - acc: 0.8918 - val_loss: 0.2797 - val_acc: 0.8881\n",
      "Epoch 85/100\n",
      "11328/11328 [==============================] - 3s 282us/step - loss: 0.2536 - acc: 0.8937 - val_loss: 0.2756 - val_acc: 0.8909\n",
      "Epoch 86/100\n",
      "11328/11328 [==============================] - 3s 287us/step - loss: 0.2504 - acc: 0.8944 - val_loss: 0.2795 - val_acc: 0.8905\n",
      "Epoch 87/100\n",
      "11328/11328 [==============================] - 3s 279us/step - loss: 0.2485 - acc: 0.8965 - val_loss: 0.2795 - val_acc: 0.8919\n",
      "Epoch 88/100\n",
      "11328/11328 [==============================] - 3s 282us/step - loss: 0.2520 - acc: 0.8946 - val_loss: 0.2952 - val_acc: 0.8817\n",
      "Epoch 89/100\n",
      "11328/11328 [==============================] - 3s 285us/step - loss: 0.2475 - acc: 0.8952 - val_loss: 0.2743 - val_acc: 0.8905\n",
      "Epoch 90/100\n",
      "11328/11328 [==============================] - 3s 285us/step - loss: 0.2481 - acc: 0.8953 - val_loss: 0.2693 - val_acc: 0.8980\n",
      "Epoch 91/100\n",
      "11328/11328 [==============================] - 3s 285us/step - loss: 0.2557 - acc: 0.8931 - val_loss: 0.2897 - val_acc: 0.8821\n",
      "Epoch 92/100\n",
      "11328/11328 [==============================] - 3s 283us/step - loss: 0.2480 - acc: 0.8933 - val_loss: 0.2782 - val_acc: 0.8923\n",
      "Epoch 93/100\n",
      "11328/11328 [==============================] - 3s 284us/step - loss: 0.2430 - acc: 0.8983 - val_loss: 0.2725 - val_acc: 0.8905\n",
      "Epoch 94/100\n",
      "11328/11328 [==============================] - 3s 275us/step - loss: 0.2477 - acc: 0.8939 - val_loss: 0.2773 - val_acc: 0.8891\n",
      "Epoch 95/100\n",
      "11328/11328 [==============================] - 3s 278us/step - loss: 0.2410 - acc: 0.8979 - val_loss: 0.2734 - val_acc: 0.8881\n",
      "Epoch 96/100\n",
      "11328/11328 [==============================] - 3s 276us/step - loss: 0.2409 - acc: 0.9005 - val_loss: 0.2697 - val_acc: 0.8927\n",
      "Epoch 97/100\n",
      "11328/11328 [==============================] - 3s 284us/step - loss: 0.2299 - acc: 0.9069 - val_loss: 0.2728 - val_acc: 0.8930\n",
      "Epoch 98/100\n",
      "11328/11328 [==============================] - 3s 278us/step - loss: 0.2447 - acc: 0.8980 - val_loss: 0.2661 - val_acc: 0.8948\n",
      "Epoch 99/100\n",
      "11328/11328 [==============================] - 3s 278us/step - loss: 0.2442 - acc: 0.8985 - val_loss: 0.2654 - val_acc: 0.8941\n",
      "Epoch 100/100\n",
      "11328/11328 [==============================] - 3s 281us/step - loss: 0.2445 - acc: 0.8953 - val_loss: 0.2979 - val_acc: 0.8761\n",
      "************************************\n",
      "************************************\n",
      "Training on fold 2\n",
      "14184 (11348, 1000, 4) (11348,) (2836, 1000, 4) (2836,)\n",
      "Train on 11348 samples, validate on 2836 samples\n",
      "Epoch 1/100\n",
      "11348/11348 [==============================] - 10s 858us/step - loss: 0.6854 - acc: 0.5353 - val_loss: 0.6734 - val_acc: 0.5885\n",
      "Epoch 2/100\n",
      "11348/11348 [==============================] - 4s 364us/step - loss: 0.6730 - acc: 0.5530 - val_loss: 0.6619 - val_acc: 0.5659\n",
      "Epoch 3/100\n",
      "11348/11348 [==============================] - 4s 353us/step - loss: 0.6673 - acc: 0.5560 - val_loss: 0.6567 - val_acc: 0.5903\n",
      "Epoch 4/100\n",
      "11348/11348 [==============================] - 4s 349us/step - loss: 0.6655 - acc: 0.5605 - val_loss: 0.6537 - val_acc: 0.5956\n",
      "Epoch 5/100\n",
      "11348/11348 [==============================] - 4s 331us/step - loss: 0.6642 - acc: 0.5724 - val_loss: 0.6615 - val_acc: 0.5532\n",
      "Epoch 6/100\n",
      "11348/11348 [==============================] - 4s 325us/step - loss: 0.6624 - acc: 0.5733 - val_loss: 0.6494 - val_acc: 0.5970\n",
      "Epoch 7/100\n",
      "11348/11348 [==============================] - 4s 313us/step - loss: 0.6597 - acc: 0.5871 - val_loss: 0.6507 - val_acc: 0.5920\n",
      "11348/11348 [==============================] - 3s 277us/step - loss: 0.5557 - acc: 0.7161 - val_loss: 0.5229 - val_acc: 0.7440\n",
      "Epoch 43/100\n",
      "11348/11348 [==============================] - 3s 273us/step - loss: 0.5542 - acc: 0.7158 - val_loss: 0.5259 - val_acc: 0.7426\n",
      "Epoch 44/100\n",
      "11348/11348 [==============================] - 3s 285us/step - loss: 0.5482 - acc: 0.7206 - val_loss: 0.5532 - val_acc: 0.7137\n",
      "Epoch 45/100\n",
      "11348/11348 [==============================] - 3s 279us/step - loss: 0.5465 - acc: 0.7182 - val_loss: 0.5142 - val_acc: 0.7496\n",
      "Epoch 46/100\n",
      "11348/11348 [==============================] - 3s 280us/step - loss: 0.5406 - acc: 0.7239 - val_loss: 0.5052 - val_acc: 0.7599\n",
      "Epoch 47/100\n",
      "11348/11348 [==============================] - 3s 284us/step - loss: 0.5392 - acc: 0.7271 - val_loss: 0.5000 - val_acc: 0.7645\n",
      "Epoch 48/100\n",
      "11348/11348 [==============================] - 3s 280us/step - loss: 0.5432 - acc: 0.7210 - val_loss: 0.5051 - val_acc: 0.7556\n",
      "Epoch 49/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11348/11348 [==============================] - 3s 283us/step - loss: 0.5371 - acc: 0.7302 - val_loss: 0.5200 - val_acc: 0.7493\n",
      "Epoch 50/100\n",
      "11348/11348 [==============================] - 3s 283us/step - loss: 0.5342 - acc: 0.7329 - val_loss: 0.4975 - val_acc: 0.7680\n",
      "Epoch 51/100\n",
      "11348/11348 [==============================] - 3s 283us/step - loss: 0.5271 - acc: 0.7360 - val_loss: 0.5291 - val_acc: 0.7412\n",
      "Epoch 52/100\n",
      "11348/11348 [==============================] - 3s 275us/step - loss: 0.5345 - acc: 0.7260 - val_loss: 0.5260 - val_acc: 0.7387\n",
      "Epoch 53/100\n",
      "11348/11348 [==============================] - 3s 274us/step - loss: 0.5238 - acc: 0.7393 - val_loss: 0.5022 - val_acc: 0.7592\n",
      "Epoch 54/100\n",
      "11348/11348 [==============================] - 3s 282us/step - loss: 0.5235 - acc: 0.7348 - val_loss: 0.4893 - val_acc: 0.7719\n",
      "Epoch 55/100\n",
      "11348/11348 [==============================] - 3s 279us/step - loss: 0.5199 - acc: 0.7370 - val_loss: 0.5008 - val_acc: 0.7581\n",
      "Epoch 56/100\n",
      "11348/11348 [==============================] - 3s 285us/step - loss: 0.5189 - acc: 0.7418 - val_loss: 0.5077 - val_acc: 0.7539\n",
      "Epoch 57/100\n",
      "11348/11348 [==============================] - 3s 282us/step - loss: 0.5160 - acc: 0.7451 - val_loss: 0.4818 - val_acc: 0.7719\n",
      "Epoch 58/100\n",
      "11348/11348 [==============================] - 3s 280us/step - loss: 0.5157 - acc: 0.7459 - val_loss: 0.4828 - val_acc: 0.7645\n",
      "Epoch 59/100\n",
      "11348/11348 [==============================] - 3s 285us/step - loss: 0.5122 - acc: 0.7451 - val_loss: 0.5007 - val_acc: 0.7489\n",
      "Epoch 60/100\n",
      "11348/11348 [==============================] - 3s 286us/step - loss: 0.5111 - acc: 0.7443 - val_loss: 0.4733 - val_acc: 0.7747\n",
      "Epoch 61/100\n",
      "11348/11348 [==============================] - 3s 282us/step - loss: 0.5069 - acc: 0.7511 - val_loss: 0.4764 - val_acc: 0.7775\n",
      "Epoch 62/100\n",
      "11348/11348 [==============================] - 3s 278us/step - loss: 0.5030 - acc: 0.7578 - val_loss: 0.4794 - val_acc: 0.7705\n",
      "Epoch 63/100\n",
      "11348/11348 [==============================] - 3s 286us/step - loss: 0.4977 - acc: 0.7585 - val_loss: 0.5214 - val_acc: 0.7398\n",
      "Epoch 64/100\n",
      "11348/11348 [==============================] - 3s 284us/step - loss: 0.4978 - acc: 0.7578 - val_loss: 0.4624 - val_acc: 0.7860\n",
      "Epoch 65/100\n",
      "11348/11348 [==============================] - 3s 281us/step - loss: 0.4991 - acc: 0.7568 - val_loss: 0.4858 - val_acc: 0.7648\n",
      "Epoch 66/100\n",
      "11348/11348 [==============================] - 3s 283us/step - loss: 0.4936 - acc: 0.7576 - val_loss: 0.5015 - val_acc: 0.7472\n",
      "Epoch 67/100\n",
      "11348/11348 [==============================] - 3s 278us/step - loss: 0.4954 - acc: 0.7620 - val_loss: 0.4599 - val_acc: 0.7842\n",
      "Epoch 68/100\n",
      "11348/11348 [==============================] - 3s 283us/step - loss: 0.4908 - acc: 0.7624 - val_loss: 0.4887 - val_acc: 0.7652\n",
      "Epoch 69/100\n",
      "11348/11348 [==============================] - 3s 277us/step - loss: 0.4928 - acc: 0.7602 - val_loss: 0.4552 - val_acc: 0.7881\n",
      "Epoch 70/100\n",
      "11348/11348 [==============================] - 3s 284us/step - loss: 0.4851 - acc: 0.7652 - val_loss: 0.4588 - val_acc: 0.7874\n",
      "Epoch 71/100\n",
      "11348/11348 [==============================] - 3s 280us/step - loss: 0.4822 - acc: 0.7696 - val_loss: 0.4630 - val_acc: 0.7839\n",
      "Epoch 72/100\n",
      "11348/11348 [==============================] - 3s 282us/step - loss: 0.4783 - acc: 0.7725 - val_loss: 0.4816 - val_acc: 0.7609\n",
      "Epoch 73/100\n",
      "11348/11348 [==============================] - 3s 279us/step - loss: 0.4771 - acc: 0.7712 - val_loss: 0.4790 - val_acc: 0.7669\n",
      "Epoch 74/100\n",
      "11348/11348 [==============================] - 3s 275us/step - loss: 0.4824 - acc: 0.7694 - val_loss: 0.4469 - val_acc: 0.7906\n",
      "Epoch 75/100\n",
      "11348/11348 [==============================] - 3s 275us/step - loss: 0.4750 - acc: 0.7724 - val_loss: 0.4624 - val_acc: 0.7824\n",
      "Epoch 76/100\n",
      "11348/11348 [==============================] - 3s 277us/step - loss: 0.4743 - acc: 0.7722 - val_loss: 0.4624 - val_acc: 0.7772\n",
      "Epoch 77/100\n",
      "11348/11348 [==============================] - 3s 282us/step - loss: 0.4720 - acc: 0.7739 - val_loss: 0.4435 - val_acc: 0.7951\n",
      "Epoch 78/100\n",
      "11348/11348 [==============================] - 3s 274us/step - loss: 0.4691 - acc: 0.7759 - val_loss: 0.4353 - val_acc: 0.7987\n",
      "Epoch 79/100\n",
      "11348/11348 [==============================] - 3s 275us/step - loss: 0.4624 - acc: 0.7791 - val_loss: 0.4322 - val_acc: 0.8004\n",
      "Epoch 80/100\n",
      "11348/11348 [==============================] - 3s 276us/step - loss: 0.4651 - acc: 0.7814 - val_loss: 0.4415 - val_acc: 0.7913\n",
      "Epoch 81/100\n",
      "11348/11348 [==============================] - 3s 283us/step - loss: 0.4654 - acc: 0.7778 - val_loss: 0.4387 - val_acc: 0.7927\n",
      "Epoch 82/100\n",
      "11348/11348 [==============================] - 3s 274us/step - loss: 0.4570 - acc: 0.7871 - val_loss: 0.4356 - val_acc: 0.7909\n",
      "Epoch 83/100\n",
      "11348/11348 [==============================] - 3s 276us/step - loss: 0.4609 - acc: 0.7814 - val_loss: 0.4763 - val_acc: 0.7669\n",
      "Epoch 84/100\n",
      "11348/11348 [==============================] - 3s 280us/step - loss: 0.4557 - acc: 0.7838 - val_loss: 0.4317 - val_acc: 0.8018\n",
      "Epoch 85/100\n",
      "11348/11348 [==============================] - 3s 281us/step - loss: 0.4574 - acc: 0.7798 - val_loss: 0.4368 - val_acc: 0.7927\n",
      "Epoch 86/100\n",
      "11348/11348 [==============================] - 3s 284us/step - loss: 0.4528 - acc: 0.7883 - val_loss: 0.4324 - val_acc: 0.7965\n",
      "Epoch 87/100\n",
      "11348/11348 [==============================] - 3s 284us/step - loss: 0.4528 - acc: 0.7852 - val_loss: 0.4213 - val_acc: 0.8050\n",
      "Epoch 88/100\n",
      "11348/11348 [==============================] - 3s 281us/step - loss: 0.4535 - acc: 0.7873 - val_loss: 0.4167 - val_acc: 0.8036\n",
      "Epoch 89/100\n",
      "11348/11348 [==============================] - 3s 281us/step - loss: 0.4504 - acc: 0.7904 - val_loss: 0.4365 - val_acc: 0.7962\n",
      "Epoch 90/100\n",
      "11348/11348 [==============================] - 3s 285us/step - loss: 0.4433 - acc: 0.7923 - val_loss: 0.4246 - val_acc: 0.8078\n",
      "Epoch 91/100\n",
      "11348/11348 [==============================] - 3s 277us/step - loss: 0.4456 - acc: 0.7902 - val_loss: 0.4202 - val_acc: 0.8103\n",
      "Epoch 92/100\n",
      "11348/11348 [==============================] - 3s 285us/step - loss: 0.4429 - acc: 0.7968 - val_loss: 0.4064 - val_acc: 0.8124\n",
      "Epoch 93/100\n",
      "11348/11348 [==============================] - 3s 277us/step - loss: 0.4367 - acc: 0.7964 - val_loss: 0.4324 - val_acc: 0.7909\n",
      "Epoch 94/100\n",
      "11348/11348 [==============================] - 3s 279us/step - loss: 0.4392 - acc: 0.7967 - val_loss: 0.4579 - val_acc: 0.7849\n",
      "Epoch 95/100\n",
      "11348/11348 [==============================] - 3s 276us/step - loss: 0.4382 - acc: 0.7981 - val_loss: 0.4282 - val_acc: 0.8029\n",
      "Epoch 96/100\n",
      "11348/11348 [==============================] - 3s 281us/step - loss: 0.4319 - acc: 0.8000 - val_loss: 0.4067 - val_acc: 0.8156\n",
      "Epoch 97/100\n",
      "11348/11348 [==============================] - 3s 279us/step - loss: 0.4334 - acc: 0.7990 - val_loss: 0.4036 - val_acc: 0.8145\n",
      "Epoch 98/100\n",
      "11348/11348 [==============================] - 3s 279us/step - loss: 0.4315 - acc: 0.8032 - val_loss: 0.4193 - val_acc: 0.8110\n",
      "Epoch 99/100\n",
      "11348/11348 [==============================] - 3s 278us/step - loss: 0.4373 - acc: 0.7981 - val_loss: 0.4173 - val_acc: 0.8103\n",
      "Epoch 100/100\n",
      "11348/11348 [==============================] - 3s 279us/step - loss: 0.4267 - acc: 0.8045 - val_loss: 0.3985 - val_acc: 0.8170\n",
      "************************************\n",
      "************************************\n",
      "Training on fold 3\n",
      "14160 (11328, 1000, 4) (11328,) (2832, 1000, 4) (2832,)\n",
      "Train on 11328 samples, validate on 2832 samples\n",
      "Epoch 1/100\n",
      "11328/11328 [==============================] - 10s 879us/step - loss: 0.6903 - acc: 0.5291 - val_loss: 0.6868 - val_acc: 0.5438\n",
      "Epoch 2/100\n",
      "11328/11328 [==============================] - 4s 349us/step - loss: 0.6865 - acc: 0.5380 - val_loss: 0.6834 - val_acc: 0.5251\n",
      "Epoch 3/100\n",
      "11328/11328 [==============================] - 4s 326us/step - loss: 0.6845 - acc: 0.5311 - val_loss: 0.6833 - val_acc: 0.5233\n",
      "Epoch 4/100\n",
      "11328/11328 [==============================] - 3s 309us/step - loss: 0.6800 - acc: 0.5439 - val_loss: 0.6770 - val_acc: 0.5625\n",
      "Epoch 5/100\n",
      "11328/11328 [==============================] - 3s 304us/step - loss: 0.6773 - acc: 0.5511 - val_loss: 0.6749 - val_acc: 0.5586\n",
      "Epoch 6/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11328/11328 [==============================] - 3s 307us/step - loss: 0.6760 - acc: 0.5474 - val_loss: 0.6727 - val_acc: 0.5742\n",
      "Epoch 7/100\n",
      "11328/11328 [==============================] - 3s 307us/step - loss: 0.6744 - acc: 0.5530 - val_loss: 0.6715 - val_acc: 0.5664\n",
      "Epoch 8/100\n",
      "11328/11328 [==============================] - 3s 304us/step - loss: 0.6735 - acc: 0.5512 - val_loss: 0.6701 - val_acc: 0.5689\n",
      "Epoch 9/100\n",
      "11328/11328 [==============================] - 3s 306us/step - loss: 0.6710 - acc: 0.5580 - val_loss: 0.6748 - val_acc: 0.5279\n",
      "Epoch 10/100\n",
      "11328/11328 [==============================] - 4s 310us/step - loss: 0.6692 - acc: 0.5600 - val_loss: 0.6684 - val_acc: 0.5696\n",
      "Epoch 11/100\n",
      "11328/11328 [==============================] - 3s 306us/step - loss: 0.6690 - acc: 0.5662 - val_loss: 0.6667 - val_acc: 0.5802\n",
      "Epoch 12/100\n",
      "11328/11328 [==============================] - 3s 305us/step - loss: 0.6692 - acc: 0.5629 - val_loss: 0.6695 - val_acc: 0.5491\n",
      "Epoch 13/100\n",
      "11328/11328 [==============================] - 3s 298us/step - loss: 0.6677 - acc: 0.5675 - val_loss: 0.6733 - val_acc: 0.5445\n",
      "Epoch 14/100\n",
      "11328/11328 [==============================] - 3s 294us/step - loss: 0.6677 - acc: 0.5669 - val_loss: 0.6641 - val_acc: 0.5992\n",
      "Epoch 15/100\n",
      "11328/11328 [==============================] - 3s 288us/step - loss: 0.6663 - acc: 0.5727 - val_loss: 0.6642 - val_acc: 0.5847\n",
      "Epoch 16/100\n",
      "11328/11328 [==============================] - 3s 293us/step - loss: 0.6652 - acc: 0.5760 - val_loss: 0.6628 - val_acc: 0.5936\n",
      "Epoch 17/100\n",
      "11328/11328 [==============================] - 3s 290us/step - loss: 0.6643 - acc: 0.5822 - val_loss: 0.6623 - val_acc: 0.5929\n",
      "Epoch 18/100\n",
      "11328/11328 [==============================] - 3s 285us/step - loss: 0.6644 - acc: 0.5776 - val_loss: 0.6614 - val_acc: 0.6063\n",
      "Epoch 19/100\n",
      "11328/11328 [==============================] - 3s 292us/step - loss: 0.6636 - acc: 0.5803 - val_loss: 0.6614 - val_acc: 0.5886\n",
      "Epoch 20/100\n",
      "11328/11328 [==============================] - 3s 294us/step - loss: 0.6625 - acc: 0.5807 - val_loss: 0.6593 - val_acc: 0.6073\n",
      "Epoch 21/100\n",
      "11328/11328 [==============================] - 3s 286us/step - loss: 0.6606 - acc: 0.5839 - val_loss: 0.6604 - val_acc: 0.5745\n",
      "Epoch 22/100\n",
      "11328/11328 [==============================] - 3s 287us/step - loss: 0.6597 - acc: 0.5880 - val_loss: 0.6682 - val_acc: 0.5667\n",
      "Epoch 23/100\n",
      "11328/11328 [==============================] - 3s 287us/step - loss: 0.6586 - acc: 0.5911 - val_loss: 0.6568 - val_acc: 0.6088\n",
      "Epoch 24/100\n",
      "11328/11328 [==============================] - 3s 287us/step - loss: 0.6585 - acc: 0.5941 - val_loss: 0.6598 - val_acc: 0.5706\n",
      "Epoch 25/100\n",
      "11328/11328 [==============================] - 3s 285us/step - loss: 0.6580 - acc: 0.5933 - val_loss: 0.6540 - val_acc: 0.6102\n",
      "Epoch 26/100\n",
      "11328/11328 [==============================] - 3s 284us/step - loss: 0.6556 - acc: 0.5965 - val_loss: 0.6535 - val_acc: 0.6098\n",
      "Epoch 27/100\n",
      "11328/11328 [==============================] - 3s 282us/step - loss: 0.6543 - acc: 0.5968 - val_loss: 0.6596 - val_acc: 0.5918\n",
      "Epoch 28/100\n",
      "11328/11328 [==============================] - 3s 283us/step - loss: 0.6533 - acc: 0.6010 - val_loss: 0.6490 - val_acc: 0.6098\n",
      "Epoch 29/100\n",
      "11328/11328 [==============================] - 3s 291us/step - loss: 0.6539 - acc: 0.6008 - val_loss: 0.6509 - val_acc: 0.6172\n",
      "Epoch 30/100\n",
      "11328/11328 [==============================] - 3s 286us/step - loss: 0.6496 - acc: 0.6038 - val_loss: 0.6534 - val_acc: 0.5879\n",
      "Epoch 31/100\n",
      "11328/11328 [==============================] - 3s 284us/step - loss: 0.6494 - acc: 0.6111 - val_loss: 0.6459 - val_acc: 0.6148\n",
      "Epoch 32/100\n",
      "11328/11328 [==============================] - 3s 289us/step - loss: 0.6481 - acc: 0.6109 - val_loss: 0.6520 - val_acc: 0.6148\n",
      "Epoch 33/100\n",
      "11328/11328 [==============================] - 3s 285us/step - loss: 0.6467 - acc: 0.6157 - val_loss: 0.6529 - val_acc: 0.6130\n",
      "Epoch 34/100\n",
      "11328/11328 [==============================] - 3s 279us/step - loss: 0.6452 - acc: 0.6145 - val_loss: 0.6409 - val_acc: 0.6342\n",
      "Epoch 35/100\n",
      "11328/11328 [==============================] - 3s 280us/step - loss: 0.6447 - acc: 0.6166 - val_loss: 0.6525 - val_acc: 0.6028\n",
      "Epoch 36/100\n",
      "11328/11328 [==============================] - 3s 273us/step - loss: 0.6453 - acc: 0.6126 - val_loss: 0.6607 - val_acc: 0.5904\n",
      "Epoch 37/100\n",
      "11328/11328 [==============================] - 3s 279us/step - loss: 0.6429 - acc: 0.6199 - val_loss: 0.6441 - val_acc: 0.6158\n",
      "Epoch 38/100\n",
      "11328/11328 [==============================] - 3s 282us/step - loss: 0.6409 - acc: 0.6209 - val_loss: 0.6373 - val_acc: 0.6363\n",
      "Epoch 39/100\n",
      "11328/11328 [==============================] - 3s 278us/step - loss: 0.6386 - acc: 0.6266 - val_loss: 0.6424 - val_acc: 0.6165\n",
      "Epoch 40/100\n",
      "11328/11328 [==============================] - 3s 283us/step - loss: 0.6382 - acc: 0.6258 - val_loss: 0.6311 - val_acc: 0.6427\n",
      "Epoch 41/100\n",
      "11328/11328 [==============================] - 3s 275us/step - loss: 0.6354 - acc: 0.6269 - val_loss: 0.6463 - val_acc: 0.6133\n",
      "Epoch 42/100\n",
      "11328/11328 [==============================] - 3s 280us/step - loss: 0.6385 - acc: 0.6237 - val_loss: 0.6393 - val_acc: 0.6183\n",
      "Epoch 43/100\n",
      "11328/11328 [==============================] - 3s 283us/step - loss: 0.6341 - acc: 0.6326 - val_loss: 0.6313 - val_acc: 0.6356\n",
      "Epoch 44/100\n",
      "11328/11328 [==============================] - 3s 282us/step - loss: 0.6326 - acc: 0.6324 - val_loss: 0.6410 - val_acc: 0.6176\n",
      "Epoch 45/100\n",
      "11328/11328 [==============================] - 3s 280us/step - loss: 0.6331 - acc: 0.6346 - val_loss: 0.6344 - val_acc: 0.6194\n",
      "Epoch 46/100\n",
      "11328/11328 [==============================] - 3s 282us/step - loss: 0.6335 - acc: 0.6361 - val_loss: 0.6324 - val_acc: 0.6342\n",
      "Epoch 47/100\n",
      "11328/11328 [==============================] - 3s 278us/step - loss: 0.6308 - acc: 0.6328 - val_loss: 0.6311 - val_acc: 0.6363\n",
      "Epoch 48/100\n",
      "11328/11328 [==============================] - 3s 282us/step - loss: 0.6305 - acc: 0.6381 - val_loss: 0.6205 - val_acc: 0.6568\n",
      "Epoch 49/100\n",
      "11328/11328 [==============================] - 3s 272us/step - loss: 0.6290 - acc: 0.6385 - val_loss: 0.6357 - val_acc: 0.6261\n",
      "Epoch 50/100\n",
      "11328/11328 [==============================] - 3s 276us/step - loss: 0.6260 - acc: 0.6398 - val_loss: 0.6364 - val_acc: 0.6254\n",
      "Epoch 51/100\n",
      "11328/11328 [==============================] - 3s 284us/step - loss: 0.6261 - acc: 0.6426 - val_loss: 0.6191 - val_acc: 0.6480\n",
      "Epoch 52/100\n",
      "11328/11328 [==============================] - 3s 288us/step - loss: 0.6236 - acc: 0.6465 - val_loss: 0.6209 - val_acc: 0.6525\n",
      "Epoch 53/100\n",
      "11328/11328 [==============================] - 3s 284us/step - loss: 0.6204 - acc: 0.6508 - val_loss: 0.6148 - val_acc: 0.6540\n",
      "Epoch 54/100\n",
      "11328/11328 [==============================] - 3s 284us/step - loss: 0.6216 - acc: 0.6472 - val_loss: 0.6186 - val_acc: 0.6423\n",
      "Epoch 55/100\n",
      "11328/11328 [==============================] - 3s 286us/step - loss: 0.6209 - acc: 0.6456 - val_loss: 0.6264 - val_acc: 0.6367\n",
      "Epoch 56/100\n",
      "11328/11328 [==============================] - 3s 282us/step - loss: 0.6186 - acc: 0.6514 - val_loss: 0.6155 - val_acc: 0.6593\n",
      "Epoch 57/100\n",
      "11328/11328 [==============================] - 3s 277us/step - loss: 0.6166 - acc: 0.6524 - val_loss: 0.6117 - val_acc: 0.6698\n",
      "Epoch 58/100\n",
      "11328/11328 [==============================] - 3s 282us/step - loss: 0.6168 - acc: 0.6521 - val_loss: 0.6120 - val_acc: 0.6543\n",
      "Epoch 59/100\n",
      "11328/11328 [==============================] - 3s 283us/step - loss: 0.6150 - acc: 0.6565 - val_loss: 0.6490 - val_acc: 0.6077\n",
      "Epoch 60/100\n",
      "11328/11328 [==============================] - 3s 285us/step - loss: 0.6175 - acc: 0.6486 - val_loss: 0.6225 - val_acc: 0.6384\n",
      "Epoch 61/100\n",
      "11328/11328 [==============================] - 3s 279us/step - loss: 0.6157 - acc: 0.6533 - val_loss: 0.6053 - val_acc: 0.6706\n",
      "Epoch 62/100\n",
      "11328/11328 [==============================] - 3s 280us/step - loss: 0.6133 - acc: 0.6574 - val_loss: 0.6132 - val_acc: 0.6575\n",
      "Epoch 63/100\n",
      "11328/11328 [==============================] - 3s 275us/step - loss: 0.6111 - acc: 0.6587 - val_loss: 0.6053 - val_acc: 0.6741\n",
      "Epoch 64/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11328/11328 [==============================] - 3s 272us/step - loss: 0.6112 - acc: 0.6582 - val_loss: 0.5995 - val_acc: 0.6840\n",
      "Epoch 65/100\n",
      "11328/11328 [==============================] - 3s 274us/step - loss: 0.6073 - acc: 0.6654 - val_loss: 0.5966 - val_acc: 0.6826\n",
      "Epoch 66/100\n",
      "11328/11328 [==============================] - 3s 276us/step - loss: 0.6080 - acc: 0.6622 - val_loss: 0.6071 - val_acc: 0.6638\n",
      "Epoch 67/100\n",
      "11328/11328 [==============================] - 3s 278us/step - loss: 0.6083 - acc: 0.6662 - val_loss: 0.6339 - val_acc: 0.6243\n",
      "Epoch 68/100\n",
      "11328/11328 [==============================] - 3s 275us/step - loss: 0.6090 - acc: 0.6618 - val_loss: 0.5961 - val_acc: 0.6790\n",
      "Epoch 69/100\n",
      "11328/11328 [==============================] - 3s 282us/step - loss: 0.6058 - acc: 0.6666 - val_loss: 0.5942 - val_acc: 0.6882\n",
      "Epoch 70/100\n",
      "11328/11328 [==============================] - 3s 285us/step - loss: 0.6024 - acc: 0.6688 - val_loss: 0.5932 - val_acc: 0.6893\n",
      "Epoch 71/100\n",
      "11328/11328 [==============================] - 3s 279us/step - loss: 0.6010 - acc: 0.6735 - val_loss: 0.6086 - val_acc: 0.6596\n",
      "Epoch 72/100\n",
      "11328/11328 [==============================] - 3s 273us/step - loss: 0.6004 - acc: 0.6698 - val_loss: 0.5972 - val_acc: 0.6847\n",
      "Epoch 73/100\n",
      "11328/11328 [==============================] - 3s 278us/step - loss: 0.6007 - acc: 0.6701 - val_loss: 0.6130 - val_acc: 0.6568\n",
      "Epoch 74/100\n",
      "11328/11328 [==============================] - 3s 277us/step - loss: 0.5990 - acc: 0.6786 - val_loss: 0.5897 - val_acc: 0.6953\n",
      "Epoch 75/100\n",
      "11328/11328 [==============================] - 3s 274us/step - loss: 0.5979 - acc: 0.6724 - val_loss: 0.5930 - val_acc: 0.6847\n",
      "Epoch 76/100\n",
      "11328/11328 [==============================] - 3s 285us/step - loss: 0.5984 - acc: 0.6731 - val_loss: 0.6009 - val_acc: 0.6684\n",
      "Epoch 77/100\n",
      "11328/11328 [==============================] - 3s 282us/step - loss: 0.5929 - acc: 0.6775 - val_loss: 0.5844 - val_acc: 0.6999\n",
      "Epoch 78/100\n",
      "11328/11328 [==============================] - 3s 282us/step - loss: 0.5942 - acc: 0.6794 - val_loss: 0.5910 - val_acc: 0.6868\n",
      "Epoch 79/100\n",
      "11328/11328 [==============================] - 3s 284us/step - loss: 0.5896 - acc: 0.6839 - val_loss: 0.5950 - val_acc: 0.6840\n",
      "Epoch 80/100\n",
      "11328/11328 [==============================] - 3s 279us/step - loss: 0.5918 - acc: 0.6743 - val_loss: 0.5904 - val_acc: 0.6903\n",
      "Epoch 81/100\n",
      "11328/11328 [==============================] - 3s 276us/step - loss: 0.5915 - acc: 0.6807 - val_loss: 0.5901 - val_acc: 0.6850\n",
      "Epoch 82/100\n",
      "11328/11328 [==============================] - 3s 282us/step - loss: 0.5891 - acc: 0.6819 - val_loss: 0.5792 - val_acc: 0.7048\n",
      "Epoch 83/100\n",
      "11328/11328 [==============================] - 3s 276us/step - loss: 0.5852 - acc: 0.6909 - val_loss: 0.5837 - val_acc: 0.6931\n",
      "Epoch 84/100\n",
      "11328/11328 [==============================] - 3s 282us/step - loss: 0.5887 - acc: 0.6864 - val_loss: 0.5741 - val_acc: 0.7140\n",
      "Epoch 85/100\n",
      " 7868/11328 [===================>..........] - ETA: 0s - loss: 0.5884 - acc: 0.6871"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11348/11348 [==============================] - 3s 278us/step - loss: 0.5911 - acc: 0.6744 - val_loss: 0.5771 - val_acc: 0.6943\n",
      "Epoch 89/100\n",
      "11348/11348 [==============================] - 3s 284us/step - loss: 0.5888 - acc: 0.6759 - val_loss: 0.5693 - val_acc: 0.7003\n",
      "Epoch 90/100\n",
      "11348/11348 [==============================] - 3s 277us/step - loss: 0.5850 - acc: 0.6771 - val_loss: 0.5674 - val_acc: 0.7010\n",
      "Epoch 91/100\n",
      "11348/11348 [==============================] - 3s 280us/step - loss: 0.5865 - acc: 0.6827 - val_loss: 0.5758 - val_acc: 0.6932\n",
      "Epoch 92/100\n",
      "11348/11348 [==============================] - 3s 277us/step - loss: 0.5806 - acc: 0.6879 - val_loss: 0.5655 - val_acc: 0.7063\n",
      "Epoch 93/100\n",
      "11348/11348 [==============================] - 3s 286us/step - loss: 0.5816 - acc: 0.6819 - val_loss: 0.5695 - val_acc: 0.6901\n",
      "Epoch 94/100\n",
      "11348/11348 [==============================] - 3s 277us/step - loss: 0.5836 - acc: 0.6822 - val_loss: 0.5663 - val_acc: 0.6922\n",
      "Epoch 95/100\n",
      "11348/11348 [==============================] - 3s 277us/step - loss: 0.5780 - acc: 0.6858 - val_loss: 0.5614 - val_acc: 0.7070\n",
      "Epoch 96/100\n",
      "11348/11348 [==============================] - 3s 277us/step - loss: 0.5788 - acc: 0.6861 - val_loss: 0.5898 - val_acc: 0.6798\n",
      "Epoch 97/100\n",
      "11348/11348 [==============================] - 3s 275us/step - loss: 0.5802 - acc: 0.6910 - val_loss: 0.5597 - val_acc: 0.7052\n",
      "Epoch 98/100\n",
      "11348/11348 [==============================] - 3s 283us/step - loss: 0.5786 - acc: 0.6864 - val_loss: 0.5578 - val_acc: 0.7126\n",
      "Epoch 99/100\n",
      "11348/11348 [==============================] - 3s 275us/step - loss: 0.5763 - acc: 0.6858 - val_loss: 0.5693 - val_acc: 0.6855\n",
      "Epoch 100/100\n",
      "11348/11348 [==============================] - 3s 276us/step - loss: 0.5756 - acc: 0.6913 - val_loss: 0.5599 - val_acc: 0.7084\n",
      "************************************\n",
      "************************************\n",
      "Training on fold 5\n",
      "28344 (22676, 1000, 4) (22676,) (5668, 1000, 4) (5668,)\n",
      "Train on 22676 samples, validate on 5668 samples\n",
      "Epoch 1/100\n",
      "22676/22676 [==============================] - 14s 636us/step - loss: 0.6896 - acc: 0.5437 - val_loss: 0.6873 - val_acc: 0.5445\n",
      "Epoch 2/100\n",
      "22676/22676 [==============================] - 7s 321us/step - loss: 0.6857 - acc: 0.5432 - val_loss: 0.6844 - val_acc: 0.5445\n",
      "Epoch 3/100\n",
      "22676/22676 [==============================] - 7s 301us/step - loss: 0.6828 - acc: 0.5459 - val_loss: 0.6825 - val_acc: 0.5445\n",
      "Epoch 4/100\n",
      "22676/22676 [==============================] - 7s 303us/step - loss: 0.6812 - acc: 0.5392 - val_loss: 0.6807 - val_acc: 0.5445\n",
      "Epoch 5/100\n",
      "22676/22676 [==============================] - 7s 299us/step - loss: 0.6788 - acc: 0.5423 - val_loss: 0.6795 - val_acc: 0.5445\n",
      "Epoch 6/100\n",
      "22676/22676 [==============================] - 7s 299us/step - loss: 0.6770 - acc: 0.5464 - val_loss: 0.6787 - val_acc: 0.5446\n",
      "Epoch 7/100\n",
      "22676/22676 [==============================] - 7s 302us/step - loss: 0.6766 - acc: 0.5380 - val_loss: 0.6784 - val_acc: 0.5485\n",
      "Epoch 8/100\n",
      "22676/22676 [==============================] - 6s 283us/step - loss: 0.6759 - acc: 0.5362 - val_loss: 0.6776 - val_acc: 0.5460\n",
      "Epoch 9/100\n",
      "22676/22676 [==============================] - 7s 288us/step - loss: 0.6753 - acc: 0.5422 - val_loss: 0.6782 - val_acc: 0.5653\n",
      "Epoch 10/100\n",
      "22676/22676 [==============================] - 7s 288us/step - loss: 0.6739 - acc: 0.5439 - val_loss: 0.6781 - val_acc: 0.5538\n",
      "Epoch 11/100\n",
      "22676/22676 [==============================] - 6s 286us/step - loss: 0.6743 - acc: 0.5413 - val_loss: 0.6768 - val_acc: 0.5473\n",
      "Epoch 12/100\n",
      "22676/22676 [==============================] - 6s 286us/step - loss: 0.6729 - acc: 0.5475 - val_loss: 0.6770 - val_acc: 0.5618\n",
      "Epoch 13/100\n",
      "22676/22676 [==============================] - 7s 289us/step - loss: 0.6732 - acc: 0.5477 - val_loss: 0.6761 - val_acc: 0.5640\n",
      "Epoch 14/100\n",
      "22676/22676 [==============================] - 7s 288us/step - loss: 0.6719 - acc: 0.5529 - val_loss: 0.6761 - val_acc: 0.5618\n",
      "Epoch 15/100\n",
      "22676/22676 [==============================] - 6s 286us/step - loss: 0.6720 - acc: 0.5468 - val_loss: 0.6763 - val_acc: 0.5547\n",
      "Epoch 16/100\n",
      "22676/22676 [==============================] - 6s 283us/step - loss: 0.6718 - acc: 0.5461 - val_loss: 0.6755 - val_acc: 0.5577\n",
      "Epoch 17/100\n",
      "22676/22676 [==============================] - 6s 278us/step - loss: 0.6713 - acc: 0.5512 - val_loss: 0.6744 - val_acc: 0.5672\n",
      "Epoch 18/100\n",
      "22676/22676 [==============================] - 6s 282us/step - loss: 0.6714 - acc: 0.5488 - val_loss: 0.6740 - val_acc: 0.5692\n",
      "Epoch 19/100\n",
      "22676/22676 [==============================] - 6s 280us/step - loss: 0.6703 - acc: 0.5541 - val_loss: 0.6734 - val_acc: 0.5667\n",
      "Epoch 20/100\n",
      "22676/22676 [==============================] - 6s 280us/step - loss: 0.6708 - acc: 0.5537 - val_loss: 0.6746 - val_acc: 0.5529\n",
      "Epoch 21/100\n",
      "22676/22676 [==============================] - 6s 276us/step - loss: 0.6705 - acc: 0.5513 - val_loss: 0.6738 - val_acc: 0.5522\n",
      "Epoch 22/100\n",
      "22676/22676 [==============================] - 6s 277us/step - loss: 0.6700 - acc: 0.5549 - val_loss: 0.6725 - val_acc: 0.5644\n",
      "Epoch 23/100\n",
      "22676/22676 [==============================] - 6s 274us/step - loss: 0.6692 - acc: 0.5570 - val_loss: 0.6719 - val_acc: 0.5683\n",
      "Epoch 24/100\n",
      "22676/22676 [==============================] - 6s 274us/step - loss: 0.6688 - acc: 0.5553 - val_loss: 0.6716 - val_acc: 0.5690\n",
      "Epoch 25/100\n",
      "22676/22676 [==============================] - 6s 273us/step - loss: 0.6685 - acc: 0.5575 - val_loss: 0.6713 - val_acc: 0.5649\n",
      "Epoch 26/100\n",
      "22676/22676 [==============================] - 6s 272us/step - loss: 0.6687 - acc: 0.5543 - val_loss: 0.6716 - val_acc: 0.5563\n",
      "Epoch 27/100\n",
      "22676/22676 [==============================] - 6s 277us/step - loss: 0.6672 - acc: 0.5538 - val_loss: 0.6703 - val_acc: 0.5685\n",
      "Epoch 28/100\n",
      "22676/22676 [==============================] - 6s 280us/step - loss: 0.6677 - acc: 0.5570 - val_loss: 0.6698 - val_acc: 0.5651\n",
      "Epoch 29/100\n",
      "22676/22676 [==============================] - 6s 273us/step - loss: 0.6667 - acc: 0.5582 - val_loss: 0.6698 - val_acc: 0.5757\n",
      "Epoch 30/100\n",
      "22676/22676 [==============================] - 6s 282us/step - loss: 0.6665 - acc: 0.5599 - val_loss: 0.6693 - val_acc: 0.5667\n",
      "Epoch 31/100\n",
      "22676/22676 [==============================] - 6s 279us/step - loss: 0.6662 - acc: 0.5575 - val_loss: 0.6684 - val_acc: 0.5681\n",
      "Epoch 32/100\n",
      "22676/22676 [==============================] - 6s 282us/step - loss: 0.6656 - acc: 0.5609 - val_loss: 0.6685 - val_acc: 0.5656\n",
      "Epoch 33/100\n",
      "22676/22676 [==============================] - 6s 276us/step - loss: 0.6662 - acc: 0.5537 - val_loss: 0.6682 - val_acc: 0.5614\n",
      "Epoch 34/100\n",
      "22676/22676 [==============================] - 6s 274us/step - loss: 0.6646 - acc: 0.5602 - val_loss: 0.6681 - val_acc: 0.5711\n",
      "Epoch 35/100\n",
      "22676/22676 [==============================] - 6s 276us/step - loss: 0.6647 - acc: 0.5581 - val_loss: 0.6670 - val_acc: 0.5603\n",
      "Epoch 36/100\n",
      "22676/22676 [==============================] - 6s 275us/step - loss: 0.6636 - acc: 0.5594 - val_loss: 0.6684 - val_acc: 0.5517\n",
      "Epoch 37/100\n",
      "22676/22676 [==============================] - 6s 278us/step - loss: 0.6633 - acc: 0.5565 - val_loss: 0.6665 - val_acc: 0.5559\n",
      "Epoch 38/100\n",
      "22676/22676 [==============================] - 6s 281us/step - loss: 0.6631 - acc: 0.5578 - val_loss: 0.6655 - val_acc: 0.5690\n",
      "Epoch 39/100\n",
      "22676/22676 [==============================] - 6s 284us/step - loss: 0.6630 - acc: 0.5574 - val_loss: 0.6649 - val_acc: 0.5700\n",
      "Epoch 40/100\n",
      "22676/22676 [==============================] - 6s 281us/step - loss: 0.6620 - acc: 0.5612 - val_loss: 0.6651 - val_acc: 0.5658\n",
      "Epoch 41/100\n",
      "22676/22676 [==============================] - 6s 282us/step - loss: 0.6619 - acc: 0.5583 - val_loss: 0.6639 - val_acc: 0.5732\n",
      "Epoch 42/100\n",
      "22676/22676 [==============================] - 6s 280us/step - loss: 0.6612 - acc: 0.5612 - val_loss: 0.6638 - val_acc: 0.5737\n",
      "Epoch 43/100\n",
      "22676/22676 [==============================] - 6s 283us/step - loss: 0.6604 - acc: 0.5591 - val_loss: 0.6627 - val_acc: 0.5723\n",
      "Epoch 44/100\n",
      "22676/22676 [==============================] - 6s 283us/step - loss: 0.6602 - acc: 0.5617 - val_loss: 0.6627 - val_acc: 0.5603\n",
      "Epoch 45/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22676/22676 [==============================] - 6s 277us/step - loss: 0.6598 - acc: 0.5649 - val_loss: 0.6622 - val_acc: 0.5707\n",
      "Epoch 46/100\n",
      "22676/22676 [==============================] - 6s 275us/step - loss: 0.6582 - acc: 0.5688 - val_loss: 0.6611 - val_acc: 0.5729\n",
      "Epoch 47/100\n",
      "22676/22676 [==============================] - 6s 274us/step - loss: 0.6590 - acc: 0.5661 - val_loss: 0.6610 - val_acc: 0.5716\n",
      "Epoch 48/100\n",
      "22676/22676 [==============================] - 6s 274us/step - loss: 0.6566 - acc: 0.5727 - val_loss: 0.6614 - val_acc: 0.5672\n",
      "Epoch 49/100\n",
      "22676/22676 [==============================] - 6s 275us/step - loss: 0.6574 - acc: 0.5700 - val_loss: 0.6622 - val_acc: 0.5699\n",
      "Epoch 50/100\n",
      "22676/22676 [==============================] - 6s 277us/step - loss: 0.6568 - acc: 0.5721 - val_loss: 0.6600 - val_acc: 0.5757\n",
      "Epoch 51/100\n",
      "22676/22676 [==============================] - 6s 280us/step - loss: 0.6565 - acc: 0.5726 - val_loss: 0.6585 - val_acc: 0.5707\n",
      "Epoch 52/100\n",
      "22676/22676 [==============================] - 6s 282us/step - loss: 0.6563 - acc: 0.5679 - val_loss: 0.6588 - val_acc: 0.5803\n",
      "Epoch 53/100\n",
      "22676/22676 [==============================] - 6s 280us/step - loss: 0.6553 - acc: 0.5712 - val_loss: 0.6578 - val_acc: 0.5757\n",
      "Epoch 54/100\n",
      "22676/22676 [==============================] - 6s 281us/step - loss: 0.6544 - acc: 0.5754 - val_loss: 0.6565 - val_acc: 0.5789\n",
      "Epoch 55/100\n",
      "22676/22676 [==============================] - 6s 279us/step - loss: 0.6544 - acc: 0.5776 - val_loss: 0.6577 - val_acc: 0.5801\n",
      "Epoch 56/100\n",
      "22676/22676 [==============================] - 6s 284us/step - loss: 0.6527 - acc: 0.5775 - val_loss: 0.6563 - val_acc: 0.5810\n",
      "Epoch 57/100\n",
      "22676/22676 [==============================] - 6s 280us/step - loss: 0.6533 - acc: 0.5780 - val_loss: 0.6575 - val_acc: 0.5767\n",
      "Epoch 58/100\n",
      "22676/22676 [==============================] - 6s 281us/step - loss: 0.6535 - acc: 0.5762 - val_loss: 0.6556 - val_acc: 0.5835\n",
      "Epoch 59/100\n",
      "22676/22676 [==============================] - 6s 279us/step - loss: 0.6529 - acc: 0.5779 - val_loss: 0.6547 - val_acc: 0.5879\n",
      "Epoch 60/100\n",
      "22676/22676 [==============================] - 6s 276us/step - loss: 0.6522 - acc: 0.5796 - val_loss: 0.6557 - val_acc: 0.5859\n",
      "Epoch 61/100\n",
      "22676/22676 [==============================] - 6s 284us/step - loss: 0.6501 - acc: 0.5860 - val_loss: 0.6537 - val_acc: 0.5866\n",
      "Epoch 62/100\n",
      "22676/22676 [==============================] - 6s 286us/step - loss: 0.6513 - acc: 0.5806 - val_loss: 0.6551 - val_acc: 0.5792\n",
      "Epoch 63/100\n",
      "22676/22676 [==============================] - 6s 280us/step - loss: 0.6509 - acc: 0.5818 - val_loss: 0.6523 - val_acc: 0.5972\n",
      "Epoch 64/100\n",
      "22676/22676 [==============================] - 6s 285us/step - loss: 0.6490 - acc: 0.5831 - val_loss: 0.6517 - val_acc: 0.5947\n",
      "Epoch 65/100\n",
      "22676/22676 [==============================] - 6s 283us/step - loss: 0.6486 - acc: 0.5883 - val_loss: 0.6499 - val_acc: 0.5932\n",
      "Epoch 66/100\n",
      "22676/22676 [==============================] - 6s 282us/step - loss: 0.6487 - acc: 0.5837 - val_loss: 0.6507 - val_acc: 0.5954\n",
      "Epoch 67/100\n",
      "22676/22676 [==============================] - 7s 288us/step - loss: 0.6479 - acc: 0.5882 - val_loss: 0.6482 - val_acc: 0.5932\n",
      "Epoch 68/100\n",
      "22676/22676 [==============================] - 6s 284us/step - loss: 0.6471 - acc: 0.5889 - val_loss: 0.6473 - val_acc: 0.5937\n",
      "Epoch 69/100\n",
      "22676/22676 [==============================] - 7s 288us/step - loss: 0.6477 - acc: 0.5895 - val_loss: 0.6477 - val_acc: 0.6034\n",
      "Epoch 70/100\n",
      "22676/22676 [==============================] - 6s 283us/step - loss: 0.6448 - acc: 0.5912 - val_loss: 0.6460 - val_acc: 0.6041\n",
      "Epoch 71/100\n",
      "22676/22676 [==============================] - 6s 279us/step - loss: 0.6455 - acc: 0.5921 - val_loss: 0.6475 - val_acc: 0.5999\n",
      "Epoch 72/100\n",
      "22676/22676 [==============================] - 6s 279us/step - loss: 0.6452 - acc: 0.5940 - val_loss: 0.6449 - val_acc: 0.6032\n",
      "Epoch 73/100\n",
      "22676/22676 [==============================] - 6s 280us/step - loss: 0.6447 - acc: 0.5947 - val_loss: 0.6482 - val_acc: 0.6041\n",
      "Epoch 74/100\n",
      "22676/22676 [==============================] - 6s 281us/step - loss: 0.6426 - acc: 0.5962 - val_loss: 0.6440 - val_acc: 0.6023\n",
      "Epoch 75/100\n",
      "22676/22676 [==============================] - 6s 282us/step - loss: 0.6448 - acc: 0.5927 - val_loss: 0.6447 - val_acc: 0.6050\n",
      "Epoch 76/100\n",
      "22676/22676 [==============================] - 6s 279us/step - loss: 0.6436 - acc: 0.5927 - val_loss: 0.6469 - val_acc: 0.6009\n",
      "Epoch 77/100\n",
      "22676/22676 [==============================] - 6s 280us/step - loss: 0.6417 - acc: 0.5945 - val_loss: 0.6421 - val_acc: 0.6096\n",
      "Epoch 78/100\n",
      "22676/22676 [==============================] - 6s 276us/step - loss: 0.6430 - acc: 0.5959 - val_loss: 0.6422 - val_acc: 0.6103\n",
      "Epoch 79/100\n",
      "22676/22676 [==============================] - 6s 278us/step - loss: 0.6409 - acc: 0.5995 - val_loss: 0.6446 - val_acc: 0.6057\n",
      "Epoch 80/100\n",
      "22676/22676 [==============================] - 6s 279us/step - loss: 0.6424 - acc: 0.5936 - val_loss: 0.6405 - val_acc: 0.6080\n",
      "Epoch 81/100\n",
      "22676/22676 [==============================] - 6s 286us/step - loss: 0.6403 - acc: 0.6005 - val_loss: 0.6428 - val_acc: 0.6027\n",
      "Epoch 82/100\n",
      "22676/22676 [==============================] - 6s 284us/step - loss: 0.6398 - acc: 0.6020 - val_loss: 0.6446 - val_acc: 0.6009\n",
      "Epoch 83/100\n",
      "22676/22676 [==============================] - 6s 282us/step - loss: 0.6386 - acc: 0.6031 - val_loss: 0.6456 - val_acc: 0.6117\n",
      "Epoch 84/100\n",
      "22676/22676 [==============================] - 6s 274us/step - loss: 0.6388 - acc: 0.6020 - val_loss: 0.6416 - val_acc: 0.6106\n",
      "Epoch 85/100\n",
      "22676/22676 [==============================] - 6s 276us/step - loss: 0.6391 - acc: 0.6009 - val_loss: 0.6396 - val_acc: 0.6173\n",
      "Epoch 86/100\n",
      "22676/22676 [==============================] - 6s 277us/step - loss: 0.6383 - acc: 0.6001 - val_loss: 0.6388 - val_acc: 0.6223\n",
      "Epoch 87/100\n",
      "22676/22676 [==============================] - 6s 285us/step - loss: 0.6376 - acc: 0.6038 - val_loss: 0.6376 - val_acc: 0.6152\n",
      "Epoch 88/100\n",
      "22676/22676 [==============================] - 6s 281us/step - loss: 0.6377 - acc: 0.6001 - val_loss: 0.6398 - val_acc: 0.6156\n",
      "Epoch 89/100\n",
      "22676/22676 [==============================] - 6s 281us/step - loss: 0.6363 - acc: 0.6039 - val_loss: 0.6383 - val_acc: 0.6113\n",
      "Epoch 90/100\n",
      "22676/22676 [==============================] - 6s 276us/step - loss: 0.6362 - acc: 0.6042 - val_loss: 0.6361 - val_acc: 0.6184\n",
      "Epoch 91/100\n",
      "22676/22676 [==============================] - 6s 283us/step - loss: 0.6350 - acc: 0.6064 - val_loss: 0.6371 - val_acc: 0.6179\n",
      "Epoch 92/100\n",
      "22676/22676 [==============================] - 6s 283us/step - loss: 0.6354 - acc: 0.6038 - val_loss: 0.6353 - val_acc: 0.6189\n",
      "Epoch 93/100\n",
      "22676/22676 [==============================] - 6s 281us/step - loss: 0.6344 - acc: 0.6106 - val_loss: 0.6341 - val_acc: 0.6193\n",
      "Epoch 94/100\n",
      "22676/22676 [==============================] - 6s 279us/step - loss: 0.6345 - acc: 0.6091 - val_loss: 0.6340 - val_acc: 0.6214\n",
      "Epoch 95/100\n",
      "22676/22676 [==============================] - 6s 277us/step - loss: 0.6320 - acc: 0.6151 - val_loss: 0.6367 - val_acc: 0.6205\n",
      "Epoch 96/100\n",
      "22676/22676 [==============================] - 6s 279us/step - loss: 0.6322 - acc: 0.6110 - val_loss: 0.6345 - val_acc: 0.6254\n",
      "Epoch 97/100\n",
      "22676/22676 [==============================] - 6s 276us/step - loss: 0.6325 - acc: 0.6095 - val_loss: 0.6326 - val_acc: 0.6251\n",
      "Epoch 98/100\n",
      "22676/22676 [==============================] - 6s 278us/step - loss: 0.6327 - acc: 0.6086 - val_loss: 0.6326 - val_acc: 0.6249\n",
      "Epoch 99/100\n",
      "22676/22676 [==============================] - 6s 282us/step - loss: 0.6316 - acc: 0.6113 - val_loss: 0.6315 - val_acc: 0.6272\n",
      "Epoch 100/100\n",
      "22676/22676 [==============================] - 6s 278us/step - loss: 0.6322 - acc: 0.6124 - val_loss: 0.6340 - val_acc: 0.6224\n",
      "************************************\n"
     ]
    }
   ],
   "source": [
    "save_model_path = '/home/ubuntu/data/team_neural_network/code/models'\n",
    "k_fold_history = []\n",
    "for i in range(1, k+1):\n",
    "    print(\"************************************\")\n",
    "    print(\"Training on fold {}\".format(i))\n",
    "    \n",
    "    data_x_path = '/home/ubuntu/data/temp/buffers/folds/fold{}/fold{}_x.data'.format(i, i)\n",
    "    data_y_path = '/home/ubuntu/data/temp/buffers/folds/fold{}/fold{}_y.data'.format(i, i)\n",
    "    # Data preparation\n",
    "    data_x = pickle.load(open(data_x_path, 'rb'))\n",
    "    data_y = pickle.load(open(data_y_path, 'rb'))\n",
    "    train_x, train_y, val_x, val_y = data_split(data_x, data_y, seed = 157)\n",
    "    train_x, val_x = pad_for_detector(train_x, m), pad_for_detector(val_x, 10)\n",
    "    \n",
    "    # Training\n",
    "    model = get_hybrid(opt, num_filters = n, kernel_size = m)\n",
    "    history = train(model, train_x, train_y, (val_x, val_y), config)\n",
    "    \n",
    "    # Saving models\n",
    "    k_fold_history.append(history)\n",
    "    model_name = 'hybrid_net_fold' + str(i) + '.h5'\n",
    "    model.save(os.path.join(save_model_path, model_name))\n",
    "    print(\"************************************\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "motifs_from_k_folds = {i:[] for i in range(1, k+1)}\n",
    "\n",
    "for i in range(1, k+1):\n",
    "    model_name = 'hybrid_net_fold' + str(i) + '.h5'\n",
    "    model = load_model(os.path.join('../../models/', model_name))\n",
    "\n",
    "    filters = model.layers[0].get_weights()[0]\n",
    "    bias = model.layers[0].get_weights()[1]\n",
    "\n",
    "    layer_name = model.layers[0].get_config()['name']\n",
    "    intermediate_layer_model = Model(inputs=model.input,\n",
    "                                     outputs=model.get_layer(layer_name).output)\n",
    "    intermediate_output = intermediate_layer_model.predict(val_x)\n",
    "\n",
    "    activated_subseq = get_activated_subseq(intermediate_output, val_x, m)\n",
    "    for k in list(activated_subseq):\n",
    "        char_list = get_char_list(activated_subseq[k])\n",
    "        uniques, freqs = get_freqs(char_list)\n",
    "        candidates = get_candidates(uniques, freqs, 0.45)\n",
    "        motifs_from_k_folds[i].append(get_motif(candidates))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************\n",
      "motifs extracted from fold 1\n",
      "\n",
      "['TGTTTGAAGA', 'TGTTTGAAAA']\n",
      "['GCAAATTGCG', 'GCAAATTGCT']\n",
      "['AATATGTTCG', 'AATATGATCG', 'AATATGTGCG', 'AATATGAGCG']\n",
      "['CCTGGTGGGC']\n",
      "['AATGGAAGTT', 'AAGGGAAGTT', 'AATGTAAGTT', 'AAGGTAAGTT', 'AATGGCAGTT', 'AAGGGCAGTT', 'AATGTCAGTT', 'AAGGTCAGTT']\n",
      "['TTATTCAGGT', 'CTATTCAGGT', 'TTATTCTGGT', 'CTATTCTGGT']\n",
      "['TATTCAAATT', 'TATTCAAATC']\n",
      "['AATACTACAA']\n",
      "['TAAACAAATC', 'GAAACAAATC', 'TTAACAAATC', 'GTAACAAATC']\n",
      "['CGGCACCAGC', 'CGGCGCCAGC']\n",
      "['TCGATTTGAC']\n",
      "['AGTGCATATC', 'AGTGCCTATC']\n",
      "['CGTTGGTGCC', 'CGTCGGTGCC', 'CGTTGGTTCC', 'CGTCGGTTCC', 'CGTTGGTGCG', 'CGTCGGTGCG', 'CGTTGGTTCG', 'CGTCGGTTCG']\n",
      "['CATAACTGAC', 'CATAACCGAC', 'CATAACTGAA', 'CATAACCGAA']\n",
      "['TCATGTCATA', 'TCATCTCATA']\n",
      "**************************\n",
      "**************************\n",
      "motifs extracted from fold 2\n",
      "\n",
      "['GTATCCTACT', 'TTATCCTACT', 'GTATCATACT', 'TTATCATACT']\n",
      "['CCAACAAAAG', 'CCCACAAAAG', 'CCAACAAATG', 'CCCACAAATG']\n",
      "['CAAGCAGCAT', 'CAAGCAGCAC']\n",
      "['CAACGCTCAA', 'CAACGCTCGA']\n",
      "['GTAATTCCCG', 'ATAATTCCCG', 'GTAATTACCG', 'ATAATTACCG', 'GTAATTCCGG', 'ATAATTCCGG', 'GTAATTACGG', 'ATAATTACGG']\n",
      "['CGGGAGGAGC', 'CGGGAGGTGC']\n",
      "['TCTCTCCGTT', 'TGTCTCCGTT', 'TCTCTCCGAT', 'TGTCTCCGAT']\n",
      "['GCCGCTCGTC', 'GCCGCGCGTC', 'GCCGCTGGTC', 'GCCGCGGGTC']\n",
      "['ATTCTTCTTT', 'ATTCTTCTGT', 'ATTCTTCTTC', 'ATTCTTCTGC']\n",
      "['CGCCGCCAGG', 'AGCCGCCAGG', 'CGCTGCCAGG', 'AGCTGCCAGG']\n",
      "['TAACGAACGG']\n",
      "['CTCATCCTCC', 'CTCAGCCTCC']\n",
      "['TTTTGTTGTG', 'GTTTGTTGTG']\n",
      "['TTAATTTTGA', 'TTAATTTTAA']\n",
      "['AACACAATCA', 'ATCACAATCA', 'AACACAACCA', 'ATCACAACCA']\n",
      "**************************\n",
      "**************************\n",
      "motifs extracted from fold 3\n",
      "\n",
      "['TGCTGCAGGC', 'GGCTGCAGGC', 'TGCTGCAGCC', 'GGCTGCAGCC']\n",
      "['CTACACTGCG', 'ATACACTGCG', 'CTAGACTGCG', 'ATAGACTGCG', 'CTACAGTGCG', 'ATACAGTGCG', 'CTAGAGTGCG', 'ATAGAGTGCG']\n",
      "['AAAAACAACG', 'AAAAACACCG', 'AAAAACAAAG', 'AAAAACACAG']\n",
      "['GCTGGTTGTT', 'CCTGGTTGTT']\n",
      "['ACTGGTTGAT']\n",
      "['CTTTTTATGT']\n",
      "['TATTCAAGAT', 'TATTCTAGAT', 'TATTCAAGTT', 'TATTCTAGTT']\n",
      "['AGTCCTGAAT', 'AGTCCTGGAT']\n",
      "['GTGACAAGCG', 'GAGACAAGCG']\n",
      "['CTGCCTCTGT', 'CTGGCTCTGT', 'CTGCCTCCGT', 'CTGGCTCCGT']\n",
      "['TGGAGCAATA', 'TGAAGCAATA']\n",
      "['GGCAGGTATA']\n",
      "['CGACGAGTAT', 'CGAGGAGTAT']\n",
      "['CAGAAATGGA']\n",
      "['TCAAGGAAAC', 'TCAAGGCAAC']\n",
      "**************************\n",
      "**************************\n",
      "motifs extracted from fold 4\n",
      "\n",
      "['TGGTTATACT', 'GGGTTATACT']\n",
      "['CCGACATCAG', 'CCGACAGCAG']\n",
      "['CAAGCCCCAT', 'CAAGCCCCAG']\n",
      "['CCACGCAGGT', 'CCACGCCGGT', 'CCACGCACGT', 'CCACGCCCGT', 'CCACGCAGGA', 'CCACGCCGGA', 'CCACGCACGA', 'CCACGCCCGA']\n",
      "['GTAGCTTCGG', 'GTAGCTTCCG', 'GTAGCTTCGC', 'GTAGCTTCCC']\n",
      "['CGGGAGCTGC', 'CGGGAACTGC']\n",
      "['TGTTGCCGAT', 'TGTTTCCGAT']\n",
      "['GCAGCCGTTC', 'GCAGCAGTTC', 'GCAGCCATTC', 'GCAGCAATTC']\n",
      "['ATTCTACTTC']\n",
      "['CGCCGCTGTG', 'CGCCGCTATG']\n",
      "['GATCAGAGAG', 'AATCAGAGAG', 'GATGAGAGAG', 'AATGAGAGAG', 'GATCAAAGAG', 'AATCAAAGAG', 'GATGAAAGAG', 'AATGAAAGAG']\n",
      "['CGCATCCTTC', 'CTCATCCTTC']\n",
      "['GTTTATTTTT', 'ATTTATTTTT']\n",
      "['TCAATTTTGA', 'TCAATTTTAA']\n",
      "['AGCACATTCG', 'AGCAGATTCG', 'AGCACATTCA', 'AGCAGATTCA']\n",
      "**************************\n",
      "**************************\n",
      "motifs extracted from fold 5\n",
      "\n",
      "['ATTAAAACAT', 'CTTAAAACAT', 'ATTCAAACAT', 'CTTCAAACAT']\n",
      "['CACGCGACAA', 'GACGCGACAA', 'CACGCGAGAA', 'GACGCGAGAA']\n",
      "['CTGCATAGAA', 'TTGCATAGAA', 'CTGCCTAGAA', 'TTGCCTAGAA', 'CTGCACAGAA', 'TTGCACAGAA', 'CTGCCCAGAA', 'TTGCCCAGAA']\n",
      "['TCCTCTTCAC', 'TACTCTTCAC', 'TCATCTTCAC', 'TAATCTTCAC', 'TCCTCTGCAC', 'TACTCTGCAC', 'TCATCTGCAC', 'TAATCTGCAC', 'TCCTCTTAAC', 'TACTCTTAAC', 'TCATCTTAAC', 'TAATCTTAAC', 'TCCTCTGAAC', 'TACTCTGAAC', 'TCATCTGAAC', 'TAATCTGAAC']\n",
      "['ACAAGATGCG', 'AAAAGATGCG']\n",
      "['CCCTTGCGCC']\n",
      "['ACTGGCGCCG', 'ACTTGCGCCG']\n",
      "['TGCGGCACTG', 'TGCGGTACTG', 'TGCGGCAATG', 'TGCGGTAATG', 'TGCGGCACCG', 'TGCGGTACCG', 'TGCGGCAACG', 'TGCGGTAACG']\n",
      "['TGATCTTCAC', 'TGCTCTTCAC', 'TGATCTTCTC', 'TGCTCTTCTC']\n",
      "['TAATTTGTCA']\n",
      "['AAAATCCAGC', 'AAAATCCAGG']\n",
      "['GAGCATATAG', 'GAGCCTATAG', 'GAGCAAATAG', 'GAGCCAATAG']\n",
      "['ATTATCTCCT', 'ATTATATCCT', 'ATTATCTCAT', 'ATTATATCAT']\n",
      "['TTAAAAATAA']\n",
      "['TGTGTGTACT']\n",
      "**************************\n"
     ]
    }
   ],
   "source": [
    "k = 5\n",
    "for i in range(1, k+1):\n",
    "    print(\"**************************\")\n",
    "    print(\"motifs extracted from fold {}\\n\".format(i))\n",
    "    for seq in motifs_from_k_folds[i]:\n",
    "        print(seq)\n",
    "    print(\"**************************\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
