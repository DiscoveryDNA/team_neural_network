{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tesla K80\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils import data\n",
    "from lookahead import Lookahead\n",
    "from models_n_training import train\n",
    "import torch\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import math\n",
    "from utilities import sampling, one_hot_encoding, curtail, get_training_data, load_data, data_split, dianostic_plots, pad_for_detector\n",
    "from transformer import*\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
    "if use_cuda:\n",
    "    print(torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = pickle.load(open('/home/ubuntu/data/dmel_seq/train_tk_x_ex.np', 'rb'))\n",
    "train_y = pickle.load(open('/home/ubuntu/data/dmel_seq/train_tk_y_ex.np', 'rb'))\n",
    "val_x = pickle.load(open('/home/ubuntu/data/dmel_seq/val_tk_x_ex.np', 'rb'))\n",
    "val_y = pickle.load(open('/home/ubuntu/data/dmel_seq/val_tk_y_ex.np', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([11408, 3388]),\n",
       " torch.Size([11408]),\n",
       " torch.Size([2854, 3388]),\n",
       " torch.Size([2854]))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x, val_x = torch.from_numpy(train_x), torch.from_numpy(val_x)\n",
    "train_y, val_y = torch.from_numpy(train_y), torch.from_numpy(val_y)\n",
    "\n",
    "# Generate dataset for data loader\n",
    "train_dataset = data.TensorDataset(train_x, train_y)\n",
    "val_dataset = data.TensorDataset(val_x, val_y)\n",
    "train_x.shape, train_y.shape, val_x.shape, val_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reference: http://nlp.seas.harvard.edu/2018/04/03/attention.html\n",
    "# class PositionalEncoding(nn.Module):\n",
    "#     \"Implement the PE function.\"\n",
    "#     def __init__(self, d_model, dropout, max_len=5000):\n",
    "#         super(PositionalEncoding, self).__init__()\n",
    "#         self.dropout = nn.Dropout(p=dropout)\n",
    "        \n",
    "#         # Compute the positional encodings once in log space.\n",
    "#         pe = torch.zeros(max_len, d_model)\n",
    "#         position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "# #         div_term = torch.exp(torch.arange(0, d_model, 2) *\n",
    "# #                              -(math.log(10000.0) / d_model))\n",
    "#         div_term = 1 / (10000 ** (torch.arange(0., d_model, 2) / d_model)) \n",
    "#         pe[:, 0::2] = torch.sin(position * div_term).cuda()\n",
    "#         pe[:, 1::2] = torch.cos(position * div_term).cuda()\n",
    "#         pe = pe.unsqueeze(0)\n",
    "#         self.register_buffer('pe', pe)\n",
    "        \n",
    "#     def forward(self, x):\n",
    "#         x = x + torch.Tensor(self.pe[:, :x.size(1)]).cuda()\n",
    "#         return self.dropout(x)\n",
    "\n",
    "# https://towardsdatascience.com/how-to-code-the-transformer-in-pytorch-24db27c8f9ec\n",
    "class PositionalEncoder(nn.Module):\n",
    "    def __init__(self, d_model, dropout, max_seq_len = 5000):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        \n",
    "        # create constant 'pe' matrix with values dependant on \n",
    "        # pos and i\n",
    "        pe = torch.zeros(max_seq_len, d_model)\n",
    "        pe.requires_grad=False\n",
    "        for pos in range(max_seq_len):\n",
    "            for i in range(0, d_model, 2):\n",
    "                pe[pos, i] = \\\n",
    "                math.sin(pos / (10000 ** ((2 * i)/d_model)))\n",
    "                pe[pos, i + 1] = \\\n",
    "                math.cos(pos / (10000 ** ((2 * (i + 1))/d_model)))\n",
    "                \n",
    "        self.pe = pe.unsqueeze(0)[:, :max_seq_len].cuda()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # make embeddings relatively larger\n",
    "        x = x * math.sqrt(self.d_model)\n",
    "        #add constant to embedding\n",
    "        seq_len = x.size(1)\n",
    "        x = x + self.pe\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Self-attention encoder net\n",
    "class SAENet(nn.Module):\n",
    "    \n",
    "    def __init__(self, d_model, vocab, nhead, num_layers, fc_dim, dropout=0.1):\n",
    "        super(SAENet, self).__init__()\n",
    "        \n",
    "        encoder_layer = TransformerEncoderLayer(d_model, nhead)\n",
    "        self.encoder = TransformerEncoder(encoder_layer, num_layers)\n",
    "        self.src_embed = nn.Embedding(vocab, d_model)\n",
    "        self.position = PositionalEncoder(d_model, dropout, max_seq_len=3388)\n",
    "        self.fc1 = nn.Linear(d_model, fc_dim)\n",
    "        self.fc2 = nn.Linear(fc_dim, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, seq):\n",
    "        seq = self.src_embed(seq)\n",
    "        seq = self.position(seq)\n",
    "        seq = self.encoder(seq)\n",
    "        out = self.fc2(self.dropout(F.relu(self.fc1(seq))))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerClassifier(nn.Module):\n",
    "    def __init__(self, d_model, vocab, nhead, num_layers, fc_dim, dropout=0.5):\n",
    "        super(TransformerClassifier, self).__init__()\n",
    "        self.encoder = SAENet(d_model, vocab, nhead, num_layers, fc_dim)\n",
    "        self.fc = nn.Linear(d_model, 1)\n",
    "        self.head = nn.Linear(3388, 1)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, seq):\n",
    "        seq = self.encoder(seq)\n",
    "        seq = torch.squeeze(self.dropout(F.relu(self.fc(seq))))\n",
    "        seq = torch.squeeze(self.head(seq))\n",
    "        return nn.Sigmoid()(seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tclassifier = TransformerClassifier(d_model=100, vocab=5, nhead=5, num_layers=1, fc_dim=150).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "train_loader = data.DataLoader(train_dataset, batch_size=64)\n",
    "with torch.no_grad():\n",
    "    for batch, labels in train_loader:\n",
    "        batch, labels = batch.to(device), labels.to(device)\n",
    "        print(tclassifier(batch).shape)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_opt = torch.optim.Adam(tclassifier.parameters(), lr=1e-4, weight_decay=1e-5, betas=(0.9, 0.999)) # Any optimizer\n",
    "lookahead = Lookahead(base_opt, k=5, alpha=0.5) # Initialize Lookahead\n",
    "optimizers = {'adam': torch.optim.Adam(tclassifier.parameters(), lr=1e-3, weight_decay=1e-4),\n",
    "              'rmsprop': torch.optim.RMSprop(tclassifier.parameters(), lr=1e-3, weight_decay=1e-4), \n",
    "              'lookahead': lookahead}\n",
    "\n",
    "config = {'epochs':500, 'device':device, \n",
    "          'opt': optimizers['lookahead'],\n",
    "          'criterion':nn.BCELoss(),\n",
    "          'batch_size': 64,\n",
    "          'log_interval':1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 11408 samples, validate on 2854 samples\n",
      "***************************************\n",
      "Epoch 1: training loss 0.69268559976663, training acc 0.5334322625698324\n",
      "Time: 185.32201981544495 \n",
      "\n",
      "[Validation loss 0.6926648179690044, validation acc 0.5322368421052631] \n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-7b00cb98cfa2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtclassifier\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/data/team_neural_network/code/experiments/transformer_encoder/models_n_training.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, train_dataset, val_dataset, config)\u001b[0m\n\u001b[1;32m    105\u001b[0m             \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m             \u001b[0my_hat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_hat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m             \u001b[0;31m# Backward pass and updating weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    510\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mweak_script_method\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    511\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 512\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary_cross_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    513\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    514\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mbinary_cross_entropy\u001b[0;34m(input, target, weight, size_average, reduce, reduction)\u001b[0m\n\u001b[1;32m   2111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2112\u001b[0m     return torch._C._nn.binary_cross_entropy(\n\u001b[0;32m-> 2113\u001b[0;31m         input, target, weight, reduction_enum)\n\u001b[0m\u001b[1;32m   2114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train(tclassifier, train_dataset, val_dataset, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
