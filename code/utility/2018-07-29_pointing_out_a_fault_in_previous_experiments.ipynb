{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Date**: 2018-07-29\n",
    "\n",
    "**Authors**: Yichen Fang\n",
    "\n",
    "**Purpose**: Pointing out a fault in previous experiments, and testing the old neural network on new correct one-hot encoded output with motif attached\n",
    "\n",
    "**Background**:\n",
    "- It was discovered that the old one-hot encoded outputs with motif attached are incorrect. There was a serious fault in the old motif processing file (for some reason, a level of indentation of a block of code was wrong), causing faulty outputs to be produced. This error caused: for each section, the 24 sequences were all assigned the motifs of the first sequence (i.e. the motifs assigned for all 23 sequences other than the first are not correct). Hence, the results are not valid due to this fault.\n",
    "- As a result, the results based on those incorrect outputs in previous experiments are no longer valid.\n",
    "- This experiement serves the purpose to test the effect of old RNN model on the new correct one-hot encoded outputs.\n",
    "\n",
    "**Experiment**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the real/random sequences buffer path, length to curtail all sequences to, and the number of motifs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_buffer_path = \"/home/ubuntu/newOutput/10_percent/random_0.1_instance_7.txt\"\n",
    "random_buffer_path = \"/home/ubuntu/formatted/random_sequences/random_sequence_buffer.txt\"\n",
    "curtail_len = 3000\n",
    "motif_num = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the `pickle` buffered list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8088"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(real_buffer_path, \"rb\") as buff:\n",
    "    seq_record_list = pickle.load(buff)\n",
    "len(seq_record_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell randomly shuffles the sequences. The shuffling ensures that, for each DNA section (each consists of 24 segments), there are definitely some sequences being allocated to the training data. In this way, the final trained model would be able to learn all the characteristics from the data we have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sequences in training/validation set are: 7102\n",
      "Number of sequences in testing set are: 986\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from random import shuffle\n",
    "\n",
    "first_list = [] # to add to training set\n",
    "second_list = [] # to add to test set\n",
    "current = [] # contains all 24 sequences from the same DNA section\n",
    "\n",
    "for i in range(len(seq_record_list)):\n",
    "    current.append(seq_record_list.pop())\n",
    "    if len(current) == 24:\n",
    "        shuffle(current) # Shuffle the 24 sequences from the same DNA section\n",
    "        random_select = random.randint(18, 24) # Allocate the number of sequences to the training set\n",
    "        first_list.extend(current[:random_select])\n",
    "        second_list.extend(current[random_select:])\n",
    "        current = []\n",
    "\n",
    "shuffle(first_list) # Shuffle again to eliminate dependencies\n",
    "shuffle(second_list) # Shuffle again to eliminate dependencies\n",
    "\n",
    "seq_record_list = first_list + second_list\n",
    "\n",
    "print(\"Number of sequences in training/validation set are: \" + str(len(first_list)))\n",
    "print(\"Number of sequences in testing set are: \" + str(len(second_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_val_num = len(first_list)\n",
    "test_num = len(second_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell transforms the data into a format that is recognizable by the neural network model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A helper function to flatten a 2d list to 1d.\n",
    "# Input: [[1, 2], [2, 3], [3, 4, 5]]\n",
    "# Output: [1, 2, 2, 3, 3, 4, 5]\n",
    "def flatten(lst):\n",
    "    new_lst = []\n",
    "    for sub_lst in lst:\n",
    "        for item in sub_lst:\n",
    "            new_lst.append(item)\n",
    "    return new_lst\n",
    "\n",
    "# A helper function to transform a lst so that its length becomes read_len by:\n",
    "# 1. If len(lst) > read_len, curtail the end of the lst.\n",
    "# 2. If len(lst) < read_len, keep extending the end of the lst with 0 (NA).\n",
    "def curtail(lst, read_len, motif_number):\n",
    "    if len(lst) > read_len:\n",
    "        lst = lst[:read_len]\n",
    "    else:\n",
    "        for i in range(read_len - len(lst)):\n",
    "            lst.append([0 for _ in range(motif_number + 4)])\n",
    "    return lst\n",
    "\n",
    "# Produce the train-test split\n",
    "# length_read: the length that you want all DNA sequences to conform to\n",
    "def prepare_input(training_size, test_size, length_read, original_list, motif_number):\n",
    "    X_train = []\n",
    "    y_train = []\n",
    "    X_test = []\n",
    "    y_test = []\n",
    "    seq_count = 0\n",
    "    while seq_count < training_size:\n",
    "        X_train.append(flatten(curtail(original_list[seq_count][3], length_read, motif_number)))\n",
    "        y_train.append(int(original_list[seq_count][1]))\n",
    "        seq_count += 1\n",
    "    while seq_count < (training_size + test_size):\n",
    "        X_test.append(flatten(curtail(original_list[seq_count][3], length_read, motif_number)))\n",
    "        y_test.append(int(original_list[seq_count][1]))\n",
    "        seq_count += 1\n",
    "    return X_train, y_train, X_test, y_test\n",
    "\n",
    "# Turn list into numpy tensors that can directly feed into a neural network model\n",
    "def to_np_array(X_train, y_train, X_test, y_test):\n",
    "    X_train = np.array(X_train)\n",
    "    y_train = np.array(y_train)\n",
    "    if len(y_train.shape) == 1:\n",
    "        y_train = np.transpose(np.array([y_train]))\n",
    "    X_test = np.array(X_test)\n",
    "    y_test = np.transpose(np.array(y_test))\n",
    "    if len(y_test.shape) == 1:\n",
    "        y_test = np.transpose(np.array([y_test]))\n",
    "    return X_train, y_train, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(7102, 21000), (7102, 1), (986, 21000), (986, 1)]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, y_train, X_test, y_test = prepare_input(train_val_num, test_num, curtail_len, seq_record_list, motif_num)\n",
    "X_train, y_train, X_test, y_test = to_np_array(X_train, y_train, X_test, y_test)\n",
    "[X_train.shape, y_train.shape, X_test.shape, y_test.shape]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We run the experiment with four LSTM layers, having 8, 8, 4, 4 units respectively, and 250 epoches:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'keras'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-4996e9b699fb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSequential\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDense\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCuDNNLSTM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCuDNNGRU\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'keras'"
     ]
    }
   ],
   "source": [
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Dense, CuDNNLSTM, CuDNNGRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_rnn = X_train.reshape(train_val_num, curtail_len, motif_num + 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(CuDNNLSTM(8, input_shape=(curtail_len, motif_num + 4), return_sequences=True))\n",
    "model.add(CuDNNLSTM(8, return_sequences=True))\n",
    "model.add(CuDNNLSTM(4, return_sequences=True))\n",
    "model.add(CuDNNLSTM(4))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n",
    "history = model.fit(X_train_rnn, y_train, epochs=30, batch_size=128, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Result**:\n",
    "\n",
    "The following cell **visualize** the training/validation accuracies and losses over each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs = range(1, len(acc) + 1)\n",
    "\n",
    "plt.plot(epochs, acc, 'bo', label='Training Accuracy')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation Accuracy')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.xlabel('epoches')\n",
    "plt.legend()\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(epochs, loss, 'bo', label='Training Loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('epoches')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusion**:\n",
    "- From the graphs above, one can conclude that the old RNN model does not actually work satisfiably on the correct data. Hence, we need to overhaul the neural network model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
