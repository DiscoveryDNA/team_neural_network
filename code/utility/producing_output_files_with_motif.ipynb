{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Author**: Yichen Fang\n",
    "\n",
    "**Purpose**:\n",
    "\n",
    "The purpose of this notebook is to produce output files by one-hot encoding the dna sequence, and adding the tfbs scores to the one-hot encoding.\n",
    "\n",
    "The output files are first saved as plain txt files in the specified output folder. They are also combined together into one huge list which is stored as a `pickle` buffer (so that the loading time of the output is faster).\n",
    "\n",
    "**Important Note Before Using the File**:\n",
    "\n",
    "1. Always check the **output location** and the **buffer file name** are correct and intended. Otherwise, you may accidentally **overwrite** previous results.\n",
    "2. ALways check the motif file names corresponds to the ones you are using. Otherwise, the script would not work correctly.\n",
    "3. This file should only be used to generate outputs that contain motifs. To generate outputs without motifs attached, please use the script `producing_output_files_without_motif.ipynb`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import ast\n",
    "import pickle\n",
    "import shelve\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from Bio import SeqIO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set address variables in the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_folder_path = \"/home/ubuntu/data/team_neural_network/data/input/3.24_species_only\" + \"/\"\n",
    "output_folder_path = \"/home/ubuntu/formatted/output\" + \"/\"\n",
    "motif_folder_path = \"/home/ubuntu/raw/5_TFBS_scores_18July2018\" + \"/\"\n",
    "path_to_buffer_file = \"/home/ubuntu/formatted/buffers\" + \"/\" + \"all_data_buffer.txt\" \n",
    "# NOTE: the buffer file need not be created beforehands. Just write the path\n",
    "# and the file name here. The file would be created automatically by the system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the motif variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "motif_1 = \"zelda\"\n",
    "motif_2 = \"hb\"\n",
    "motif_3 = \"bcd\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell imports all the TFBS scores and transform them into a dictionary called `all_scores`.\n",
    "\n",
    "`all_scores` has the data structure: `{motif: {species: {raw_position: score}}}`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A helper function to extract the motif name from the csv name.\n",
    "def get_motif(name):\n",
    "    if name == 'zelda':\n",
    "        return motif_1\n",
    "    if name == 'hb':\n",
    "        return motif_2\n",
    "    if name == 'bcd':\n",
    "        return motif_3\n",
    "\n",
    "def motif_processing(motif_name):\n",
    "    all_csvs = glob.glob(motif_folder_path + motif_name + '/*.csv')\n",
    "    all_scores = shelve.open(\"/home/ubuntu/formatted/motif_dic\")\n",
    "    curr_motif = {}\n",
    "    u = 0\n",
    "    for csv_ in all_csvs:\n",
    "        with open(csv_, encoding='utf-8') as csv_file:\n",
    "            for a_line in csv_file:\n",
    "                curr_line = a_line.split('\\t')\n",
    "                strand = curr_line[6]\n",
    "                if strand == 'positive\\n':\n",
    "                    score = float(curr_line[2])\n",
    "                    species = curr_line[4]\n",
    "                    raw_position = int(curr_line[5])\n",
    "                    if species not in curr_motif:\n",
    "                        curr_motif[species] = {}\n",
    "                    curr_motif[species][raw_position] = score\n",
    "        u += 1\n",
    "        print(motif_name + ': ' + str(u))\n",
    "    all_scores[motif_name] = curr_motif\n",
    "    all_scores.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "motif_processing('zelda')\n",
    "motif_processing('hb')\n",
    "motif_processing('bcd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_scores = shelve.open(\"/home/ubuntu/formatted/motif_dic\")\n",
    "new_scores = shelve.open(\"/home/ubuntu/formatted/species_motif_dic\")\n",
    "\n",
    "def redesign_shelve(motif):\n",
    "    v = 0\n",
    "    current_motif = all_scores[motif]\n",
    "    for species in current_motif:\n",
    "        if species not in new_scores:\n",
    "            species_dic = {}\n",
    "            species_dic[motif] = current_motif[species]\n",
    "            new_scores[species] = species_dic\n",
    "        else:\n",
    "            species_dic = new_scores[species]\n",
    "            species_dic[motif] = current_motif[species]\n",
    "            new_scores[species] = species_dic\n",
    "        v += 1\n",
    "        print(motif + \": \" + str(v))\n",
    "\n",
    "redesign_shelve('zelda')\n",
    "redesign_shelve('hb')\n",
    "redesign_shelve('bcd')\n",
    "\n",
    "new_scores.close()\n",
    "all_scores.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The aim of the following cell is to produce a one-hot encoding scheme with TFBS scores embedded for each DNA sequence segment.\n",
    "\n",
    "It consists of three parts:\n",
    "\n",
    "1. Read in all DNA sequence segments.\n",
    "2. Transform each position of the DNA sequence into a 4-letter one-hot encoding based on the `base_pairs` dictionary.\n",
    "3. For each position, attach the TFBS scores to the end of the one-hot encoding.\n",
    "4. Output the final encoding into `txt` files for bookkeeping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the following dictionary to perform the transformation\n",
    "base_pairs = {'A': [1, 0, 0, 0], \n",
    "              'C': [0, 1, 0, 0],\n",
    "              'G': [0, 0, 1, 0],\n",
    "              'T': [0, 0, 0, 1],\n",
    "              'a': [1, 0, 0, 0],\n",
    "              'c': [0, 1, 0, 0],\n",
    "              'g': [0, 0, 1, 0],\n",
    "              't': [0, 0, 0, 1],\n",
    "              'n': [0, 0, 0, 0],\n",
    "              'N': [0, 0, 0, 0]}\n",
    "\n",
    "file_num_limit = 10000    # The maximum number of files to be decoded\n",
    "file_count = 0\n",
    "\n",
    "def lacking_motif(sequence):\n",
    "    if 'zelda' not in sequence:\n",
    "        return True\n",
    "    elif 'hb' not in sequence:\n",
    "        return True\n",
    "    elif 'bcd' not in sequence:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "new_scores = shelve.open(\"/home/ubuntu/formatted/species_motif_dic\")\n",
    "\n",
    "# Iterate through every file\n",
    "for file in os.listdir(input_folder_path):\n",
    "    one_hot = []\n",
    "    # When the number of file decoded has reached the limit, stop\n",
    "    if file_count < file_num_limit:\n",
    "        data = list(SeqIO.parse(input_folder_path + file,\"fasta\"))\n",
    "        for n in range(0, len(data)):\n",
    "            # Extract the header information\n",
    "            header = data[n].description.split('|')\n",
    "            descr = data[n].description\n",
    "            regionID = header[0]\n",
    "            expressed = header[1]\n",
    "            speciesID = header[2]\n",
    "            strand = header[3]\n",
    "            # Complement all sequences in the negative DNA strand\n",
    "            if strand == '-':\n",
    "                # Using the syntax [e for e in base_pairs[n]] to create a new pointer for each position\n",
    "                one_hot.append([descr, expressed, speciesID, [[e for e in base_pairs[n]] for n in data[n].seq.complement()]])\n",
    "            else:\n",
    "                one_hot.append([descr, expressed, speciesID, [[e for e in base_pairs[n]] for n in data[n].seq]])\n",
    "        # Attach the TFBS scores to the end of each position\n",
    "        to_write = True\n",
    "        for item in one_hot:\n",
    "            # Only outputs sequences that currently have TFBS scores\n",
    "            # Ignore all sequences that do not have TFBS scores yet\n",
    "            sequence_name = item[0]\n",
    "            if sequence_name not in new_scores:\n",
    "                to_write = False\n",
    "                break\n",
    "            current_sequence = new_scores[sequence_name]\n",
    "            if lacking_motif(current_sequence):\n",
    "                to_write = False\n",
    "                break\n",
    "            i = 0\n",
    "            for encoding in item[3]:\n",
    "                # Take care of positions that do not have TFBS scores, attaching 0 as placeholder (i.e. NA)\n",
    "                if i not in current_sequence['zelda']:\n",
    "                    encoding.extend([0, 0, 0])\n",
    "                else:\n",
    "                    encoding.append(current_sequence['zelda'][i])\n",
    "                    encoding.append(current_sequence['hb'][i])\n",
    "                    encoding.append(current_sequence['bcd'][i])\n",
    "                i += 1\n",
    "        # Write the final encoding into txt files\n",
    "        if to_write:\n",
    "            with open(output_folder_path + regionID + \".txt\", mode=\"w\", encoding='utf-8') as output:\n",
    "                output.write(str(one_hot))\n",
    "            file_count += 1\n",
    "            print(\"output: \" + str(file_count))\n",
    "\n",
    "new_scores.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The rest of the notebook uses the one-hot encoding files produced above to build a neural network prototype to make sure everything works as intended.\n",
    "\n",
    "The following cell reads in one-hot encoding files as a list `seq_record_list`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_txts = glob.glob(output_folder_path + '*.txt')\n",
    "seq_record_list = []\n",
    "i = 0\n",
    "# Iterate through all one-hot encoding files\n",
    "for txt_ in all_txts:\n",
    "    i += 1\n",
    "    print(i)\n",
    "    with open(txt_, encoding='utf-8') as f:\n",
    "        # attach the one-hot encoding information of this file to the end of seq_record_list\n",
    "        seq_record_list += ast.literal_eval(f.read())\n",
    "len(seq_record_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell saves `seq_record_list` as a `pickle` buffer so that it can be retreated much faster next time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(path_to_buffer_file, \"wb\") as buff:\n",
    "    pickle.dump(seq_record_list, buff)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_tensorflow_p36)",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
